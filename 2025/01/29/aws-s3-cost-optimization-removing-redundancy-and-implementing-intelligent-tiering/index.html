<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Aws S3 Cost Optimization: Removing Redundancy and Implementing Intelligent Tiering | devDosvid blog</title>
<meta name=keywords content><meta name=description content="Legacy setups can hide costs, and sometimes big. By challenging this, I learned some cool stuff about S3 Intelligent-Tiering and Lifecycle Configuration."><meta name=author content="Serhii Vasylenko"><link rel=canonical href=https://devdosvid.blog/2025/01/29/aws-s3-cost-optimization-removing-redundancy-and-implementing-intelligent-tiering/><link crossorigin=anonymous href=/assets/css/stylesheet.5646840c6f09d9c80952a9aac690950f7d6243912ac8bdf52882c5d1990edd8c.css integrity="sha256-VkaEDG8J2cgJUqmqxpCVD31iQ5EqyL31KILF0ZkO3Yw=" rel="preload stylesheet" as=style><link rel=icon href=https://devdosvid.blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://devdosvid.blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://devdosvid.blog/favicon-32x32.png><link rel=apple-touch-icon href=https://devdosvid.blog/apple-touch-icon.png><link rel=mask-icon href=https://devdosvid.blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://devdosvid.blog/2025/01/29/aws-s3-cost-optimization-removing-redundancy-and-implementing-intelligent-tiering/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><style>@font-face{font-family:albert sans;font-style:normal;font-display:swap;font-weight:300;src:url(/assets/fonts/AlbertSans-Light.woff2)format('woff');unicode-range:U+??,U+131,U+152-153,U+2BB-2BC,U+2C6,U+2DA,U+2DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:albert sans;font-style:italic;font-display:swap;font-weight:300;src:url(/assets/fonts/AlbertSans-LightItalic.woff2)format('woff');unicode-range:U+??,U+131,U+152-153,U+2BB-2BC,U+2C6,U+2DA,U+2DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:albert sans;font-style:normal;font-display:swap;font-weight:400;src:url(/assets/fonts/AlbertSans-Regular.woff2)format('woff');unicode-range:U+??,U+131,U+152-153,U+2BB-2BC,U+2C6,U+2DA,U+2DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:albert sans;font-style:italic;font-display:swap;font-weight:400;src:url(/assets/fonts/AlbertSans-Italic.woff2)format('woff');unicode-range:U+??,U+131,U+152-153,U+2BB-2BC,U+2C6,U+2DA,U+2DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:albert sans;font-style:normal;font-display:swap;font-weight:500;src:url(/assets/fonts/AlbertSans-Medium.woff2)format('woff');unicode-range:U+??,U+131,U+152-153,U+2BB-2BC,U+2C6,U+2DA,U+2DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:albert sans;font-style:italic;font-display:swap;font-weight:500;src:url(/assets/fonts/AlbertSans-MediumItalic.woff2)format('woff');unicode-range:U+??,U+131,U+152-153,U+2BB-2BC,U+2C6,U+2DA,U+2DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:albert sans;font-style:normal;font-display:swap;font-weight:600;src:url(/assets/fonts/AlbertSans-SemiBold.woff2)format('woff');unicode-range:U+??,U+131,U+152-153,U+2BB-2BC,U+2C6,U+2DA,U+2DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:albert sans;font-style:italic;font-display:swap;font-weight:600;src:url(/assets/fonts/AlbertSans-SemiBoldItalic.woff2)format('woff');unicode-range:U+??,U+131,U+152-153,U+2BB-2BC,U+2C6,U+2DA,U+2DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style><meta name=twitter:creator content="@vasylenko"><meta name=twitter:site content="@vasylenko"><script async src=https://scripts.simpleanalyticscdn.com/latest.js></script><meta property="og:url" content="https://devdosvid.blog/2025/01/29/aws-s3-cost-optimization-removing-redundancy-and-implementing-intelligent-tiering/"><meta property="og:site_name" content="devDosvid blog"><meta property="og:title" content="Aws S3 Cost Optimization: Removing Redundancy and Implementing Intelligent Tiering"><meta property="og:description" content="Legacy setups can hide costs, and sometimes big. By challenging this, I learned some cool stuff about S3 Intelligent-Tiering and Lifecycle Configuration."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-29T02:48:15+01:00"><meta property="article:modified_time" content="2025-02-09T18:58:16+01:00"><meta property="og:image" content="https://devdosvid.blog/2025/01/29/aws-s3-cost-optimization-removing-redundancy-and-implementing-intelligent-tiering/cover.webp"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://devdosvid.blog/2025/01/29/aws-s3-cost-optimization-removing-redundancy-and-implementing-intelligent-tiering/cover.webp"><meta name=twitter:title content="Aws S3 Cost Optimization: Removing Redundancy and Implementing Intelligent Tiering"><meta name=twitter:description content="Legacy setups can hide costs, and sometimes big. By challenging this, I learned some cool stuff about S3 Intelligent-Tiering and Lifecycle Configuration."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://devdosvid.blog/posts/"},{"@type":"ListItem","position":2,"name":"Aws S3 Cost Optimization: Removing Redundancy and Implementing Intelligent Tiering","item":"https://devdosvid.blog/2025/01/29/aws-s3-cost-optimization-removing-redundancy-and-implementing-intelligent-tiering/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Aws S3 Cost Optimization: Removing Redundancy and Implementing Intelligent Tiering","name":"Aws S3 Cost Optimization: Removing Redundancy and Implementing Intelligent Tiering","description":"Legacy setups can hide costs, and sometimes big. By challenging this, I learned some cool stuff about S3 Intelligent-Tiering and Lifecycle Configuration.","keywords":[],"articleBody":"Legacy architectural decisions often carry hidden costs. Here‚Äôs how questioning a storage configuration saved us a hundred thousand and taught some lessons about S3.\nThis is a technical walkthrough of identifying, analyzing, and solving two specific S3 cost optimization problems I faced recently: eliminating unnecessary data and implementing intelligent storage tiering.\nNote: All mentioned AWS prices are from January 2025 in the us-east-1 region. AWS pricing and features may have changed since the publication.\nWhat was the cause Picture this: In December 2024, two S3 buckets quietly consumed about $19,500. The price is just for one month. That‚Äôs nearly $235,000 annually, but the size of buckets slowly grows, so it‚Äôs going to be more. These buckets are for JFrog Artifactory, which supports AWS S3 as a file storage. The first bucket contains CI/CD builds and cached third-party artifacts ‚Äî essential data we need daily. The second bucket is the replica in another region. This build data is important, yet paying $120,000 a year to duplicate that data seems overkill.\nNow the question is: WHY do we have such a setup with replication? As always happens, that was configured long ago, so no one knows by now.\nLet‚Äôs give this a fresh look: S3 provides 11-nines durability by design; if something gets deleted by accident, we can always re-run CI/CD and build that again. Does replication provide any value?\nThe problem statement is simple: the buckets are enormous, we pay half the price for nothing, and the amount of data might grow more.\nTwo S3 buckets: main in us-east-1 and replica in us-west-2 About 340TB each 21+ million objects per bucket Average object size: 17MB 7% monthly growth rate based on the last 12 months‚Äô trend The two-fold solution to that was not that trivial, though.\nAddressing the unpredicted S3 bucket growth and access patterns By the design of S3 service, a client (you or your application) should specify the storage class when uploading the object to an S3 bucket; there is no thing like ‚Äúdefault storage class for new objects‚Äù.\nSurprisingly, despite a versatile configuration options for S3, JFrog Artifactory does not allow setting storage classes for the objects it stores. So, everything you store is sent to S3 with the Standard storage class.\nOn a large scale, with many teams and projects, it is pretty hard to predict the lifetime and the frequency of one or other artifact for the particular team. Auto-cleanup options are built into Artifactory, but they do not answer the retention questions anyway: How long? How frequent?\nLuckily, AWS has two powerful things to address these issues:\nIntelligent-Tiering storage class Lifecycle Configuration Benefits of Intelligent-Tiering Storage class As its name implies, Intelligent-Tiering automatically defines the best access tier when access patterns change. There are three basic tiers: one is optimized for frequent access, another for infrequent access, and a very low-cost tier, which is optimized for rarely accessed data.\nFor a relatively small price, which is negligible on scale, S3 does the monitoring and tier transition of the objects:\nMonitoring: $0.0025 per 1,000 objects/month Storage for GB/month: $0.021 ‚Äî frequent tier: object stored here by default $0.0125 ‚Äî infrequent tier: object moved here after 30 days without access $0.004 ‚Äî archive tier: object moved here after 90 days without access The vast advantage of Intelligent-Tiering comes with the infrequent and archive tier prices on scale.\nBut note that objects smaller than 128 KB are not eligible for auto tiering.\nS3 Intelligent-Tiering Lifecycle Configuration to move S3 objects between storage classes So now the question is: how to move all the objects to Intelligent-Tiering, old and any new ones?\nHere comes Lifecycle Configuration. S3 Lifecycle also manipulates objects, but the key difference between Intelligent-Tiering and Lifecycle Configuration is that Configuration does not have the access pattern analysis as a trigger ‚Äî the only trigger for Configuration is the time (e.g., trigger in N days).\nHowever, a trigger by time mark is precisely what‚Äôs needed when your software does not support the custom storage classes ‚Äî you want to move objects as soon as possible to Intelligent-Tiering using Lifecycle Configuration.\nHere‚Äôs how you need to configure the Lifecycle to move all the objects to Intelligent-Tiering:\nApply to all objects in the bucket Actions: The 1st option ‚Äî Transition current versions of objects between storage classes The 2nd option ‚Äî Transition noncurrent versions of objects between storage classes Specify Intelligent-Tiering for ‚ÄúTransition current version‚Äù and ‚ÄúTransition noncurrent versions‚Äù Set 0 as the number of ‚ÄúDays after object creation‚Äù and ‚ÄúDays after objects become noncurrent‚Äù S3 Lifecycle Rule: Move to Intelligent-Tiering A zero value here does not mean an immediate change of the storage class: by design, the transition will be executed at midnight UTC from the object upload date. For existing objects, the transition will begin at midnight UTC after the lifecycle rule is applied.\nI confirmed with AWS technical support that there are no indications of throttling or limitations on object access during transitions. All objects remain accessible during the transition process. Lifecycle Configuration has a price of $0.01 per 1,000 Transition requests, which technically adds $0.00001 as the one-time cost of each object affected by the policy.\nFor example, in my case, moving 21 million objects to the Intelligent-Tiering class had a one-time cost of $210, but ROI for this is much more significant the next month.\nUntrivial termination of a large S3 bucket How hard can it be to empty and delete the bucket with 21 million objects inside? (To clarify: S3 does not allow deletion of a non-empty bucket)\nThe most obvious way, at first glance, would be to use the AWS S3 web Console option called ‚ÄúEmpty‚Äù, right?\nS3 Empty Bucket Alas, it is not that simple.\nWhen you go that way, it is your browser who removes the objects by sending API calls to AWS. There is no background job for you. It does that efficiently, sending DELETE requests in 1000-item batches, but it does that as long as your IAM session remains active (or the browser window remains open, whatever ends first).\nIf your bucket has Bucket Versioning enabled, ‚ÄúEmpty‚Äù action will not remove all the object versions.\nSo then we have two other options:\nEither run some script that removes all object versions (including current, older, and null versions) and delete markers. Or set up a couple of Lifecycle Configuration rules to purge a bucket in an unattended way. If the total number of objects is relatively small, e.g., up to few hundred thousand, it is feasible to run a script that removes all object versions and delete markers. Click here to see the code snippet #!/usr/bin/env python3 import sys import boto3 def purge_bucket_interactive(bucket_name: str) -\u003e None: \"\"\" Display the bucket ARN and region, warn the user, and prompt them to type 'YES' in uppercase. If confirmed, permanently remove all object versions (including current, older, and null versions) and delete markers. \"\"\" # We use the S3 client to query bucket location info s3_client = boto3.client(\"s3\") response = s3_client.get_bucket_location(Bucket=bucket_name) region = response.get(\"LocationConstraint\") or \"us-east-1\" # Construct the bucket's ARN (for standard S3 buckets, the region is not typically embedded) bucket_arn = f\"arn:aws:s3:::{bucket_name}\" print(f\"Bucket ARN : {bucket_arn}\") print(f\"Bucket region: {region}\") print(\"WARNING: This will permanently remove ALL VERSIONS of every object.\") print(\"It is NOT REVERSIBLE!!!\\n\") confirm = input(\"Type 'YES' in uppercase to proceed with the permanent removal: \").strip() if confirm != \"YES\": print(\"Aborted. No changes made.\") return # Proceed with deletion print(\"Starting purge... This might take a while.\") # Use the S3 resource to delete all versions s3 = boto3.resource(\"s3\") bucket = s3.Bucket(bucket_name) bucket.object_versions.all().delete() print(\"All object versions and delete markers have been removed.\") if __name__ == \"__main__\": if len(sys.argv) \u003c 2: print(f\"Usage: {sys.argv[0]} \") sys.exit(1) bucket_to_purge = sys.argv[1] purge_bucket_interactive(bucket_to_purge) But if you have millions of objects, you‚Äôd better use Lifecycle Configuration rules to empty a version-enabled bucket. First, you need to pause the versioning. Then, create two Lifecycle Configuration rules.\nThe first rule will delete all versions of the objects:\nApply to all objects in the bucket Actions: The 3rd option ‚Äî ‚ÄúExpire current versions of objects‚Äù The 4th option ‚Äî ‚ÄúPermanently delete previous versions of objects‚Äù The number of days you would like the current version to expire, to do as soon as possible, enter 1 in the text box. That makes it ‚Äú1 day‚Äù. For the days after which the noncurrent versions will be permanently deleted, enter 1 in the text box. That makes it ‚Äú1 day‚Äù of being noncurrent. S3 Lifecycle Rule: Delete Versions Due to how S3 versioning works, a special DELETE marker will be created for each object processed by the first rule. To handle the delete markers, you must create a new lifecycle rule, as you won‚Äôt be able to select the option to delete the ‚Äúdelete markers‚Äù in the first rule.\nFor the second lifecycle rule, the steps are similar, and the only difference is that you must select the 5th and last option: ‚ÄúDelete expired object delete markers or incomplete multipart uploads.‚Äù Select both the options ‚Äî ‚ÄúDelete expired object delete markers‚Äù and ‚ÄúDelete incomplete multipart uploads‚Äù ‚Äî and set it to 1 day.\nS3 Lifecycle Rule: Delete Markers S3 Lifecycle operations are asynchronous, and it may take some time for the Lifecycle to delete the objects in your S3 bucket. However, at midnight UTC, once the S3 objects are marked for expiration, you are no longer charged for storing that objects, and S3 will do the rest. Implementation Results Cost reduction: $120,000 baseline or $270,000 if the 7% growth rate remains One-time transition cost: $210 Implementation time: 2 days Storage class transition completed in 1 day Key Takeaways Challenge your architecture decisions overtime. Challenge redundancy ‚Äî is it providing real value? Regular cost reviews can be a source of quick wins. Let automated solutions do the job, and do not overengineer things. AWS Simple Storage Service, despite its name, might be tricky, but AWS has a bunch of tools to help you. S3 Lifecycle rules proved their worth twice: first by automating our transition to Intelligent-Tiering despite Artifactory‚Äôs limitations and then by cleaning up millions of versioned objects without operational overhead.\n","wordCount":"1688","inLanguage":"en","image":"https://devdosvid.blog/2025/01/29/aws-s3-cost-optimization-removing-redundancy-and-implementing-intelligent-tiering/cover.webp","datePublished":"2025-01-29T02:48:15+01:00","dateModified":"2025-02-09T18:58:16+01:00","author":{"@type":"Person","name":"Serhii Vasylenko"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://devdosvid.blog/2025/01/29/aws-s3-cost-optimization-removing-redundancy-and-implementing-intelligent-tiering/"},"publisher":{"@type":"Person","name":"devDosvid blog","logo":{"@type":"ImageObject","url":"https://devdosvid.blog/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://devdosvid.blog/ accesskey=h title="üè† Home (Alt + H)">üè† Home</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://devdosvid.blog/index.xml title="RSS Feed"><span>RSS Feed</span></a></li><li><a href=https://devdosvid.blog/archive title="All posts"><span>All posts</span></a></li><li><a href=https://devdosvid.blog/about/ title="About me"><span>About me</span></a></li><li><a href=https://devdosvid.blog/series title="Post Series"><span>Post Series</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://devdosvid.blog/>Home</a>&nbsp;¬ª&nbsp;<a href=https://devdosvid.blog/posts/>Posts</a></div><h1 class=post-title>Aws S3 Cost Optimization: Removing Redundancy and Implementing Intelligent Tiering</h1><div class=post-description>Legacy setups can hide costs, and sometimes big. By challenging this, I learned some cool stuff about S3 Intelligent-Tiering and Lifecycle Configuration.</div><div class=post-meta><span title='2025-01-29 02:48:15 +0100 +0100'>January 29, 2025</span>&nbsp;¬∑&nbsp;Serhii Vasylenko&nbsp;|&nbsp;<a href=mailto:contact@devdosvid.blog rel="noopener noreferrer" target=_blank>Contact me</a></div></header><figure class=entry-cover><img srcset="https://devdosvid.blog/2025/01/29/aws-s3-cost-optimization-removing-redundancy-and-implementing-intelligent-tiering/cover_hu8797084166771478445.webp 360w ,https://devdosvid.blog/2025/01/29/aws-s3-cost-optimization-removing-redundancy-and-implementing-intelligent-tiering/cover_hu1549497336513461428.webp 480w ,https://devdosvid.blog/2025/01/29/aws-s3-cost-optimization-removing-redundancy-and-implementing-intelligent-tiering/cover_hu12334760033616657211.webp 720w ,https://devdosvid.blog/2025/01/29/aws-s3-cost-optimization-removing-redundancy-and-implementing-intelligent-tiering/cover_hu18435742184956350886.webp 1080w ,https://devdosvid.blog/2025/01/29/aws-s3-cost-optimization-removing-redundancy-and-implementing-intelligent-tiering/cover.webp 1200w" sizes="(min-width: 768px) 720px, 100vw" src=https://devdosvid.blog/2025/01/29/aws-s3-cost-optimization-removing-redundancy-and-implementing-intelligent-tiering/cover.webp alt="AWS S3 Cost Optimization: Removing Redundancy and Implementing Intelligent Tiering" width=1200 height=630></figure><div class=post-content><p>Legacy architectural decisions often carry hidden costs. Here&rsquo;s how questioning a storage configuration saved us a hundred thousand and taught some lessons about S3.</p><p>This is a technical walkthrough of identifying, analyzing, and solving two specific S3 cost optimization problems I faced recently: eliminating unnecessary data and implementing intelligent storage tiering.</p><p><em>Note: All mentioned AWS prices are from January 2025 in the us-east-1 region. AWS pricing and features may have changed since the publication.</em></p><h1 id=what-was-the-cause>What was the cause<a hidden class=anchor aria-hidden=true href=#what-was-the-cause>#</a></h1><p>Picture this: In December 2024, two S3 buckets quietly consumed about $19,500. The price is just for one month. That&rsquo;s nearly $235,000 annually, but the size of buckets slowly grows, so it&rsquo;s going to be more.
These buckets are for <a href=https://jfrog.com/artifactory/>JFrog Artifactory</a>, which supports AWS S3 as a file storage.
The first bucket contains CI/CD builds and cached third-party artifacts ‚Äî essential data we need daily. The second bucket is the replica in another region. This build data is important, yet paying $120,000 a year to duplicate that data seems overkill.</p><p>Now the question is: WHY do we have such a setup with replication? As always happens, that was configured long ago, so no one knows by now.</p><p>Let&rsquo;s give this a fresh look: S3 provides 11-nines durability by design; if something gets deleted by accident, we can always re-run CI/CD and build that again. Does replication provide any value?</p><p>The problem statement is simple: the buckets are enormous, we pay half the price for nothing, and the amount of data might grow more.</p><ul><li>Two S3 buckets: main in us-east-1 and replica in us-west-2</li><li>About 340TB each</li><li>21+ million objects per bucket</li><li>Average object size: 17MB</li><li>7% monthly growth rate based on the last 12 months&rsquo; trend</li></ul><p>The two-fold solution to that was not that trivial, though.</p><h1 id=addressing-the-unpredicted-s3-bucket-growth-and-access-patterns>Addressing the unpredicted S3 bucket growth and access patterns<a hidden class=anchor aria-hidden=true href=#addressing-the-unpredicted-s3-bucket-growth-and-access-patterns>#</a></h1><p>By the design of S3 service, a client (you or your application) should specify the storage class when uploading the object to an S3 bucket; there is no thing like &ldquo;default storage class for new objects&rdquo;.</p><p>Surprisingly, despite a versatile configuration options for S3, JFrog Artifactory does not allow setting storage classes for the objects it stores. So, everything you store is sent to S3 with the Standard storage class.</p><p>On a large scale, with many teams and projects, it is pretty hard to predict the lifetime and the frequency of one or other artifact for the particular team. <a href=https://jfrog.com/help/r/artifactory-cleanup-best-practices/artifactory-cleanup-best-practices>Auto-cleanup options</a> are built into Artifactory, but they do not answer the retention questions anyway: How long? How frequent?</p><p>Luckily, AWS has two powerful things to address these issues:</p><ol><li><a href=https://aws.amazon.com/getting-started/hands-on/getting-started-using-amazon-s3-intelligent-tiering/>Intelligent-Tiering storage class</a></li><li><a href=https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html>Lifecycle Configuration</a></li></ol><h2 id=benefits-of-intelligent-tiering-storage-class>Benefits of Intelligent-Tiering Storage class<a hidden class=anchor aria-hidden=true href=#benefits-of-intelligent-tiering-storage-class>#</a></h2><p>As its name implies, Intelligent-Tiering automatically defines the best access tier when access patterns change. There are three basic tiers: one is optimized for frequent access, another for infrequent access, and a very low-cost tier, which is optimized for rarely accessed data.</p><p>For a relatively small price, which is negligible on scale, S3 does the monitoring and tier transition of the objects:</p><ul><li>Monitoring: $0.0025 per 1,000 objects/month</li><li>Storage for GB/month:<ul><li>$0.021 ‚Äî frequent tier: object stored here by default</li><li>$0.0125 ‚Äî infrequent tier: object moved here after 30 days without access</li><li>$0.004 ‚Äî archive tier: object moved here after 90 days without access</li></ul></li></ul><div class=attention><p>The vast advantage of Intelligent-Tiering comes with the infrequent and archive tier prices on scale.</p><p>But note that objects smaller than 128 KB are not eligible for auto tiering.</p></div><figure><img loading=lazy src=/2025/01/29/aws-s3-cost-optimization-removing-redundancy-and-implementing-intelligent-tiering/s3-intelligent-tiering_hu5298648550714154372.webp alt="S3 Intelligent-Tiering" width=800 height=370.23><figcaption><p>S3 Intelligent-Tiering</p></figcaption></figure><h2 id=lifecycle-configuration-to-move-s3-objects-between-storage-classes>Lifecycle Configuration to move S3 objects between storage classes<a hidden class=anchor aria-hidden=true href=#lifecycle-configuration-to-move-s3-objects-between-storage-classes>#</a></h2><p>So now the question is: how to move all the objects to Intelligent-Tiering, old and any new ones?</p><p>Here comes Lifecycle Configuration. S3 Lifecycle also manipulates objects, but the key difference between Intelligent-Tiering and Lifecycle Configuration is that Configuration does not have the access pattern analysis as a trigger ‚Äî the only trigger for Configuration is the time (e.g., trigger in N days).</p><p>However, a trigger by time mark is precisely what&rsquo;s needed when your software does not support the custom storage classes ‚Äî you want to move objects as soon as possible to Intelligent-Tiering using Lifecycle Configuration.</p><p>Here&rsquo;s how you need to configure the Lifecycle to move all the objects to Intelligent-Tiering:</p><ul><li>Apply to all objects in the bucket</li><li>Actions:<ul><li>The 1st option ‚Äî Transition current versions of objects between storage classes</li><li>The 2nd option ‚Äî Transition noncurrent versions of objects between storage classes</li></ul></li><li>Specify Intelligent-Tiering for &ldquo;Transition current version&rdquo; and &ldquo;Transition noncurrent versions&rdquo;</li><li>Set 0 as the number of &ldquo;Days after object creation&rdquo; and &ldquo;Days after objects become noncurrent&rdquo;</li></ul><figure><img loading=lazy src=/2025/01/29/aws-s3-cost-optimization-removing-redundancy-and-implementing-intelligent-tiering/s3-lifecycle-move-to-it_hu9079021189187701173.webp alt="S3 Lifecycle Rule: Move to Intelligent-Tiering" width=800 height=1180.04><figcaption><p>S3 Lifecycle Rule: Move to Intelligent-Tiering</p></figcaption></figure><p>A zero value here does not mean an immediate change of the storage class: by design, the transition will be executed at midnight UTC from the object upload date. For existing objects, the transition will begin at midnight UTC after the lifecycle rule is applied.</p><div class=attention>I confirmed with AWS technical support that there are no indications of throttling or limitations on object access during transitions. All objects remain accessible during the transition process.</div><p>Lifecycle Configuration has a price of $0.01 per 1,000 Transition requests, which technically adds $0.00001 as the one-time cost of each object affected by the policy.</p><p>For example, in my case, moving 21 million objects to the Intelligent-Tiering class had a one-time cost of $210, but ROI for this is much more significant the next month.</p><h1 id=untrivial-termination-of-a-large-s3-bucket>Untrivial termination of a large S3 bucket<a hidden class=anchor aria-hidden=true href=#untrivial-termination-of-a-large-s3-bucket>#</a></h1><p>How hard can it be to empty and delete the bucket with 21 million objects inside? (To clarify: S3 does not allow deletion of a non-empty bucket)</p><p>The most obvious way, at first glance, would be to use the AWS S3 web Console option called &ldquo;Empty&rdquo;, right?</p><figure><img loading=lazy src=/2025/01/29/aws-s3-cost-optimization-removing-redundancy-and-implementing-intelligent-tiering/s3-empty-bucket_hu1098926438879126733.webp alt="S3 Empty Bucket" width=800 height=420.16><figcaption><p>S3 Empty Bucket</p></figcaption></figure><p>Alas, it is not that simple.</p><p>When you go that way, <strong>it is your browser who removes the objects by sending API calls to AWS</strong>. There is no background job for you. It does that efficiently, sending DELETE requests in 1000-item batches, but it does that as long as your IAM session remains active (or the browser window remains open, whatever ends first).</p><p>If your bucket has Bucket Versioning enabled, <strong>&ldquo;Empty&rdquo; action will not remove all the object versions</strong>.</p><p>So then we have two other options:</p><ul><li>Either run some script that removes all object versions (including current, older, and null versions) and delete markers.</li><li>Or set up a couple of Lifecycle Configuration rules to purge a bucket in an unattended way.</li></ul><p>If the total number of objects is relatively small, e.g., up to few hundred thousand, it is feasible to run a script that removes all object versions and delete markers.<div class=code-snippet><details><summary markdown=span>Click here to see the code snippet</summary><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#8b949e;font-style:italic>#!/usr/bin/env python3</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>sys</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>boto3</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>purge_bucket_interactive</span>(bucket_name: str) <span style=color:#ff7b72;font-weight:700>-&gt;</span> <span style=color:#79c0ff>None</span>:
</span></span><span style=display:flex><span>    <span style=color:#a5d6ff>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#a5d6ff>    Display the bucket ARN and region, warn the user, and prompt them to type
</span></span></span><span style=display:flex><span><span style=color:#a5d6ff>    &#39;YES&#39; in uppercase. If confirmed, permanently remove all object versions
</span></span></span><span style=display:flex><span><span style=color:#a5d6ff>    (including current, older, and null versions) and delete markers.
</span></span></span><span style=display:flex><span><span style=color:#a5d6ff>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#8b949e;font-style:italic># We use the S3 client to query bucket location info</span>
</span></span><span style=display:flex><span>    s3_client <span style=color:#ff7b72;font-weight:700>=</span> boto3<span style=color:#ff7b72;font-weight:700>.</span>client(<span style=color:#a5d6ff>&#34;s3&#34;</span>)
</span></span><span style=display:flex><span>    response <span style=color:#ff7b72;font-weight:700>=</span> s3_client<span style=color:#ff7b72;font-weight:700>.</span>get_bucket_location(Bucket<span style=color:#ff7b72;font-weight:700>=</span>bucket_name)
</span></span><span style=display:flex><span>    region <span style=color:#ff7b72;font-weight:700>=</span> response<span style=color:#ff7b72;font-weight:700>.</span>get(<span style=color:#a5d6ff>&#34;LocationConstraint&#34;</span>) <span style=color:#ff7b72;font-weight:700>or</span> <span style=color:#a5d6ff>&#34;us-east-1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#8b949e;font-style:italic># Construct the bucket&#39;s ARN (for standard S3 buckets, the region is not typically embedded)</span>
</span></span><span style=display:flex><span>    bucket_arn <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#79c0ff>f</span><span style=color:#a5d6ff>&#34;arn:aws:s3:::</span><span style=color:#a5d6ff>{</span>bucket_name<span style=color:#a5d6ff>}</span><span style=color:#a5d6ff>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    print(<span style=color:#79c0ff>f</span><span style=color:#a5d6ff>&#34;Bucket ARN   : </span><span style=color:#a5d6ff>{</span>bucket_arn<span style=color:#a5d6ff>}</span><span style=color:#a5d6ff>&#34;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#79c0ff>f</span><span style=color:#a5d6ff>&#34;Bucket region: </span><span style=color:#a5d6ff>{</span>region<span style=color:#a5d6ff>}</span><span style=color:#a5d6ff>&#34;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#a5d6ff>&#34;WARNING: This will permanently remove ALL VERSIONS of every object.&#34;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#a5d6ff>&#34;It is NOT REVERSIBLE!!!</span><span style=color:#79c0ff>\n</span><span style=color:#a5d6ff>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    confirm <span style=color:#ff7b72;font-weight:700>=</span> input(<span style=color:#a5d6ff>&#34;Type &#39;YES&#39; in uppercase to proceed with the permanent removal: &#34;</span>)<span style=color:#ff7b72;font-weight:700>.</span>strip()
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>if</span> confirm <span style=color:#ff7b72;font-weight:700>!=</span> <span style=color:#a5d6ff>&#34;YES&#34;</span>:
</span></span><span style=display:flex><span>        print(<span style=color:#a5d6ff>&#34;Aborted. No changes made.&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#ff7b72>return</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#8b949e;font-style:italic># Proceed with deletion</span>
</span></span><span style=display:flex><span>    print(<span style=color:#a5d6ff>&#34;Starting purge... This might take a while.&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#8b949e;font-style:italic># Use the S3 resource to delete all versions</span>
</span></span><span style=display:flex><span>    s3 <span style=color:#ff7b72;font-weight:700>=</span> boto3<span style=color:#ff7b72;font-weight:700>.</span>resource(<span style=color:#a5d6ff>&#34;s3&#34;</span>)
</span></span><span style=display:flex><span>    bucket <span style=color:#ff7b72;font-weight:700>=</span> s3<span style=color:#ff7b72;font-weight:700>.</span>Bucket(bucket_name)
</span></span><span style=display:flex><span>    bucket<span style=color:#ff7b72;font-weight:700>.</span>object_versions<span style=color:#ff7b72;font-weight:700>.</span>all()<span style=color:#ff7b72;font-weight:700>.</span>delete()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    print(<span style=color:#a5d6ff>&#34;All object versions and delete markers have been removed.&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>if</span> __name__ <span style=color:#ff7b72;font-weight:700>==</span> <span style=color:#a5d6ff>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>if</span> len(sys<span style=color:#ff7b72;font-weight:700>.</span>argv) <span style=color:#ff7b72;font-weight:700>&lt;</span> <span style=color:#a5d6ff>2</span>:
</span></span><span style=display:flex><span>        print(<span style=color:#79c0ff>f</span><span style=color:#a5d6ff>&#34;Usage: </span><span style=color:#a5d6ff>{</span>sys<span style=color:#ff7b72;font-weight:700>.</span>argv[<span style=color:#a5d6ff>0</span>]<span style=color:#a5d6ff>}</span><span style=color:#a5d6ff> &lt;bucket_name&gt;&#34;</span>)
</span></span><span style=display:flex><span>        sys<span style=color:#ff7b72;font-weight:700>.</span>exit(<span style=color:#a5d6ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    bucket_to_purge <span style=color:#ff7b72;font-weight:700>=</span> sys<span style=color:#ff7b72;font-weight:700>.</span>argv[<span style=color:#a5d6ff>1</span>]
</span></span><span style=display:flex><span>    purge_bucket_interactive(bucket_to_purge)
</span></span></code></pre></div></details></div></p><div class=attention>But if you have millions of objects, you&rsquo;d better use Lifecycle Configuration rules to empty a version-enabled bucket.</div><p>First, you need to pause the versioning. Then, create two Lifecycle Configuration rules.</p><p><strong>The first rule will delete all versions of the objects:</strong></p><ul><li>Apply to all objects in the bucket</li><li>Actions:<ul><li>The 3rd option ‚Äî &ldquo;Expire current versions of objects&rdquo;</li><li>The 4th option ‚Äî &ldquo;Permanently delete previous versions of objects&rdquo;</li></ul></li><li>The number of days you would like the current version to expire, to do as soon as possible, enter 1 in the text box. That makes it &ldquo;1 day&rdquo;.</li><li>For the days after which the noncurrent versions will be permanently deleted, enter 1 in the text box. That makes it &ldquo;1 day&rdquo; of being noncurrent.</li></ul><figure><img loading=lazy src=/2025/01/29/aws-s3-cost-optimization-removing-redundancy-and-implementing-intelligent-tiering/s3-lifecycle-delete-versions_hu18354963282920142177.webp alt="S3 Lifecycle Rule: Delete Versions" width=800 height=1000.66><figcaption><p>S3 Lifecycle Rule: Delete Versions</p></figcaption></figure><p>Due to how S3 versioning works, a special DELETE marker will be created for each object processed by the first rule. To handle the delete markers, you must create a new lifecycle rule, as you won‚Äôt be able to select the option to delete the &ldquo;delete markers&rdquo; in the first rule.</p><p><strong>For the second lifecycle rule</strong>, the steps are similar, and the only difference is that you must select the 5th and last option: &ldquo;Delete expired object delete markers or incomplete multipart uploads.&rdquo; Select both the options ‚Äî &ldquo;Delete expired object delete markers&rdquo; and &ldquo;Delete incomplete multipart uploads&rdquo; ‚Äî and set it to 1 day.</p><figure><img loading=lazy src=/2025/01/29/aws-s3-cost-optimization-removing-redundancy-and-implementing-intelligent-tiering/s3-lifecycle-delete-markers_hu15721420798360840597.webp alt="S3 Lifecycle Rule: Delete Markers" width=800 height=1057.09><figcaption><p>S3 Lifecycle Rule: Delete Markers</p></figcaption></figure><div class=attention>S3 Lifecycle operations are asynchronous, and it may take some time for the Lifecycle to delete the objects in your S3 bucket. However, at midnight UTC, once the S3 objects are marked for expiration, you are no longer charged for storing that objects, and S3 will do the rest.</div><h1 id=implementation-results>Implementation Results<a hidden class=anchor aria-hidden=true href=#implementation-results>#</a></h1><ul><li>Cost reduction: $120,000 baseline or $270,000 if the 7% growth rate remains</li><li>One-time transition cost: $210</li><li>Implementation time: 2 days</li><li>Storage class transition completed in 1 day</li></ul><h1 id=key-takeaways>Key Takeaways<a hidden class=anchor aria-hidden=true href=#key-takeaways>#</a></h1><ul><li>Challenge your architecture decisions overtime.</li><li>Challenge redundancy ‚Äî is it providing real value?</li><li>Regular cost reviews can be a source of quick wins.</li><li>Let automated solutions do the job, and do not overengineer things.</li><li>AWS Simple Storage Service, despite its name, might be tricky, but AWS has a bunch of tools to help you.</li></ul><p>S3 Lifecycle rules proved their worth twice: first by automating our transition to Intelligent-Tiering despite Artifactory&rsquo;s limitations and then by cleaning up millions of versioned objects without operational overhead.</p></div><footer class=post-footer><div class=substack-embedded-container><h3>Subscribe to blog updates!</h3><iframe title=Substack class=substack-embedded-iframe src=https://devdosvid.substack.com/embed height=250 loading=lazy></iframe></div><script src=https://giscus.app/client.js data-repo=vasylenko/devdosvid.blog data-repo-id="MDEwOlJlcG9zaXRvcnkyNDUyNTMxODE=" data-category=General data-category-id=DIC_kwDODp5EPc4CA59c data-mapping=title data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-lang=en data-theme=light crossorigin=anonymous async></script><div class=license><p>This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-nd/4.0/>Creative Commons Attribution-NoDerivatives 4.0 International License</a></p></div><nav class=paginav><a class=next href=https://devdosvid.blog/2024/06/03/unpacking-aws-outages-system-design-lessons-from-post-event-summaries/><span class=title>Next ¬ª</span><br><span>Unpacking AWS Outages: System Design Lessons from Post-Event Summaries</span></a></nav></footer></article></main><footer class=footer><div style=width:10em;margin:auto><p style=background-color:#0082ca;color:#fff>From Ukrainian</p><p style=background-color:#ffb548;color:#fff>with love ‚ù§Ô∏è</p></div><span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>