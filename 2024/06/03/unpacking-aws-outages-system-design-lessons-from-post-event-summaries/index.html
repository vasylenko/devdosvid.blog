<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Unpacking AWS Outages: System Design Lessons from Post-Event Summaries | devDosvid blog</title>
<meta name=keywords content="aws,cloud,monitoring,redundancy,system design,post event reports,incident response,incident management,postmortems,cascade failures,lessons from AWS"><meta name=description content="Explore AWS outage case studies, uncovering essential strategies for building resilient systems by understanding dependencies and preventing cascading failures"><meta name=author content="Serhii Vasylenko"><link rel=canonical href=https://devdosvid.blog/2024/06/03/unpacking-aws-outages-system-design-lessons-from-post-event-summaries/><link crossorigin=anonymous href=/assets/css/stylesheet.5646840c6f09d9c80952a9aac690950f7d6243912ac8bdf52882c5d1990edd8c.css integrity="sha256-VkaEDG8J2cgJUqmqxpCVD31iQ5EqyL31KILF0ZkO3Yw=" rel="preload stylesheet" as=style><link rel=icon href=https://devdosvid.blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://devdosvid.blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://devdosvid.blog/favicon-32x32.png><link rel=apple-touch-icon href=https://devdosvid.blog/apple-touch-icon.png><link rel=mask-icon href=https://devdosvid.blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://devdosvid.blog/2024/06/03/unpacking-aws-outages-system-design-lessons-from-post-event-summaries/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><style>@font-face{font-family:albert sans;font-style:normal;font-display:swap;font-weight:300;src:url(/assets/fonts/AlbertSans-Light.woff2)format('woff');unicode-range:U+??,U+131,U+152-153,U+2BB-2BC,U+2C6,U+2DA,U+2DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:albert sans;font-style:italic;font-display:swap;font-weight:300;src:url(/assets/fonts/AlbertSans-LightItalic.woff2)format('woff');unicode-range:U+??,U+131,U+152-153,U+2BB-2BC,U+2C6,U+2DA,U+2DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:albert sans;font-style:normal;font-display:swap;font-weight:400;src:url(/assets/fonts/AlbertSans-Regular.woff2)format('woff');unicode-range:U+??,U+131,U+152-153,U+2BB-2BC,U+2C6,U+2DA,U+2DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:albert sans;font-style:italic;font-display:swap;font-weight:400;src:url(/assets/fonts/AlbertSans-Italic.woff2)format('woff');unicode-range:U+??,U+131,U+152-153,U+2BB-2BC,U+2C6,U+2DA,U+2DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:albert sans;font-style:normal;font-display:swap;font-weight:500;src:url(/assets/fonts/AlbertSans-Medium.woff2)format('woff');unicode-range:U+??,U+131,U+152-153,U+2BB-2BC,U+2C6,U+2DA,U+2DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:albert sans;font-style:italic;font-display:swap;font-weight:500;src:url(/assets/fonts/AlbertSans-MediumItalic.woff2)format('woff');unicode-range:U+??,U+131,U+152-153,U+2BB-2BC,U+2C6,U+2DA,U+2DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:albert sans;font-style:normal;font-display:swap;font-weight:600;src:url(/assets/fonts/AlbertSans-SemiBold.woff2)format('woff');unicode-range:U+??,U+131,U+152-153,U+2BB-2BC,U+2C6,U+2DA,U+2DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:albert sans;font-style:italic;font-display:swap;font-weight:600;src:url(/assets/fonts/AlbertSans-SemiBoldItalic.woff2)format('woff');unicode-range:U+??,U+131,U+152-153,U+2BB-2BC,U+2C6,U+2DA,U+2DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style><meta name=twitter:creator content="@vasylenko"><meta name=twitter:site content="@vasylenko"><script async src=https://scripts.simpleanalyticscdn.com/latest.js></script><meta property="og:url" content="https://devdosvid.blog/2024/06/03/unpacking-aws-outages-system-design-lessons-from-post-event-summaries/"><meta property="og:site_name" content="devDosvid blog"><meta property="og:title" content="Unpacking AWS Outages: System Design Lessons from Post-Event Summaries"><meta property="og:description" content="Explore AWS outage case studies, uncovering essential strategies for building resilient systems by understanding dependencies and preventing cascading failures"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-06-03T00:52:54+02:00"><meta property="article:modified_time" content="2025-01-31T00:20:04+01:00"><meta property="og:image" content="https://devdosvid.blog/2024/06/03/unpacking-aws-outages-system-design-lessons-from-post-event-summaries/cover-image.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://devdosvid.blog/2024/06/03/unpacking-aws-outages-system-design-lessons-from-post-event-summaries/cover-image.png"><meta name=twitter:title content="Unpacking AWS Outages: System Design Lessons from Post-Event Summaries"><meta name=twitter:description content="Explore AWS outage case studies, uncovering essential strategies for building resilient systems by understanding dependencies and preventing cascading failures"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://devdosvid.blog/posts/"},{"@type":"ListItem","position":2,"name":"Unpacking AWS Outages: System Design Lessons from Post-Event Summaries","item":"https://devdosvid.blog/2024/06/03/unpacking-aws-outages-system-design-lessons-from-post-event-summaries/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Unpacking AWS Outages: System Design Lessons from Post-Event Summaries","name":"Unpacking AWS Outages: System Design Lessons from Post-Event Summaries","description":"Explore AWS outage case studies, uncovering essential strategies for building resilient systems by understanding dependencies and preventing cascading failures","keywords":["aws","cloud","monitoring","redundancy","system design","post event reports","incident response","incident management","postmortems","cascade failures","lessons from AWS"],"articleBody":"In this blog post, I‚Äôm excited to share some valuable system design insights drawn from AWS‚Äôs post-event summaries on major outages.\nSystem design is a topic I‚Äôm especially passionate about‚Äîit‚Äôs even my favorite interview question at Grammarly, and I love to conduct those interviews. This fascination led me to thoroughly analyze AWS‚Äôs PES in search of the most interesting cases.\nLearning from mistakes is essential. Yet it‚Äôs precious to learn from others‚Äô mistakes because they come for free (for you, but not for their owners). The AWS team is doing a great job sharing their Post-Event Summaries because they not only demonstrate the open engineering culture but also help others.\nFrom the sixteen reports available at the time of writing, I‚Äôve selected the four most captivating ones. Each presents unexpected challenges and turns of events, along with valuable outcomes we can learn from. Let‚Äôs explore these intriguing reports and uncover the key strategies for building more resilient systems.\nRemirroring Storm The April 21, 2011, Amazon EC2/EBS event1 in the US East Region provides valuable insights into dependency management and the dangers of cascading failures.\nA network configuration change, promoted as a part of normal scaling activity, set off a cascade of failures in the Amazon Elastic Block Store system. The intention was simple ‚Äî to upgrade the capacity of the primary network. This operation involved a traffic shift between underlying networks, but it was executed incorrectly so that change caused many EBS nodes to disconnect from their replicas. When these nodes reconnected, they tried to replicate their data on other nodes, quickly overwhelming the EBS cluster‚Äôs capacity. This surge, or as AWS called it ‚Äúremirroring storm,‚Äù left many EBS volumes ‚Äústuck,‚Äù unable to process read and write operations.\nBlast Radius: Initially, the issue affected only a single Availability Zone in the US East Region, and about 13% of the volumes were in this ‚Äústuck‚Äù state. However, the outage soon spread to the entire region. The EBS control plane, responsible for handling API requests, was dependent on the degraded EBS cluster. The increased traffic from the remirroring storm overwhelmed the control plane, making it intermittently unavailable and affecting users across the region.\nAffected Services and Processes:\nEC2: Users faced difficulties launching new EBS-backed instances and managing existing ones. EBS: Many volumes became ‚Äústuck,‚Äù rendering them inaccessible and impacting EC2 instances dependent on them. RDS: As a service dependent on EBS, some RDS databases, particularly single-AZ deployments, became inaccessible due to the EBS volume issues. This incident underscores the importance of building resilient systems. The EBS control plane‚Äôs dependence on a single Availability Zone and the absence of back-off mechanisms in the remirroring process were critical factors in the cascading failures.\nElectrical Storm The June 29, 2012, AWS services event2 in the US East Region exemplifies how a localized power outage can trigger a region-wide service disruption due to complex dependencies.\nA severe electrical storm caused a power outage in a single data center, impacting a small portion of AWS resources in the US East Region ‚Äî a single-digit percentage of the total resources in the region (as of the date of the incident).\nBlast Radius: The power outage was initially confined to the affected Availability Zone. However, it soon led to degrading service control planes that manage resources across the entire region. While these control planes aren‚Äôt required for ongoing resource usage, their degradation hindered users‚Äô ability to respond to the outage, such using the AWS console to try moving resources to other Availability Zones.\nAffected Services and Processes:\nEC2 and EBS: Approximately 7% of EC2 instances and a similar proportion of EBS volumes in the region were offline until power was restored and systems restarted. The control planes for both services were significantly impacted, making it difficult for users to launch new instances, create EBS volumes, or attach volumes in any Availability Zone within the region. ELB: Although the direct impact was limited to ELBs within the affected data center, the service‚Äôs inability to process new requests quickly hampered recovery for users trying to replace lost EC2 capacity in other Availability Zones. RDS: Many Single-AZ databases in the affected zone became inaccessible due to their dependent EBS volumes being affected. A few Multi-AZ RDS instances, designed for redundancy, also failed to failover automatically due to a software bug triggered by the specific server shutdown sequences during the power outage. Even though we host our applications in the cloud and power outages may not be a primary concern when starting new projects, this event underscores the critical importance of designing fault-tolerant systems. It also highlights the potential for cascading failures when a small percentage of infrastructure is impacted. Dependencies on control planes and the interconnected nature of services can significantly amplify the impact of localized outages.\nSimple Point of Failure Another excellent example of how small can quickly become big ‚Äî is the Amazon SimpleDB service disruption on June 13, 2014, in the US East Region3. This incident demonstrates how a seemingly minor issue can escalate into a significant disruption due to dependencies on a centralized service.\nA power outage in a single data center caused multiple storage nodes to become unavailable. This sudden failure led to a spike in load on the internal lock service, which manages node responsibility for data and metadata. And while this lock services is replicated accross multiple data centers, the load spike wat too sudden and too high.\nBlast Radius: Initially, the impact was confined to the storage nodes in the affected data center. However, the increased load on the centralized lock service, crucial for all SimpleDB operations, caused cascading failures that affected the entire service.\nAffected Services and Processes:\nSimpleDB: The service became unavailable for all API calls, except for a small fraction of eventually consistent read calls, because the storage and metadata nodes couldn‚Äôt renew their membership with the overloaded lock service. This unavailability prevented users from accessing and managing their data. This outage highlights the critical importance of addressing the single point of failure when designing systems. The centralized nature of the lock service, intended for coordination, became a single point of failure. A more distributed or load-balanced approach for the lock service could have mitigated the impact of the simultaneous node failures.\nScaling for the better In search of absolute The Amazon Kinesis event in the US East Region on November 25, 20204, is a perfect example of how adding capacity can unexpectedly trigger a cascade of failures due to unforeseen dependencies and resource limitations. What could possibly go wrong by adding more nodes to the cluster? Scaling horizontally is best practice, right? Well, it depends.\nA small capacity addition to the front-end fleet of the Kinesis service led to unexpected behavior in the front-end servers responsible for routing requests.\nThese newly added servers exceeded the maximum allowed threads due to operating system configuration: each front-end server creates operating system threads for each of the other servers in the front-end fleet ‚Äî this is needed for services to learn about new servers added to the cluster. Eventually, all this caused cache construction failures and prevented servers from routing requests to the back-end clusters.\nBlast Radius: Although the initial trigger was a capacity addition intended to enhance performance, the resulting issue affected the entire Kinesis service in the US East Region. The dependency of many AWS services on Kinesis amplified the impact significantly.\nAffected Services and Processes:\nKinesis: Customers experienced failures and increased latencies when putting and getting Kinesis records, rendering the service unusable for real-time data processing. Cognito: As a dependent service on Kinesis, Cognito faced elevated API failures and increased latencies for user pools and identity pools. This disruption prevented external users from authenticating or obtaining temporary AWS credentials. CloudWatch: Kinesis Data Streams are used by CloudWatch to process metrics and log data. The event caused increased error rates and latencies for CloudWatch APIs (PutMetricData and PutLogEvents), with alarms transitioning to an INSUFFICIENT_DATA state, hindering monitoring and alerting capabilities. Auto Scaling and Lambda: These services rely on CloudWatch metrics, so they were indirectly affected, too. Reactive Auto Scaling policies experienced delays, and Lambda function invocations encountered increased error rates due to memory contention caused by backlogged CloudWatch metric data. This incident highlights the importance of thoroughly understanding dependencies, resource limitations, and potential failure points when making changes to a system, even those intended to improve capacity or performance. Robust testing and monitoring are crucial to identify and mitigate such unexpected behaviors.\nFinal Thoughts The AWS outage reports reveal a fundamental truth about system design: complexity and interdependency can be both our greatest strengths and our most significant vulnerabilities.\nAs we build and scale our systems, let‚Äôs strive for simplicity, resilience, and a thorough understanding of our dependencies.\nTo ensure our systems are prepared for the unexpected, it‚Äôs crucial to internalize and act upon the lessons these incidents teach us. Here are some key takeaways to guide us in this:\nEmbrace Resilience: Design systems with robust fault-tolerance mechanisms to handle unexpected failures without cascading effects. Understand Dependencies: Map out and regularly review your system‚Äôs dependencies to identify and mitigate potential single points of failure. Continuous Learning: Analyze past incidents, both your own and industry-wide, to gain insights and improve your system design. Proactive Monitoring: Implement comprehensive monitoring and alerting to detect and address issues before they escalate. Thorough Testing: Regularly test your systems under various failure scenarios to ensure they can withstand real-world conditions. The path to reliable systems is paved with continuous learning and adaptation. Let‚Äôs embrace these lessons and push the boundaries of what our systems can achieve, ensuring that we are always prepared for the unexpected. üôÇ\nSummary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region, April 21, 2011¬†‚Ü©Ô∏é\nSummary of the AWS Service Event in the US East Region, June 29, 2012¬†‚Ü©Ô∏é\nSummary of the Amazon SimpleDB Service Disruption, June 13, 2014.¬†‚Ü©Ô∏é\nSummary of the Amazon Kinesis Event in the Northern Virginia (US-EAST-1) Region, November 25, 2020¬†‚Ü©Ô∏é\n","wordCount":"1666","inLanguage":"en","image":"https://devdosvid.blog/2024/06/03/unpacking-aws-outages-system-design-lessons-from-post-event-summaries/cover-image.png","datePublished":"2024-06-03T00:52:54+02:00","dateModified":"2025-01-31T00:20:04+01:00","author":{"@type":"Person","name":"Serhii Vasylenko"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://devdosvid.blog/2024/06/03/unpacking-aws-outages-system-design-lessons-from-post-event-summaries/"},"publisher":{"@type":"Person","name":"devDosvid blog","logo":{"@type":"ImageObject","url":"https://devdosvid.blog/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://devdosvid.blog/ accesskey=h title="üè† Home (Alt + H)">üè† Home</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://devdosvid.blog/index.xml title="RSS Feed"><span>RSS Feed</span></a></li><li><a href=https://devdosvid.blog/archive title="All posts"><span>All posts</span></a></li><li><a href=https://devdosvid.blog/about/ title="About me"><span>About me</span></a></li><li><a href=https://devdosvid.blog/series title="Post Series"><span>Post Series</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://devdosvid.blog/>Home</a>&nbsp;¬ª&nbsp;<a href=https://devdosvid.blog/posts/>Posts</a></div><h1 class=post-title>Unpacking AWS Outages: System Design Lessons from Post-Event Summaries</h1><div class=post-description>Explore AWS outage case studies, uncovering essential strategies for building resilient systems by understanding dependencies and preventing cascading failures</div><div class=post-meta><span title='2024-06-03 00:52:54 +0200 +0200'>June 3, 2024</span>&nbsp;¬∑&nbsp;Serhii Vasylenko&nbsp;|&nbsp;<a href=mailto:contact@devdosvid.blog rel="noopener noreferrer" target=_blank>Contact me</a></div></header><figure class=entry-cover><img srcset="https://devdosvid.blog/2024/06/03/unpacking-aws-outages-system-design-lessons-from-post-event-summaries/cover-image_hu10462688336419420812.png 360w ,https://devdosvid.blog/2024/06/03/unpacking-aws-outages-system-design-lessons-from-post-event-summaries/cover-image_hu11327853157550214838.png 480w ,https://devdosvid.blog/2024/06/03/unpacking-aws-outages-system-design-lessons-from-post-event-summaries/cover-image_hu29045341455922895.png 720w ,https://devdosvid.blog/2024/06/03/unpacking-aws-outages-system-design-lessons-from-post-event-summaries/cover-image_hu6797415566644699677.png 1080w ,https://devdosvid.blog/2024/06/03/unpacking-aws-outages-system-design-lessons-from-post-event-summaries/cover-image_hu15914611664692772699.png 1500w ,https://devdosvid.blog/2024/06/03/unpacking-aws-outages-system-design-lessons-from-post-event-summaries/cover-image.png 1792w" sizes="(min-width: 768px) 720px, 100vw" src=https://devdosvid.blog/2024/06/03/unpacking-aws-outages-system-design-lessons-from-post-event-summaries/cover-image.png alt="preventing cascading failures" width=1792 height=1024></figure><div class=post-content><p>In this blog post, I‚Äôm excited to share some valuable system design insights drawn from AWS‚Äôs post-event summaries on major outages.</p><p>System design is a topic I‚Äôm especially passionate about‚Äîit‚Äôs even my favorite interview question at Grammarly, and I love to conduct those interviews. This fascination led me to thoroughly analyze AWS‚Äôs PES in search of the most interesting cases.</p><p>Learning from mistakes is essential. Yet it&rsquo;s precious to learn from others&rsquo; mistakes because they come for free (for you, but not for their owners). The AWS team is doing a great job sharing their Post-Event Summaries because they not only demonstrate the open engineering culture but also help others.</p><p>From the sixteen reports available at the time of writing, I‚Äôve selected the four most captivating ones. Each presents unexpected challenges and turns of events, along with valuable outcomes we can learn from. Let&rsquo;s explore these intriguing reports and uncover the key strategies for building more resilient systems.</p><h2 id=remirroring-storm>Remirroring Storm<a hidden class=anchor aria-hidden=true href=#remirroring-storm>#</a></h2><p>The April 21, 2011, Amazon EC2/EBS event<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> in the US East Region provides valuable insights into dependency management and the dangers of cascading failures.</p><p>A network configuration change, promoted as a part of normal scaling activity, set off a cascade of failures in the Amazon Elastic Block Store system. The intention was simple ‚Äî to upgrade the capacity of the primary network. This operation involved a traffic shift between underlying networks, but it was executed incorrectly so that change caused many EBS nodes to disconnect from their replicas. When these nodes reconnected, they tried to replicate their data on other nodes, quickly overwhelming the EBS cluster‚Äôs capacity. This surge, or as AWS called it ‚Äúremirroring storm,‚Äù left many EBS volumes ‚Äústuck,‚Äù unable to process read and write operations.</p><p><strong>Blast Radius</strong>: Initially, the issue affected only a single Availability Zone in the US East Region, and about 13% of the volumes were in this ‚Äústuck‚Äù state. However, the outage soon spread to the entire region. The EBS control plane, responsible for handling API requests, was dependent on the degraded EBS cluster. The increased traffic from the remirroring storm overwhelmed the control plane, making it intermittently unavailable and affecting users across the region.</p><p><strong>Affected Services and Processes</strong>:</p><ul><li>EC2: Users faced difficulties launching new EBS-backed instances and managing existing ones.</li><li>EBS: Many volumes became ‚Äústuck,‚Äù rendering them inaccessible and impacting EC2 instances dependent on them.</li><li>RDS: As a service dependent on EBS, some RDS databases, particularly single-AZ deployments, became inaccessible due to the EBS volume issues.</li></ul><p>This incident underscores the importance of building resilient systems. The EBS control plane‚Äôs dependence on a single Availability Zone and the absence of back-off mechanisms in the remirroring process were critical factors in the cascading failures.</p><h2 id=electrical-storm>Electrical Storm<a hidden class=anchor aria-hidden=true href=#electrical-storm>#</a></h2><p>The June 29, 2012, AWS services event<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> in the US East Region exemplifies how a localized power outage can trigger a region-wide service disruption due to complex dependencies.</p><p>A severe electrical storm caused a power outage in a single data center, impacting a small portion of AWS resources in the US East Region ‚Äî a single-digit percentage of the total resources in the region (as of the date of the incident).</p><p><strong>Blast Radius</strong>: The power outage was initially confined to the affected Availability Zone. However, it soon led to degrading service control planes that manage resources across the entire region. While these control planes aren&rsquo;t required for ongoing resource usage, their degradation hindered users&rsquo; ability to respond to the outage, such using the AWS console to try moving resources to other Availability Zones.</p><p><strong>Affected Services and Processes</strong>:</p><ul><li>EC2 and EBS: Approximately 7% of EC2 instances and a similar proportion of EBS volumes in the region were offline until power was restored and systems restarted. The control planes for both services were significantly impacted, making it difficult for users to launch new instances, create EBS volumes, or attach volumes in any Availability Zone within the region.</li><li>ELB: Although the direct impact was limited to ELBs within the affected data center, the service&rsquo;s inability to process new requests quickly hampered recovery for users trying to replace lost EC2 capacity in other Availability Zones.</li><li>RDS: Many Single-AZ databases in the affected zone became inaccessible due to their dependent EBS volumes being affected. A few Multi-AZ RDS instances, designed for redundancy, also failed to failover automatically due to a software bug triggered by the specific server shutdown sequences during the power outage.</li></ul><p>Even though we host our applications in the cloud and power outages may not be a primary concern when starting new projects, this event underscores the critical importance of designing fault-tolerant systems. It also highlights the potential for cascading failures when a small percentage of infrastructure is impacted. Dependencies on control planes and the interconnected nature of services can significantly amplify the impact of localized outages.</p><h2 id=simple-point-of-failure>Simple Point of Failure<a hidden class=anchor aria-hidden=true href=#simple-point-of-failure>#</a></h2><p>Another excellent example of how small can quickly become big ‚Äî is the Amazon SimpleDB service disruption on June 13, 2014, in the US East Region<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>. This incident demonstrates how a seemingly minor issue can escalate into a significant disruption due to dependencies on a centralized service.</p><p>A power outage in a single data center caused multiple storage nodes to become unavailable. This sudden failure led to a spike in load on the internal lock service, which manages node responsibility for data and metadata. And while this lock services is replicated accross multiple data centers, the load spike wat too sudden and too high.</p><p><strong>Blast Radius</strong>: Initially, the impact was confined to the storage nodes in the affected data center. However, the increased load on the centralized lock service, crucial for all SimpleDB operations, caused cascading failures that affected the entire service.</p><p><strong>Affected Services and Processes</strong>:</p><ul><li>SimpleDB: The service became unavailable for all API calls, except for a small fraction of eventually consistent read calls, because the storage and metadata nodes couldn‚Äôt renew their membership with the overloaded lock service. This unavailability prevented users from accessing and managing their data.</li></ul><p>This outage highlights the critical importance of addressing the single point of failure when designing systems. The centralized nature of the lock service, intended for coordination, became a single point of failure. A more distributed or load-balanced approach for the lock service could have mitigated the impact of the simultaneous node failures.</p><h2 id=scaling-for-the-better>Scaling for the better<a hidden class=anchor aria-hidden=true href=#scaling-for-the-better>#</a></h2><figure><img loading=lazy src=/2024/06/03/unpacking-aws-outages-system-design-lessons-from-post-event-summaries/scaling-for-the-better_hu4237966339864993369.webp alt="In search of absolute" width=500 height=500><figcaption><p>In search of absolute</p></figcaption></figure><p>The Amazon Kinesis event in the US East Region on November 25, 2020<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>, is a perfect example of how adding capacity can unexpectedly trigger a cascade of failures due to unforeseen dependencies and resource limitations. What could possibly go wrong by adding more nodes to the cluster? Scaling horizontally is best practice, right? Well, it depends.</p><p>A small capacity addition to the front-end fleet of the Kinesis service led to unexpected behavior in the front-end servers responsible for routing requests.</p><p>These newly added servers exceeded the maximum allowed threads due to operating system configuration: each front-end server creates operating system threads for each of the other servers in the front-end fleet ‚Äî this is needed for services to learn about new servers added to the cluster. Eventually, all this caused cache construction failures and prevented servers from routing requests to the back-end clusters.</p><p><strong>Blast Radius</strong>: Although the initial trigger was a capacity addition intended to enhance performance, the resulting issue affected the entire Kinesis service in the US East Region. The dependency of many AWS services on Kinesis amplified the impact significantly.</p><p><strong>Affected Services and Processes</strong>:</p><ul><li>Kinesis: Customers experienced failures and increased latencies when putting and getting Kinesis records, rendering the service unusable for real-time data processing.</li><li>Cognito: As a dependent service on Kinesis, Cognito faced elevated API failures and increased latencies for user pools and identity pools. This disruption prevented external users from authenticating or obtaining temporary AWS credentials.</li><li>CloudWatch: Kinesis Data Streams are used by CloudWatch to process metrics and log data. The event caused increased error rates and latencies for CloudWatch APIs (PutMetricData and PutLogEvents), with alarms transitioning to an INSUFFICIENT_DATA state, hindering monitoring and alerting capabilities.</li><li>Auto Scaling and Lambda: These services rely on CloudWatch metrics, so they were indirectly affected, too. Reactive Auto Scaling policies experienced delays, and Lambda function invocations encountered increased error rates due to memory contention caused by backlogged CloudWatch metric data.</li></ul><p>This incident highlights the importance of thoroughly understanding dependencies, resource limitations, and potential failure points when making changes to a system, even those intended to improve capacity or performance. Robust testing and monitoring are crucial to identify and mitigate such unexpected behaviors.</p><h2 id=final-thoughts>Final Thoughts<a hidden class=anchor aria-hidden=true href=#final-thoughts>#</a></h2><p>The AWS outage reports reveal a fundamental truth about system design: <strong>complexity and interdependency can be both our greatest strengths and our most significant vulnerabilities</strong>.</p><p>As we build and scale our systems, let&rsquo;s strive for simplicity, resilience, and a thorough understanding of our dependencies.</p><p>To ensure our systems are prepared for the unexpected, it&rsquo;s crucial to internalize and act upon the lessons these incidents teach us. Here are some key takeaways to guide us in this:</p><ol><li><strong>Embrace Resilience</strong>: Design systems with robust fault-tolerance mechanisms to handle unexpected failures without cascading effects.</li><li><strong>Understand Dependencies</strong>: Map out and regularly review your system&rsquo;s dependencies to identify and mitigate potential single points of failure.</li><li><strong>Continuous Learning</strong>: Analyze past incidents, both your own and industry-wide, to gain insights and improve your system design.</li><li><strong>Proactive Monitoring</strong>: Implement comprehensive monitoring and alerting to detect and address issues before they escalate.</li><li><strong>Thorough Testing</strong>: Regularly test your systems under various failure scenarios to ensure they can withstand real-world conditions.</li></ol><p>The path to reliable systems is paved with continuous learning and adaptation. Let&rsquo;s embrace these lessons and push the boundaries of what our systems can achieve, ensuring that we are always prepared for the unexpected. üôÇ</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://aws.amazon.com/message/65648/>Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region, April 21, 2011</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://aws.amazon.com/message/67457/>Summary of the AWS Service Event in the US East Region, June 29, 2012</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://aws.amazon.com/message/65649/>Summary of the Amazon SimpleDB Service Disruption, June 13, 2014.</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p><a href=https://aws.amazon.com/message/11201/>Summary of the Amazon Kinesis Event in the Northern Virginia (US-EAST-1) Region, November 25, 2020</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><div class=substack-embedded-container><h3>Subscribe to blog updates!</h3><iframe title=Substack class=substack-embedded-iframe src=https://devdosvid.substack.com/embed height=250 loading=lazy></iframe></div><script src=https://giscus.app/client.js data-repo=vasylenko/devdosvid.blog data-repo-id="MDEwOlJlcG9zaXRvcnkyNDUyNTMxODE=" data-category=General data-category-id=DIC_kwDODp5EPc4CA59c data-mapping=title data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-lang=en data-theme=light crossorigin=anonymous async></script><ul class=post-tags></ul><div class=license><p>This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-nd/4.0/>Creative Commons Attribution-NoDerivatives 4.0 International License</a></p></div><nav class=paginav><a class=next href=https://devdosvid.blog/2024/04/16/a-deep-dive-into-terraform-static-code-analysis-tools-features-and-comparisons/><span class=title>Next ¬ª</span><br><span>A Deep Dive Into Terraform Static Code Analysis Tools: Features and Comparisons</span></a></nav></footer></article></main><footer class=footer><div style=width:10em;margin:auto><p style=background-color:#0082ca;color:#fff>From Ukrainian</p><p style=background-color:#ffb548;color:#fff>with love ‚ù§Ô∏è</p></div><span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>