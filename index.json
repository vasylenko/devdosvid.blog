[{"content":"It‚Äôs been almost a year since I started using macOS EC2 instances on AWS: there were ups and downs in service offerings and a lot of discoveries with macOS AMI build automation.\nAnd I like this small but so helpful update of EC2 service very much: with mac1.metal instances, seamless integration of Apple-oriented CI/CD with other AWS infrastructure could finally happen.\nWhile management of a single mac1.metal node (or a tiny number of ones) is not a big deal (especially when Dedicated Host support was added to Terraform provider), governing the fleet of instances is still complicated. Or it has been complicated until recent days.\nOfficial / Unofficial Auto Scaling for macOS With a growing number of instances, the following challenges arise:\n Scale mac1.metal instances horizontally Automatically allocate and release Dedicated Hosts needed for instances Automatically replace unhealthy instances  If you have worked with AWS before, you know that Auto Scaling Group service can solve such things.\nHowever, official documentation (as of October 2021) states: ‚ÄúYou cannot use Mac instances with Amazon EC2 Auto Scaling‚Äù.\nBut in fact, you can.\nCombining services to get real power So how does all that work?\nLet‚Äôs review the diagram that illustrates the interconnection between involved services:\n  With the help of Licence Manager service and Launch Templates, you can set up EC2 Auto Scaling Group for mac1.metal and leave the automated instance provisioning to the service.\nLicense Configuration First, you need to create a License Configuration so that the Host resource group can allocate the hots.\nGo to AWS License Manager -\u0026gt; Customer managed licenses -\u0026gt; Create customer-managed license.\nSpecify Sockets as the Licence type. You may skip setting the Number of Sockets. However, the actual limit of mac1.metal instances per account is regulated by Service Quota. The default number of mac instances allowed per account is 3. Therefore, consider increasing this to a more significant number.\n  Host resource group Second, create the Host resource group: AWS License Manager -\u0026gt; Host resource groups -\u0026gt; Create host resource group.\nWhen creating the Host resource group, check ‚ÄúAllocate hosts automatically‚Äù and ‚ÄúRelease hosts automatically‚Äù but leave ‚ÄúRecover hosts automatically‚Äù unchecked. Dedicated Host does not support host recovery for mac1.metal. However, Auto Scaling Group will maintain the desired number of instances if one fails the health check (which assumes the case of host failure as well).\nAlso, I recommend specifying ‚Äúmac1‚Äù as an allowed Instance family for the sake of transparent resource management: only this instance type is permitted to allocate hosts in the group.\n  Optionally, you may specify the license association here (the Host group will pick any compatible license) or select the license you created on step one.\nLaunch Template Create Launch Template: EC2 -\u0026gt; Launch templates -\u0026gt; Create launch template.\nI will skip the description of all Launch Template parameters (but here is a nice tutorial), if you don‚Äôt mind, and keep focus only on the items relevant to the current case.\nSpecify mac1.metal as the Instance type. Later, in Advanced details: find the Tenancy parameter and set it to ‚ÄúDedicated host‚Äù; for Target host by select ‚ÄúHost resource group‚Äù, and once selected the new parameter Tenancy host resource group will appear where you should choose your host group; select your license in License configurations parameter.\n  Auto Scaling Group Finally, create the Auto Scaling Group: EC2 -\u0026gt; Auto Scaling groups -\u0026gt; Create Auto Scaling group.\nThe vital thing to note here ‚Äî is the availability of the mac1.metal instance in particular AZ.\nMac instances are available in us-east-1 and 7 more regions, but not every Availability Zone in the region supports it. So you must figure out which AZ supports the needed instance type.\nThere is no documentation for that, but there is an AWS CLI command that can answer this question: describe-instance-type-offerings ‚Äî AWS CLI 2.3.0 Command Reference\nHere is an example for the us-east-1 region: üîç Click here to see the code snippet 1 2 3 4  \u0026gt; aws ec2 describe-instance-type-offerings --location-type availability-zone-id --filters Name=instance-type,Values=mac1.metal --region us-east-1 --output text INSTANCETYPEOFFERINGS\tmac1.metal\tuse1-az6\tavailability-zone-id INSTANCETYPEOFFERINGS\tmac1.metal\tuse1-az4\tavailability-zone-id    \nKeep that nuance in mind when selecting a subnet for the mac1.metal instances.\nWhen you know the AZ, specify the respective Subnet in the Auto Scaling Group settings, and you\u0026rsquo;re ready to go!\nBring Infrastructure as Code here I suggest describing all that as a code. I prefer Terraform, and its AWS provider supports the needed resources. Except one.\nAs of October 2021, resources supported :\n aws_servicequotas_service_quota aws_licensemanager_license_configuration aws_launch_template aws_autoscaling_group  The Host resource group is not yet supported by the provider, unfortunately. However, we can use CloudFormation in Terraform to overcome that: describe the Host resource group as aws_cloudformation_stack Terraform resource using CloudFormation template from a file.\nHere is how it looks like: üîç Click here to see the code snippet 1 2 3 4 5 6 7 8 9 10 11 12 13  resource \u0026#34;aws_licensemanager_license_configuration\u0026#34; \u0026#34;this\u0026#34; { name = local.full_name license_counting_type = \u0026#34;Socket\u0026#34; } resource \u0026#34;aws_cloudformation_stack\u0026#34; \u0026#34;this\u0026#34; { name = local.full_name# the name of CloudFormation stack  template_body = file(\u0026#34;${path.module}/resource-group-cf-stack-template.json\u0026#34;) parameters = { GroupName = local.full_name# the name for the Host group, passed to CloudFormation template  } on_failure = \u0026#34;DELETE\u0026#34; }    \nAnd the next code snippet explains the CloudFromation template (which is the resource-group-cf-stack-template.json file in the code snippet above)\nüîç Click here to see the code snippet 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58  { \u0026#34;Parameters\u0026#34; : { \u0026#34;GroupName\u0026#34; : { \u0026#34;Type\u0026#34; : \u0026#34;String\u0026#34;, \u0026#34;Description\u0026#34; : \u0026#34;The name of Host Group\u0026#34; } }, \u0026#34;Resources\u0026#34; : { \u0026#34;DedicatedHostGroup\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;AWS::ResourceGroups::Group\u0026#34;, \u0026#34;Properties\u0026#34;: { \u0026#34;Name\u0026#34;: { \u0026#34;Ref\u0026#34;: \u0026#34;GroupName\u0026#34; }, \u0026#34;Configuration\u0026#34;: [ { \u0026#34;Type\u0026#34;: \u0026#34;AWS::ResourceGroups::Generic\u0026#34;, \u0026#34;Parameters\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;allowed-resource-types\u0026#34;, \u0026#34;Values\u0026#34;: [\u0026#34;AWS::EC2::Host\u0026#34;] }, { \u0026#34;Name\u0026#34;: \u0026#34;deletion-protection\u0026#34;, \u0026#34;Values\u0026#34;: [\u0026#34;UNLESS_EMPTY\u0026#34;] } ] }, { \u0026#34;Type\u0026#34;: \u0026#34;AWS::EC2::HostManagement\u0026#34;, \u0026#34;Parameters\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;allowed-host-families\u0026#34;, \u0026#34;Values\u0026#34;: [\u0026#34;mac1\u0026#34;] }, { \u0026#34;Name\u0026#34;: \u0026#34;auto-allocate-host\u0026#34;, \u0026#34;Values\u0026#34;: [\u0026#34;true\u0026#34;] }, { \u0026#34;Name\u0026#34;: \u0026#34;auto-release-host\u0026#34;, \u0026#34;Values\u0026#34;: [\u0026#34;true\u0026#34;] }, { \u0026#34;Name\u0026#34;: \u0026#34;any-host-based-license-configuration\u0026#34;, \u0026#34;Values\u0026#34;: [\u0026#34;true\u0026#34;] } ] } ] } } }, \u0026#34;Outputs\u0026#34; : { \u0026#34;ResourceGroupARN\u0026#34; : { \u0026#34;Description\u0026#34;: \u0026#34;ResourceGroupARN\u0026#34;, \u0026#34;Value\u0026#34; : { \u0026#34;Fn::GetAtt\u0026#34; : [\u0026#34;DedicatedHostGroup\u0026#34;, \u0026#34;Arn\u0026#34;] } } } }    \nThe aws_cloudformation_stack resource will export the DedicatedHostGroup attribute (see the code of CloudFromation template), which you will use later in the Launch Template resource.\nPro tips If you manage an AWS Organization, I have good news: Host groups and Licenses are supported by Resource Access Manager service. Hence, you can host all mac instances in one account and share them with other accounts ‚Äî it might be helpful for costs allocation, for example. Also, check out my blog about AWS RAM if you are very new to this service.\nTo solve the ‚Äúwhich AZ supports mac metal‚Äù puzzle, you can leverage the aws_ec2_instance_type_offerings and aws_subnet_ids data sources.\nCosts considerations License Manager is a free of charge service, as well as Auto Scaling, and Launch Template.\nSo it‚Äôs all about the price for mac1.metal Dedicated Host which is $1.083 per hour as of October 2021. However, Saving Plans can be applied.\nPlease note that the minimum allocation time for that type of host is 24 hours. Maybe someday AWS will change that to 1-hour minimum someday (fingers crossed).\nOh. So. ASG. The Auto Scaling for mac1.metal opens new possibilities for CI/CD: you can integrate that to your favorite tool (GitLab, Jenkins, whatsoever) using AWS Lambda and provision new instances when your development/testing environments need that. Or you can use other cool ASG stuff, such as Lifecycle hooks, to create even more custom scenarios.\nConsidering the ‚Äúhidden‚Äù (undocumented) nature of the described setup, I suggest treating it as rather testing than production-ready for now. However, my tests show that everything works pretty well: hosts are allocated, instances are spawned, and the monthly bill grows.\nI suppose AWS will officially announce all this in the nearest future. Along with that, I am looking forward to the announcement of Monterey-based AMIs and maybe even M1 chip-based instances (will it be mac2.metal?).\nAnd I want to say thanks (thanks, pal!) to OliverKoo, who started digging into that back in April'21.\n","permalink":"https://serhii.vasylenko.info/2021/10/24/auto-scaling-group-for-your-macos-ec2-instances-fleet/","summary":"It‚Äôs been almost a year since I started using macOS EC2 instances on AWS: there were ups and downs in service offerings and a lot of discoveries with macOS AMI build automation.\nAnd I like this small but so helpful update of EC2 service very much: with mac1.metal instances, seamless integration of Apple-oriented CI/CD with other AWS infrastructure could finally happen.\nWhile management of a single mac1.metal node (or a tiny number of ones) is not a big deal (especially when Dedicated Host support was added to Terraform provider), governing the fleet of instances is still complicated.","title":"Auto Scaling Group for your macOS EC2 Instances fleet"},{"content":"With a multi-account approach of building the infrastructure, there is always a challenge of provision and governance of the resources to subordinate accounts within the Organization. Provision resources, keep them up to date, and decommission them properly ‚Äî that\u0026rsquo;s only a part of them.\nAWS has numerous solutions that help make this process reliable and secure, and the Resource Access Manager (RAM) is one of them. In a nutshell, the RAM service allows you to share the AWS resources you create in one AWS account with other AWS accounts. They can be your organizations' accounts, organizational units (OU), or even third-party accounts.\nSo let\u0026rsquo;s see what the RAM is and review some of its usage examples.\nWhy using RAM There are several benefits of using the RAM service:\n  Reduced operational overhead: eliminate the need of provisioning the same kind of resource multiple times ‚Äî RAM does that for you\n  Simplified security management: AWS RAM-managed permissions (at least one per resource type) define the actions that principals with access to the resources (i.e., resource users) can perform on those resources.\n  Consistent experience: you share the resource in its state and with its security configuration with an arbitrary number of accounts.\nThat plays incredibly well in the case of organization-wide sharing: new accounts get the resources automatically. And the shared resource itself looks like a native resource in the account that accepts your sharing.\n  Audit and visibility: RAM integrates with the CloudWatch and CloudTrail.\n  How to share a resource When you share a resource, the AWS account that owns that resource retains full ownership of the resource.\nSharing of the resource doesn\u0026rsquo;t change any permissions or quotas that apply to that resource. Also, you can share the resource only if you own it.\nAvailability of the shared resources scopes to the Region: the users of your shared resources can access these resources only in the same Region where resources belong.\nCreation of resource share consists of three steps:   Specify the share name and the resource(s) you want to share. It can be either one resource type or several. You can also skip the resources selection and do that later.\nIt\u0026rsquo;s possible to modify the resource share later (e.g., you want to add some resources to the share).\n  Associate permissions with resource types you share. Some resources can have only one managed permission (will be attached automatically), and some can have multiple.\nYou can check the Permissions Library in the AWS RAM Console to see what managed permissions are available.\n  Select who can use the resources you share: either external or Organization account or IAM role/user. If you share the resource with third parties, they will have to accept the sharing explicitly.\nOrganization-wide resource share is accepted implicitly if resource sharing is enabled for the Organization.\n  Finally, review the summary page of the resource share and create it.\nOnly specific actions are available to the users of shared resources. These actions mostly have the \u0026ldquo;read-only\u0026rdquo; nature and vary by resource type.\nAlso, the RAM service is supported by Terraform, so the resource sharing configuration may look like that, for example:\n1 2 3 4 5 6 7 8 9 10 11 12 13  resource \u0026#34;aws_ram_resource_share\u0026#34; \u0026#34;example\u0026#34; { name = \u0026#34;example\u0026#34; allow_external_principals = false tags = { Environment = \u0026#34;Production\u0026#34; } } resource \u0026#34;aws_ram_resource_association\u0026#34; \u0026#34;example\u0026#34; { resource_arn = aws_subnet.example.arn resource_share_arn = aws_ram_resource_share.example.arn }   Example use cases One of the trivial but valuable examples of RAM service usage is sharing a Manged Prefix List. Suppose you have some service user across your Organization, a self-hosted VPN server, for example. And you have a static set of IPs for that VPN: you trust these IPs and would like them to be allow-listed in your other services. How to report these IPs to all organization accounts/users? And if the IP set changes, how to announce that change, and what should be done to reflect that change in services that depend on it, for example, Security Groups?\nThe answer is a shared Managed Prefix List. You create the list once in the account and share it across your Organization. Other accounts automatically get access to that list and can reference the list in their Security Groups. And when the list entry is changed, they do not need to perform any actions: their Security Groups will get the updated IPs implicitly.\nAnother everyday use case of RAM is the VPC sharing that can form the foundation of the multi-account AWS architectures.\n Of course, the RAM service is not the only way to organize and centralize resource management in AWS. There are Service Catalog, Control Tower, Systems Manager, Config, and others. However, the RAM is relatively simple to adopt but is capable of providing worthy outcomes.\n","permalink":"https://serhii.vasylenko.info/2021/09/25/aws-resource-access-manager-multi-account-resource-governance/","summary":"With a multi-account approach of building the infrastructure, there is always a challenge of provision and governance of the resources to subordinate accounts within the Organization. Provision resources, keep them up to date, and decommission them properly ‚Äî that\u0026rsquo;s only a part of them.\nAWS has numerous solutions that help make this process reliable and secure, and the Resource Access Manager (RAM) is one of them. In a nutshell, the RAM service allows you to share the AWS resources you create in one AWS account with other AWS accounts.","title":"AWS Resource Access Manager ‚Äî Multi Account Resource Governance"},{"content":"In days of containers and serverless applications, Ansible looks not such a trendy thing.\nBut still, there are cases when it helps, and there are cases when it combines very well with brand new product offerings, such as EC2 Mac instances.\nThe more I use mac1.metal in AWS, the more I see that Ansible becomes a bedrock of software customization in my case.\nAnd when you have a large instances fleet, the AWS Systems Manager becomes your best friend (the sooner you get along together, the better).\nSo is it possible to use Ansible playbooks for mac1.metal on a big scale, with the help of AWS Systems Manager?\n(Not) Available out of the box AWS Systems Manager (SSM hereafter) has a pre-defined, shared Document that allows running Ansible playbooks.\nIt‚Äôs called ‚ÄúAWS-RunAnsiblePlaybook,‚Äù and you can find it in AWS SSM ‚Üí Documents ‚Üí Owned by Amazon.\nHowever, this Document is not quite ‚Äúfriendly‚Äù to macOS. When the SSM agent calls Ansible on the Mac EC2 instance, it does not recognize the Ansible installed with Homebrew (de-facto most used macOS package manager).\nSo if you try to run a command on the mac1.metal instance using this Document, you will get the following error:\n1  Ansible is not installed. Please install Ansible and rerun the command.   The root cause is trivial: the path to Ansible binary is not present on the list of paths available to the SSM agent by default.\nThere are several ways to solve that, but I believe that the most convenient one would be to create your custom Document ‚Äî a slightly adjusted version of the default one provided by AWS.\nCreating own SSM Document for Ansible installed with Homebrew All you need to do is clone the Document provided by AWS and change its code a little ‚Äî replace the callouts of ansible with the full path to the binary.\nNavigate to AWS SSM ‚Üí Documents ‚Üí Owned by Amazon and type AWS-RunAnsiblePlaybook in the search field.\nSelect the Document by pressing the circle on its top-right corner and then click Actions ‚Üí Clone document.\nGive the new SSM Document a name, e.g., macos-arbitrary-ansible-playbook, and change the ansible callouts (at the end of the code) with the full path to the ansible symlink made by Homebrew which is /usr/local/bin/ansible\nHere is the complete source code of the Document with adjusted Ansible path:\nClick to expand the code block 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87  { \u0026#34;schemaVersion\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Use this document to run arbitrary Ansible playbooks on macOS EC2 instances. Specify either YAML text or URL. If you specify both, the URL parameter will be used. Use the extravar parameter to send runtime variables to the Ansible execution. Use the check parameter to perform a dry run of the Ansible execution. The output of the dry run shows the changes that will be made when the playbook is executed.\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;playbook\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;(Optional) If you don\u0026#39;t specify a URL, then you must specify playbook YAML in this field.\u0026#34;, \u0026#34;default\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;displayType\u0026#34;: \u0026#34;textarea\u0026#34; }, \u0026#34;playbookurl\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;(Optional) If you don\u0026#39;t specify playbook YAML, then you must specify a URL where the playbook is stored. You can specify the URL in the following formats: http://example.com/playbook.yml or s3://examplebucket/plabook.url. For security reasons, you can\u0026#39;t specify a URL with quotes.\u0026#34;, \u0026#34;default\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;allowedPattern\u0026#34;: \u0026#34;^\\\\s*$|^(http|https|s3)://[^\u0026#39;]*$\u0026#34; }, \u0026#34;extravars\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;(Optional) Additional variables to pass to Ansible at runtime. Enter a space separated list of key/value pairs. For example: color=red or fruits=[apples,pears]\u0026#34;, \u0026#34;default\u0026#34;: \u0026#34;foo=bar\u0026#34;, \u0026#34;displayType\u0026#34;: \u0026#34;textarea\u0026#34;, \u0026#34;allowedPattern\u0026#34;: \u0026#34;^((^|\\\\s)\\\\w+=(\\\\S+|\u0026#39;.*\u0026#39;))*$\u0026#34; }, \u0026#34;check\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34; (Optional) Use the check parameter to perform a dry run of the Ansible execution.\u0026#34;, \u0026#34;allowedValues\u0026#34;: [ \u0026#34;True\u0026#34;, \u0026#34;False\u0026#34; ], \u0026#34;default\u0026#34;: \u0026#34;False\u0026#34; }, \u0026#34;timeoutSeconds\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;(Optional) The time in seconds for a command to be completed before it is considered to have failed.\u0026#34;, \u0026#34;default\u0026#34;: \u0026#34;3600\u0026#34; } }, \u0026#34;mainSteps\u0026#34;: [ { \u0026#34;action\u0026#34;: \u0026#34;aws:runShellScript\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;runShellScript\u0026#34;, \u0026#34;inputs\u0026#34;: { \u0026#34;timeoutSeconds\u0026#34;: \u0026#34;{{ timeoutSeconds }}\u0026#34;, \u0026#34;runCommand\u0026#34;: [ \u0026#34;#!/bin/bash\u0026#34;, \u0026#34;/usr/local/bin/ansible --version\u0026#34;, \u0026#34;if [ $? -ne 0 ]; then\u0026#34;, \u0026#34; echo \\\u0026#34;Ansible is not installed. Please install Ansible and rerun the command\\\u0026#34; \u0026gt;\u0026amp;2\u0026#34;, \u0026#34; exit 1\u0026#34;, \u0026#34;fi\u0026#34;, \u0026#34;execdir=$(dirname $0)\u0026#34;, \u0026#34;cd $execdir\u0026#34;, \u0026#34;if [ -z \u0026#39;{{playbook}}\u0026#39; ] ; then\u0026#34;, \u0026#34; if [[ \\\u0026#34;{{playbookurl}}\\\u0026#34; == http* ]]; then\u0026#34;, \u0026#34; wget \u0026#39;{{playbookurl}}\u0026#39; -O playbook.yml\u0026#34;, \u0026#34; if [ $? -ne 0 ]; then\u0026#34;, \u0026#34; echo \\\u0026#34;There was a problem downloading the playbook. Make sure the URL is correct and that the playbook exists.\\\u0026#34; \u0026gt;\u0026amp;2\u0026#34;, \u0026#34; exit 1\u0026#34;, \u0026#34; fi\u0026#34;, \u0026#34; elif [[ \\\u0026#34;{{playbookurl}}\\\u0026#34; == s3* ]] ; then\u0026#34;, \u0026#34; aws --version\u0026#34;, \u0026#34; if [ $? -ne 0 ]; then\u0026#34;, \u0026#34; echo \\\u0026#34;The AWS CLI is not installed. The CLI is required to process Amazon S3 URLs. Install the AWS CLI and run the command again.\\\u0026#34; \u0026gt;\u0026amp;2\u0026#34;, \u0026#34; exit 1\u0026#34;, \u0026#34; fi\u0026#34;, \u0026#34; aws s3 cp \u0026#39;{{playbookurl}}\u0026#39; playbook.yml\u0026#34;, \u0026#34; if [ $? -ne 0 ]; then\u0026#34;, \u0026#34; echo \\\u0026#34;Error while downloading the document from S3\\\u0026#34; \u0026gt;\u0026amp;2\u0026#34;, \u0026#34; exit 1\u0026#34;, \u0026#34; fi\u0026#34;, \u0026#34; else\u0026#34;, \u0026#34; echo \\\u0026#34;The playbook URL is not valid. Verify the URL and try again.\\\u0026#34;\u0026#34;, \u0026#34; fi\u0026#34;, \u0026#34;else\u0026#34;, \u0026#34; echo \u0026#39;{{playbook}}\u0026#39; \u0026gt; playbook.yml\u0026#34;, \u0026#34;fi\u0026#34;, \u0026#34;if [[ \\\u0026#34;{{check}}\\\u0026#34; == True ]] ; then\u0026#34;, \u0026#34; /usr/local/bin/ansible-playbook -i \\\u0026#34;localhost,\\\u0026#34; --check -c local -e \\\u0026#34;{{extravars}}\\\u0026#34; playbook.yml\u0026#34;, \u0026#34;else\u0026#34;, \u0026#34; /usr/local/bin/ansible-playbook -i \\\u0026#34;localhost,\\\u0026#34; -c local -e \\\u0026#34;{{extravars}}\\\u0026#34; playbook.yml\u0026#34;, \u0026#34;fi\u0026#34; ] } } ] }    \nApplying Ansible playbook to the fleet of mac1.metal Let‚Äôs give our new SSM Document a try! (I suppose you have at least one mac1 instance running, right?)\nIn AWS SSM, go to the Run Command feature, then click on the Run Command button.\nOn the new panel, type the name of your Document (macos-arbitrary-ansible-playbook in this example) in the search field and press enter.\nSelect the Document, and you‚Äôll see its parameters and settings.\nThe rest is self-explanatory. Enter either a playbook code or a link to the source file, add extra variables if needed, and select the target host or a filtered bunch (I like that feature with tags filtering!). Finally, click on the ‚ÄúRun‚Äù orange button to apply your playbook.\nThat‚Äôs it! Now you can make all your ansible-playbook dreams come true! üòÅ\n","permalink":"https://serhii.vasylenko.info/2021/05/27/run-ansible-playbook-mac1-metal-aws-systems-manager.html","summary":"In days of containers and serverless applications, Ansible looks not such a trendy thing.\nBut still, there are cases when it helps, and there are cases when it combines very well with brand new product offerings, such as EC2 Mac instances.\nThe more I use mac1.metal in AWS, the more I see that Ansible becomes a bedrock of software customization in my case.\nAnd when you have a large instances fleet, the AWS Systems Manager becomes your best friend (the sooner you get along together, the better).","title":"Run Ansible playbook on mac1.metal instances fleet with AWS Systems Manager"},{"content":"A couple of weeks ago, AWS released CloudFront Functions ‚Äî a ‚Äútrue edge‚Äù compute capability for the CloudFront.\nIt is ‚Äútrue edge‚Äù because Functions work on 200+ edge locations (link to doc) while its predecessor, the Lambda@Edge, runs on a small number of regional edge caches.\nOne of the use cases for Lambda@Edge was adding security HTTP headers (it‚Äôs even listed on the product page), and now there is one more way to make it using CloudFront Functions.\nWhat are security headers, and why it matters Security Headers are one of the web security pillars.\nThey specify security-related information of communication between a web application (i.e., website) and a client (i.e., browser) and protect the web app from different types of attacks. Also, HIPAA and PCI, and other security standard certifications generally include these headers in their rankings.\nWe will use CloudFront Functions to set the following headers:\n Content Security Policy Strict Transport Security X-Content-Type-Options X-XSS-Protection X-Frame-Options Referrer Policy  You can find a short and detailed explanation for each security header on Web Security cheatsheet made by Mozilla\nCloudFront Functions overview In a nutshell, CloudFront Functions allow performing simple actions against HTTP(s) request (from the client) and response (from the CloudFront cache at the edge). Functions take less than one millisecond to execute, support JavaScript (ECMAScript 5.1 compliant), and cost $0.10 per 1 million invocations.\nEvery CloudFront distribution has one (default) or more Cache behaviors, and Functions can be associated with these behaviors to execute upon a specific event.\nThat is how the request flow looks like in general, and here is where CloudFront Functions execution happens:\nCloudFront Functions support Viewer Request (after CloudFront receives a request from a client) and Viewer Response (before CloudFront forwards the response to the client) events.\nYou can read more about the events types and their properties here ‚Äî CloudFront Events That Can Trigger a Lambda Function - Amazon CloudFront.\nAlso, the CloudFront Functions allow you to manage and operate the code and lifecycle of the functions directly from the CloudFront web interface.\nSolution overview CloudFront distribution should exist before Function creation so you could associate the Function with the distribution.\nCreation and configuration of the CloudFront Function consist of the following steps:\nCreate Function In the AWS Console, open CloudFront service and lick on the Functions on the left navigation bar, then click Create function button. Enter the name of your Function (e.g., ‚Äúsecurity-headers‚Äù) and click Continue.\nBuild Function On the function settings page, you will see four tabs with the four lifecycle steps: Build, Test, Publish, Associate.\nPaste the function code into the editor and click ‚ÄúSave.‚Äù\nHere is the source code of the function:\n1 2 3 4 5 6 7 8 9 10 11 12 13  function handler(event) { var response = event.response; var headers = response.headers; headers[\u0026#39;strict-transport-security\u0026#39;] = { value: \u0026#39;max-age=63072000; includeSubdomains; preload\u0026#39;}; headers[\u0026#39;content-security-policy\u0026#39;] = { value: \u0026#34;default-src \u0026#39;none\u0026#39;; img-src \u0026#39;self\u0026#39;; script-src \u0026#39;self\u0026#39;; style-src \u0026#39;self\u0026#39;; object-src \u0026#39;none\u0026#39;; frame-ancestors \u0026#39;none\u0026#39;\u0026#34;}; headers[\u0026#39;x-content-type-options\u0026#39;] = { value: \u0026#39;nosniff\u0026#39;}; headers[\u0026#39;x-xss-protection\u0026#39;] = {value: \u0026#39;1; mode=block\u0026#39;}; headers[\u0026#39;referrer-policy\u0026#39;] = {value: \u0026#39;same-origin\u0026#39;}; headers[\u0026#39;x-frame-options\u0026#39;] = {value: \u0026#39;DENY\u0026#39;}; return response; }   Test Function Open the ‚ÄúTest‚Äù tab ‚Äî let‚Äôs try our function first before it becomes live!\nSelect Viewer Response event type and Development Stage, then select ‚ÄúViewer response with headers‚Äù as a Sample test event (you will get a simple set of headers automatically).\nNow click the blue ‚ÄúTest‚Äù button and observe the output results:\n Compute utilization represents the relative amount of time (on a scale between 0 and 100) your function took to run Check the Response headers tab and take a look at how the function added custom headers.  Publish Function Let‚Äôs publish our function. To do that, open the Publish tab and click on the blue button ‚ÄúPublish and update.‚Äù Associate your Function with CloudFront distribution Now, you can associate the function with the CloudFront distribution.\nTo do so, open the Associate tab, select the distribution and event type (Viewer Response), and select the Cache behavior of your distribution which you want to use for the association.\nOnce you associate the function with the CloudFront distribution, you can test it in live mode.\nI will use curl here to demonstrate it:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  \u0026gt; curl -i https://d30i87a4ss9ifz.cloudfront.net HTTP/2 200 content-type: text/html content-length: 140 date: Sat, 22 May 2021 00:22:18 GMT last-modified: Tue, 27 Apr 2021 23:07:14 GMT etag: \u0026#34;a855a3189f8223db53df8a0ca362dd62\u0026#34; accept-ranges: bytes server: AmazonS3 via: 1.1 50f21cb925e6471490e080147e252d7d.cloudfront.net (CloudFront) content-security-policy: default-src \u0026#39;none\u0026#39;; img-src \u0026#39;self\u0026#39;; script-src \u0026#39;self\u0026#39;; style-src \u0026#39;self\u0026#39;; object-src \u0026#39;none\u0026#39;; frame-ancestors \u0026#39;none\u0026#39; strict-transport-security: max-age=63072000; includeSubdomains; preload x-xss-protection: 1; mode=block x-frame-options: DENY referrer-policy: same-origin x-content-type-options: nosniff x-cache: Miss from cloudfront x-amz-cf-pop: WAW50-C1 x-amz-cf-id: ud3qH8rLs7QmbhUZ-DeupGwFhWLpKDSD59vr7uWC65Hui5m2U8o2mw==   You can also test your results here ‚Äî Mozilla Observatory\nRead more That was a simplified overview of the CloudFront Functions capabilities.\nBut if you want to get deeper, here is a couple of useful links to start:\n Another overview from AWS ‚Äî CloudFront Functions Launch Blog More about creating, testing, updating and publishing of CloudFront Functions ‚Äî Managing functions in CloudFront Functions - Amazon CloudFront  So what to choose? CloudFront Functions are simpler than Lambda@Edge and run faster with minimal latency and minimal time penalty for your web clients.\nLambda@Edge takes more time to invoke, but it can run upon Origin Response event so that CloudFront can cache the processed response (including headers) and return it faster afterward.\nBut again, the CloudFront Functions invocations are much cheaper (6x times) than Lambda@Edge, and you do not pay for the function execution duration.\nThe final decision would also depend on the dynamic/static nature of the content you have at your origin.\nTo make a wise and deliberate decision, try to analyze your use case using these two documentation articles:\n Choosing between CloudFront Functions and Lambda@Edge How to Decide Which CloudFront Event to Use to Trigger a Lambda Function  ","permalink":"https://serhii.vasylenko.info/2021/05/21/configure-http-security-headers-with-cloudfront-functions.html","summary":"A couple of weeks ago, AWS released CloudFront Functions ‚Äî a ‚Äútrue edge‚Äù compute capability for the CloudFront.\nIt is ‚Äútrue edge‚Äù because Functions work on 200+ edge locations (link to doc) while its predecessor, the Lambda@Edge, runs on a small number of regional edge caches.\nOne of the use cases for Lambda@Edge was adding security HTTP headers (it‚Äôs even listed on the product page), and now there is one more way to make it using CloudFront Functions.","title":"Configure HTTP Security headers with CloudFront Functions"},{"content":"I just wanted to compress one image, but went to far\u0026hellip;\nor \u0026ldquo;How to add TinyPNG image compression to your macOS Finder contextual menu.\u0026rdquo;\nWhat is it and how it works You select needed files or folders, then right-click on them, click on the Services menu item and choose TinyPNG.\nAfter a moment, the new optimized versions of images will appear near to original files.\nIf you selected a folder along with the files, the script would process all png and jpeg files in it.\nPrerequisites You need to register at TinyPNG and get your API key here ‚Äî Developer API.\nThey sometimes block some countries (for example, Ukraine) from registration; in that case, try to use a web-proxy or VPN.\nHow to create Quick Action Workflow Open Automator application. If you never used this app before, please read about it on the official user guide website.\nOn the New Action screen, chose Quick Action\nAfter you click the \u0026ldquo;Choose\u0026rdquo; button, you\u0026rsquo;ll see the workflow configuration window.\nWorkflow configuration Find the Run Shell Script action on the Utilities list in Library on the left, and drag it onto the right side of the panel.\nSet the following workflow configuration options as described below:\nWorkflow receives current files and folders in Finder\nShell /bin/zsh\nPass input as arguments\nClick the Option button at the bottom of the Action window and Uncheck Show this action when the workflow runs.\nPut the following script into the Run Shell Script window, replacing the YOUR_API_KEY_HERE string with your API key obtained from TinyPNG.\n{% gist 13cb423aa83265e79ac5ad900195603f %}\nUtilities used in the script ‚Äî explained curl ‚Äî used to make web requests (like your browser does)\ngrep ‚Äî used to parse the response for the needed header (i.e., field) with the file download link\ncut ‚Äî used to extract the URL from the parsed result\nsed ‚Äî used to remove the trailing \u0026ldquo;carriage return\u0026rdquo; symbol at the end of extracted string\nThe response body also contains a JSON object that includes the download URL; you can parse it with jq, for example. But I intentionally refused to use the jq tool because it is not pre-installed in MacOS.\nConclusion It is simple, and it does its job fine. And you don\u0026rsquo;t need to install anything to make it work.\nTo make this a bit fancier, you might also like to add a \u0026ldquo;Display Notification\u0026rdquo; (from the Utilities library on the left) after the \u0026ldquo;Run Shell Script\u0026rdquo;. The action will display a notification once image processing is completed.\nThank you for reading!\n","permalink":"https://serhii.vasylenko.info/2021/02/14/image-compression-with-tinypng-from-macos-contextual-menu.html","summary":"I just wanted to compress one image, but went to far\u0026hellip;\nor \u0026ldquo;How to add TinyPNG image compression to your macOS Finder contextual menu.\u0026rdquo;\nWhat is it and how it works You select needed files or folders, then right-click on them, click on the Services menu item and choose TinyPNG.\nAfter a moment, the new optimized versions of images will appear near to original files.\nIf you selected a folder along with the files, the script would process all png and jpeg files in it.","title":"Using TinyPNG Image Compression From MacOS Finder Contextual Menu"},{"content":"I guess macOS was designed for a user, not for the ops or engineers, so this is why its customization and usage for CI/CD are not trivial (compared to something Linux-based). A smart guess, huh?\nConfiguration Management Native Apple\u0026rsquo;s Mobile device management (a.k.a MDM) and Jamf is probably the most potent combination for macOS configuration. But as much as it\u0026rsquo;s mighty, it is a cumbersome combination, and Jamf is not free.\nThen we have Ansible, Chef, Puppet, SaltStack ‚Äî they all are good with Linux, but what about macOS?\nI tried to search for use cases of mentioned CM tools for macOS. However, I concluded that they wrap the execution of native macOS command-line utilities most of the time.\nAnd if you search for the \u0026lsquo;macos\u0026rsquo; word in Chef Supermarket or Puppet Forge, you won\u0026rsquo;t be impressed by the number of actively maintained packages. Although, here is a motivating article about using Chef automating-macos-provisioning-with-chef if you prefer it. I could not find something similar and fresh for Puppet, so I am sorry, Puppet fans.\nThat is why I decided to follow the KISS principle and chose Ansible.\nIt\u0026rsquo;s easy to write and read the configuration, it allows to group tasks and to add execution logic , and it feels more DevOps executing shell commands inside Ansible tasks instead of shell scripts; I know you know that üòÇ\nBy the way, Ansible Galaxy does not have many management packages for macOS, either. But thankfully, it has the basics:\n homebrew with homebrew_cask and homebrew_tap ‚Äî to install software launchd ‚Äî to manage services osx_defaults ‚Äî to manage some user settings (not all!)  I used Ansible to build the macOS AMI for CI/CD, so here are some tips for such a case.\nSome values are hardcoded intentionally in the code examples for the sake of simplicity and easy reading. You would probably want to parametrize them.\nXcode installation example The following tasks will help you to automate the basics.\n1 2 3 4 5 6 7 8 9 10 11 12 13  - name:Install Xcodeshell:\u0026#34;xip --expand Xcode.xip\u0026#34;args:chdir:/Applications- name:Accept License Agreementshell:\u0026#34;/Applications/Xcode.app/Contents/Developer/usr/bin/xcodebuild -license accept\u0026#34;- name:Accept License Agreementshell:\u0026#34;/Applications/Xcode.app/Contents/Developer/usr/bin/xcodebuild -runFirstLaunch\u0026#34;- name:Switch into newly installed Xcode contextshell:\u0026#34;xcode-select --switch /Applications/Xcode.app/Contents/Developer\u0026#34;  Example of software installation with Brew {% raw %}\n1 2 3 4 5 6 7 8  - name:Install common build softwarecommunity.general.homebrew:name:\u0026#34;{{ item }}\u0026#34;state:latestloop:- swiftlint- swiftformat- wget  {% endraw %}\nScreenSharing (remote desktop) configuration example 1 2 3 4 5 6 7 8 9  - name:Turn On Remote Managementshell:\u0026#34;./kickstart -activate -configure -allowAccessFor -specifiedUsers\u0026#34;args:chdir:/System/Library/CoreServices/RemoteManagement/ARDAgent.app/Contents/Resources/- name:Enable Remote Management for CI usershell:\u0026#34;./kickstart -configure -users ec2-user -access -on -privs -all\u0026#34;args:chdir:/System/Library/CoreServices/RemoteManagement/ARDAgent.app/Contents/Resources/  Shell rulez, yes.\nBuilding the AMI Packer by HashiCorp, of course.\nI would love to compare Packer with EC2 Image Builder, but it does not support macOS yet (as of Feb'21).\nPacker configuration is straightforward, so I want to highlight only the things specific to the \u0026ldquo;mac1.metal\u0026rdquo; use case.\nTimeouts As I mentioned in the previous article, the creation and deletion time of the \u0026ldquo;mac1.metal\u0026rdquo; Instance is significantly bigger than Linux. That is why you should raise the polling parameters for the builder.\nExample:\n1 2 3 4  \u0026#34;aws_polling\u0026#34;: { \u0026#34;delay_seconds\u0026#34;: 30, \u0026#34;max_attempts\u0026#34;: 60 }   And it would be best if you also increased the SSH timeout:\n1  \u0026#34;ssh_timeout\u0026#34;: \u0026#34;1h\u0026#34;   Fortunately, Packer\u0026rsquo;s AMI builder does not require an explicit declaration of the Dedicated Host ID. So you can just reference the same subnet where you allocated the Host, assuming you did it with the enabled \u0026ldquo;Auto placement\u0026rdquo; parameter during the host creation.\nExample:\n1 2  \u0026#34;tenancy\u0026#34;: \u0026#34;host\u0026#34;, \u0026#34;subnet_id\u0026#34;: \u0026#34;your-subnet-id\u0026#34;   Provisioning Packer has Ansible Provisioner that I used for the AMI. Its documentation is also very clean and straightforward.\nBut it is still worth mentioning that if you want to parametrize the Ansible playbook, then the following configuration example will be handy:\n1 2 3 4 5 6 7 8  \u0026#34;extra_arguments\u0026#34;: [ \u0026#34;--extra-vars\u0026#34;, \u0026#34;your-variable-foo=your-value-bar]\u0026#34; ], \u0026#34;ansible_env_vars\u0026#34;: [ \u0026#34;ANSIBLE_PYTHON_INTERPRETER=auto_legacy_silent\u0026#34;, \u0026#34;ANSIBLE_OTHER_ENV_VARIABLE=other_value\u0026#34; ]   Configuration at launch If you\u0026rsquo;re familiar with AWS EC2, you probably know what the Instance user data is.\nA group of AWS developers made something similar for the macOS: EC2 macOS Init.\nIt does not support cloud-init as on Linux-based Instances, but it can run shell scripts, which is quite enough.\nEC2 macOS Init utility is a Launch Daemon (macOS terminology) that runs on behalf of the root user at system boot. It executes the commands according to the so-called Priority Groups, or the sequence in other words.\nThe number of the group corresponds to the execution order. You can put several tasks into a single Priority Group, and the tool will execute them simultaneously.\nEC2 macOS Init uses a human-readable configuration file in toml format.\nExample:\n1 2 3 4 5 6 7 8 9  [[Module]] Name = \u0026#34;Create-some-folder\u0026#34; PriorityGroup = 3 FatalOnError = false RunPerInstance = true [Module.Command] Cmd = [\u0026#34;mkdir\u0026#34;, \u0026#34;/Users/ec2-user/my-directory\u0026#34;] RunAsUser = \u0026#34;ec2-user\u0026#34; EnvironmentVars = [\u0026#34;MY_VAR_FOO=myValueBar\u0026#34;]   I should clarify some things here.\nModules ‚Äî a set of pre-defined modules for different purposes. It is something similar to the Ansible modules.\nYou can find the list of available modules here ec2-macos-init/lib/ec2macosinit\nThe RunPerInstance directive controls whether a module should run. There are three of such directives, and here is what they mean:\n RunPerBoot ‚Äî module will run at every system boot RunPerInstance ‚Äî module will run once for the Instance. Each Instance has a unique ID; the init tool fetches it from the AWS API before the execution and keeps its execution history per Instance ID. When you create a new Instance from the AMI, it will have a unique ID, and the module will run again. RunOnce ‚Äî module will run only once, despite the instance ID change  I mentioned the execution history above. When EC2 macOS Init runs on the Instance first time, it creates a unique directory with the name per Instance ID to store the execution history and user data copy.\nRunPerInstance and RunOnce directives depend on the execution history, and modules with those directives will run again on the next boot if the previous execution failed. It was not obvious to me why RunOnce keeps repeating itself every boot until I dug into the source code.\nFinally, there is a module for user data. It runs at the end by default (priority group #4) and pulls the user data script from AWS API before script execution.\nI suggest looking into the default init.toml configuration file to get yourself more familiar with the capabilities of the tool.\nThe init tool can also clear its history, which is useful for the new AMI creation.\nExample:\n1  ec2-macos-init clean -all   And you can run the init manually for debugging purposes.\nExample:\n1  ec2-macos-init run   You can also combine the EC2 macOS Init actions (made by modules) with your script in user data for more accurate nontrivial configurations.\nWrapping up As a whole, building and operating macOS-based AMI does not differ from AMI management for other platforms.\nThere are the same principle stages: prepare, clear, build, execute deployment script (if necessary). Though, the particular implementation of each step has its nuances and constraints.\nSo the whole process may look as follows:\n Provision and configure needed software with Ansible playbook Clean-up system logs and EC2 macOS Init history (again, with Ansible task) Create the AMI Add more customizations at launch with EC2 macOS Init modules and user data (that also executes your Ansible playbook or shell commands)  Getting into all this was both fun and interesting. Sometimes painful, though. üòÜ\nI sincerely hope this article was helpful to you. Thank you for reading!\n","permalink":"https://serhii.vasylenko.info/2021/02/01/customizing-mac1-metal-ec2-ami.html","summary":"I guess macOS was designed for a user, not for the ops or engineers, so this is why its customization and usage for CI/CD are not trivial (compared to something Linux-based). A smart guess, huh?\nConfiguration Management Native Apple\u0026rsquo;s Mobile device management (a.k.a MDM) and Jamf is probably the most potent combination for macOS configuration. But as much as it\u0026rsquo;s mighty, it is a cumbersome combination, and Jamf is not free.","title":"Customizing mac1.metal EC2 AMI ‚Äî new guts, more glory"},{"content":" Updated on the 23rd of October, 2021: Terraform AWS provider now supports Dedicated Hosts natively\n In November 2021, AWS announced the support for Mac mini instances.\nI believe this is huge, even despite the number of constraints this solution has. This offering opens the door to seamless macOS CI/CD integration into existing AWS infrastructure.\nSo here is a quick-start example of creating the dedicated host and the instance altogether using Terraform.\nI intentionally used some hardcoded values for the sake of simplicity in the example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  resource \u0026#34;aws_ec2_host\u0026#34; \u0026#34;example_host\u0026#34; { instance_type = \u0026#34;mac1.metal\u0026#34; availability_zone = \u0026#34;us-east-1a\u0026#34; } resource \u0026#34;aws_instance\u0026#34; \u0026#34;example_instance\u0026#34; { ami = data.aws_ami.mac1metal.id host_id = aws_ec2_host.example_host.id instance_type = \u0026#34;mac1.metal\u0026#34; subnet_id = data.aws_subnet.example_subnet.id } data \u0026#34;aws_subnet\u0026#34; \u0026#34;example_subnet\u0026#34; { availability_zone = \u0026#34;us-east-1a\u0026#34; filter { name = \u0026#34;tag:Tier\u0026#34;# you should omit this filter if you don\u0026#39;t distinguish your subnets on private and public  values = [\u0026#34;private\u0026#34;] } } data \u0026#34;aws_ami\u0026#34; \u0026#34;mac1metal\u0026#34; { owners = [\u0026#34;amazon\u0026#34;] most_recent = true filter { name = \u0026#34;name\u0026#34; values = [\u0026#34;amzn-ec2-macos-11*\u0026#34;]# get latest BigSur AMI  } }   Simple as that, yes. Now, you can integrate it into your CI system and have the Mac instance with the underlying host in a bundle.\nüí° Pro tip: you can leverage the aws_ec2_instance_type_offerings Data Source and use its output with aws_subnet source to avoid availability zone hardcoding.\nTo make the code more uniform and reusable, you can wrap it into a Terraform module that accepts specific parameters (such as instance_type or availability_zone) as input variables.\n","permalink":"https://serhii.vasylenko.info/2021/01/20/terraforming-mac1-metal-at-AWS.html","summary":"Updated on the 23rd of October, 2021: Terraform AWS provider now supports Dedicated Hosts natively\n In November 2021, AWS announced the support for Mac mini instances.\nI believe this is huge, even despite the number of constraints this solution has. This offering opens the door to seamless macOS CI/CD integration into existing AWS infrastructure.\nSo here is a quick-start example of creating the dedicated host and the instance altogether using Terraform.","title":"Terraforming mac1.metal at AWS"},{"content":"Amazon EC2 Mac Instances Something cool and powerful with inevitable trade-offs. As everything in this world.\nAWS announced EC2 macOS-based instances on the 30th of November 2020, and after more than a month of tests, I would like to share some findings and impressions about it.\nFirst of all, the things you can easily find, but it\u0026rsquo;s still worth to say:\n The new instance family is called mac1.metal. Guess we should expect mac2 or mac3; otherwise, why did they put a number in the name? They added AWS Nitro System to integrate them with many AWS services. The Instance must be placed onto a Dedicated Host. Only one Instance per Host is allowed because the Host is an actual Mac Mini in that case. You don\u0026rsquo;t pay anything for the Instance itself, but you pay for the Dedicated Host leasing ‚Äî $1.083, and the minimum lease time is 24 hours. So the launch of the \u0026ldquo;mac1.metal\u0026rdquo; Instance costs $26 at minimum. Prices provided for the cheapest region ‚Äî North Virginia. You can apply Saving Plans to save some money. Mojave (10.14) and Catalina (10.15) are supported at the moment, with \u0026ldquo;support for macOS Big Sur (11.0) coming soon\u0026rdquo;. I expect it to be in 2021, though.  What can it do Here is a list of some features that the \u0026ldquo;mac1.metal\u0026rdquo; instance has:\n It lives in your VPC because it is an EC2 Instance so that you can access many other services. It supports the new gp3 EBS type (and other types as well). It supports SSM Agent and Session Manager. It has several AWS tools pre-installed. It has pre-installed Enhanced Network Interface drivers. My test upload/download to S3 was about 300GB/s. It can report CPU metrics to CloudWatch (if you ever need it, though).  What can\u0026rsquo;t it do  It can\u0026rsquo;t be used in Auto Scaling because of a Dedicated Host. It can\u0026rsquo;t recognize the attached EBS if you connected it while the Instance was running ‚Äî you must reboot the Instance to make it visible. It does not support several services that rely on additional custom software, such as \u0026ldquo;EC2 Instance Connect\u0026rdquo; and \u0026ldquo;AWS Inspect.\u0026rdquo; But I think that AWS will add macOS distros for those soon.  Launching the Instance Jeff Bar published an excellent how-to about kickstart of the \u0026ldquo;mac1.metal\u0026rdquo;, so I will focus on things he did not mention.\nOnce you allocated the Dedicated Host and launched an Instance on it, the underlying system connects the EBS with a root file system to the Mac Mini.\nIt is an AMI with 32G EBS (as per Jan'21) with macOS pre-installed.\nThat means two things:\n The built-it physical SSD is still there and still yours to use; however, AWS does not manage or support the Apple hardware\u0026rsquo;s internal SSD. You must resize the disk manually (if you specified the EBS size to be more than 32G)[1].  The time from the Instance launch till you\u0026rsquo;re able to SSH into it varies between 15 and 20 minutes.\nYou have the option to access it over SSH with your private key. If you need to set up Screen Sharing, you have to allow it through the \u0026ldquo;kickstart\u0026rdquo; command-line utility and setting the user password [2].\nDestroying the Instance What an easy thing to do, right? Well, it depends.\nWhen you click on the \u0026ldquo;Terminate\u0026rdquo; item in the Instance actions menu, the complex Instance scrubbing process begins.\nAWS wants to make sure that anyone who uses the Host (Mac mini) after you will get your data stored neither on disks (including physical SSD mentioned earlier), nor inside memory or NVRAM, nor anywhere else. They do not share the info about this scrubbing process\u0026rsquo;s details, but it takes more than an hour to complete.\nWhen scrubbing is started, the Dedicated Host transitions to the Pending state. Dedicated Host transitions to Available state once scrubbing is finished. But you must wait for another 10-15 minutes to be able to release it finally.\nI don\u0026rsquo;t know why they set the Available state value earlier than the Host is available for operations, but this is how it works now (Jan'21).\nTherefore, you can launch the next Instance on the same Host not earlier than ~1,5 hours after you terminated the previous. That doesn\u0026rsquo;t seem very pleasant in the first couple of weeks, but you will get used to it. üòÑ\nAnd again: you can release the \u0026ldquo;mac1.metal\u0026rdquo; Dedicated Host not earlier than 24 hours after it was allocated. So plan your tests wisely.\nLegal things I could not find it on a documentation page, but A Cloud Guru folks say that you must use new Instances solely for developer services, and you must agree to all of the EULAs.\nSounds reasonable to me, but that could be written somewhere in the docs still, at least. Please let me know if you found it there.\nSome more cool stuff to check: EC2 macOS Init launch daemon, which is used to initialize Mac instances. EC2 macOS Homebrew Tap (Third-Party Repository) with several management tools which come pre-installed into macOS AMI from AWS.\n Indeed it is powerful, and it has its trade-offs, such as price and some technical constraints. But it is a real MacOS device natively integrated into the AWS environment. So I guess it worth to be tried!\nThanks for reading this! Stay tuned for more user experience feedback about baking custom AMI\u0026rsquo;s, automated software provisioning with Ansible, and other adventures with mac1.metal!\n [1] How to resize the EBS at mac1.metal in Terminal\nGet the identifier of EBS (look for the first one with GUID_partition_scheme):\ndiskutil list physical external\nOr here is a more advanced version to be used in a script:\n1  DISK_ID=$(diskutil list physical external | grep \u0026#39;GUID_partition_scheme\u0026#39;| tr -s \u0026#39; \u0026#39; | cut -d\u0026#39; \u0026#39; -f6)   It would probably be disk0 if you did not attach additional EBS.\nThen run the repair job for the disk, using its identifier: diskutil repairDisk disk0\nAdvanced version:\n1  yes | diskutil repairDisk $DISK_ID   Now get the APFS container identifier (look for Apple_APFS): diskutil list physical external\nAdvanced version:\n1  APFS_ID=$(diskutil list physical external | grep \u0026#39;Apple_APFS\u0026#39; | tr -s \u0026#39; \u0026#39; | cut -d\u0026#39; \u0026#39; -f8)   It would probably be disk0s2 if you did not attach additional EBS.\nFinally, resize the APFS container: diskutil apfs resizeContainer disk0s2\nAdvanced version\n1  diskutil apfs resizeContainer $APFS_ID   [2]How to setup Screen Sharing at mac1.metal in Terminal\nThe kickstart command-line tool resides in /System/Library/CoreServices/RemoteManagement/ARDAgent.app/Contents/Resources/ so you\u0026rsquo;ll better to cd into that directory for convenience:\n1 2 3 4 5 6 7 8  # Turn On Remote Management for a user to be specified later sudo ./kickstart -activate -configure -allowAccessFor -specifiedUsers # Enable Remote Management for ec2-user user sudo ./kickstart -configure -users ec2-user -access -on -privs -all # Set the user password  sudo passwd ec2-user   ","permalink":"https://serhii.vasylenko.info/2021/01/19/mac1-metal-EC2-Instance-user-experience.html","summary":"Amazon EC2 Mac Instances Something cool and powerful with inevitable trade-offs. As everything in this world.\nAWS announced EC2 macOS-based instances on the 30th of November 2020, and after more than a month of tests, I would like to share some findings and impressions about it.\nFirst of all, the things you can easily find, but it\u0026rsquo;s still worth to say:\n The new instance family is called mac1.metal. Guess we should expect mac2 or mac3; otherwise, why did they put a number in the name?","title":"mac1.metal EC2 Instance ‚Äî user experience"},{"content":"A simple but cool announcement from AWS ‚Äî AWS CloudShell. A tool for ad-hoc AWS management via CLI directly in your browser.\nI like when AWS releases something simple to understand and yet powerful.\nSo it is not another DevOps Guru, believe me :)\n Yes, this is similar to the shells that GCE and Azure have. No, you can‚Äôt access your instances from it, so it‚Äôs not a jump server (bastion host). Yes, it has AWS CLI and other tools pre-installed. Even Python and Node.js. No, you can‚Äôt (well, you can, but should not) use it as an alternative to the day-to-day console on your laptop. Yes, you can manage all resources from that shell as much as your IAM permissions allow you (even with SSO, which is pretty cool). No, it does not support Docker. Yes, you have 1 GB of permanent storage and the ability to transfer files in and out.  More Yes and No‚Äôs here: https://docs.aws.amazon.com/cloudshell/latest/userguide/faq-list.html\nhttps://aws.amazon.com/cloudshell/faqs/\n","permalink":"https://serhii.vasylenko.info/2020/12/16/aws-cloudshell.html","summary":"A simple but cool announcement from AWS ‚Äî AWS CloudShell. A tool for ad-hoc AWS management via CLI directly in your browser.\nI like when AWS releases something simple to understand and yet powerful.\nSo it is not another DevOps Guru, believe me :)\n Yes, this is similar to the shells that GCE and Azure have. No, you can‚Äôt access your instances from it, so it‚Äôs not a jump server (bastion host).","title":"AWS CloudShell"},{"content":"The work with Terraform code may become tangled sometimes. Here are some guides on how to streamline it and make it transparent for you and your team.\nIt is extremely helpful in a team, and can benefit you even if you work individually. A good workflow enables you to streamline a process, organize it, and make it less error-prone.\nThis article summaries several approaches when working with Terraform, both individually and in a team. I tried to gather the most common ones, but you might also want to develop your own.\nThe common requirement for all of them is a version control system (such as Git). This is how you ensure nothing is lost and all your code changes are properly versioned tracked.\nTable of contents:\n Basic Concepts Core individual workflow Core team workflow Team workflow with automation Import workflow  Basic Concepts Let‚Äôs define the basic actions first.\nAll described workflows are built on top of three key steps: Write, Plan, and Apply. Nevertheless, their details and actions vary between workflows.\nIt\u0026rsquo;s a piece of cake, isn\u0026rsquo;t it? üòÜ\nWrite ‚Äì this is where you make changes to the code.\nPlan ‚Äì this is where you review changes and decide whether to accept them.\nApply ‚Äì this is where you accept changes and apply them against real infrastructure.\nIt\u0026rsquo;s a simple idea with a variety of possible implementations.\nCore individual workflow This is the most simple workflow if you work alone on a relatively small TF project. This workflow suits both local and remote backends well.\nLet\u0026rsquo;s add a bit of Git\nWrite You clone the remote code repo or pull the latest changes, edit the configuration code, then run the terraform validate and terraform fmt commands to make sure your code works well.\nPlan This is where you run the terraform plan command to make sure that your changes do what you need. This is a good time to commit your code changes changes (or you can do it in the next step).\nApply This is when you run terraform apply and introduce the changes to real infrastructure objects. Also, this is when you push committed changes to the remote repository.\nCore team workflow This workflow is good for when you work with configuration code in a team and want to use feature branches to manage the changes accurately.\nDon\u0026rsquo;t get scared, it is still simple, just follow the lines\nWrite Start by checking out a new branch, make your changes, and run the terraform validate and terraform fmt commands to make sure your code works well.\nRunning terraform plan at this step will help ensure that you\u0026rsquo;ll get what you expect.\nPlan This is where code and plan reviews happen.\nAdd the output of the terraform plan command to the Pull Request with your changes. It would be a good idea to add only the changed parts of the common output, which is the part that starts with \u0026ldquo;Terraform will perform the following actions\u0026rdquo; string.\nApply Once the PR is reviewed and merged to the upstream branch, it is safe to finally pull the upstream branch locally and apply the configuration with terraform apply.\nTeam workflow with automation In a nutshell, this workflow allows you to introduce a kind of smoke test for your infrastructure code (using plan) and also to automate the feedback in the CI process.\nThe automated part of this workflow consists of a speculative plan on commit and/or Pull Request (PR ), along with adding the output of plan to the comment of the PR. A speculative plan mean just to show the changes, and not apply them afterward.\nI like when TF plan output is included to PR, but nobody likes to read others TF plans for some reason\u0026hellip;\nWrite This step is the same as in the previous workflow.\nPlan This is where your CI tool does its job.\nLet‚Äôs review this step by step:\n You create a PR with the code changes you wish to implement. The CI pipeline is triggered by an event from your code repository (such as webhook push) and it runs a speculative plan against your code. The list of changes (a so-called \u0026ldquo;plan diff\u0026rdquo;) is added to PR for review by the CI. Once merged, the CI pipeline runs again and you get the final plan that\u0026rsquo;s ready to be applied to the infrastructure.  Apply Now that you have a branch (i.e. main) with the fresh code to apply, you need to pull it locally and run terraform apply.\nYou can also add the automated apply here ‚Äì step 5 in the picture below. This may be very useful for disposable environments such as testing, staging, development, and so on.\nThe exact CI tool to be used here is up to you: Jenkins, GitHub Actions, and Travis CI all work well.\nAn important thing to note is that the CI pipeline must be configured in a bi-directional way with your repository to get the code from it and report back with comments to PR.\nAs an option, you may consider using Terraform Cloud which has a lot of functionality, including the above mentioned repo integration, even with the free subscription.\nIf you have never worked with Terraform Cloud before and want to advice to get started, I\u0026rsquo;ll provide the links at the end of this article.\nImport workflow This workflow refers to a situation when you have some objects already created (i.e., up and running), and you need to manage them with Terraform.\nSuppose we already have an S3 bucket in AWS called \u0026ldquo;someassetsbucket\u0026rdquo; and we want to include it into our configuration code.‚Äå‚Äå\nPrepare You should create a resource block to be used later for the real object you‚Äôre going to import.\nYou don‚Äôt need to fill the arguments in it at the start, so it may be just a blank resource block, for example:\n1 2 3  resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;someassetsbucket\u0026#34; { ‚Äå‚Äå }   Import Now you need to import the information about the real object into your existing Terraform state file.\nThis can be done with the terraform import command, for example:\n1  terraform import aws_s3_bucket.assets \u0026#34;someassetsbucket\u0026#34;   ‚ÄåBe sure to also check the list of possible options import accepts with terraform import -h\nWrite Now you need to write the corresponding Terraform code for this bucket.\nTo avoid modifying your real object on the terraform apply action, you should specify all needed arguments with the exact values from the import phase.\nYou can see the details by running the terraform state show command, for example:\n1  terraform state show aws_s3_bucket.assets   The output of this command will be very similar to the configuration code. But it contains both arguments and attributes of the resource, so you need to clean it up before applying it.\nYou can use one of the following tactics:\n either copy/paste it, and then run terraform validate and terraform plan several times to make sure there are no errors like \u0026ldquo;argument is not expected here\u0026rdquo; or \u0026ldquo;this field cannot be set\u0026rdquo; or you can pick and write only the necessary arguments  In any case, be sure to refer to the documentation of the resource during this process.\nPlan The goal is to have a terraform plan output showing \u0026ldquo;~ update in-place\u0026rdquo; changes only.\nHowever, it is not always clear whether the real object will be modified or only the state file will be updated. This is why you should understand how a real object works and know its life cycle to make sure it is safe to apply the plan.\nApply This is usual the terraform apply action.\nOnce applied, your configuration and state file will correspond to the real object configuration.\nWrapping up Here is an overview of Terraform Cloud for those who never worked with it before: ‚Äå‚ÄåOverview of Terraform Cloud Features\nAnd here is a nice tutorial to start with: Get Started - Terraform Cloud\nAlso, here is an overview of workflows at scale from the HashiCorp CTO which might be useful for more experienced Terraform users: Terraform Workflow Best Practices at Scale\nThank you for reading. I hope you will try one of these workflows, or develop your own!‚Äå‚Äå\n This article was originaly published on FreeCodeCamp paltform by me, but I still want to keep it here for the record. Canonical link to original publication was properly set in the page headers. ","permalink":"https://serhii.vasylenko.info/2020/09/16/terraform-workflow-working-individually-and-in-a-team.html","summary":"The work with Terraform code may become tangled sometimes. Here are some guides on how to streamline it and make it transparent for you and your team.\nIt is extremely helpful in a team, and can benefit you even if you work individually. A good workflow enables you to streamline a process, organize it, and make it less error-prone.\nThis article summaries several approaches when working with Terraform, both individually and in a team.","title":"Terraform Workflow ‚Äî Working Individually and in a Team"},{"content":"I successfully passed the \u0026ldquo;HashiCorp Certified ‚Äî Terraform Associate\u0026rdquo; exam last Friday and decided to share some advice for exam preparation.\nMake yourself a plan Make a list of things you are going to go through: links to the study materials, practice tasks, some labs, some articles on relative blogs (Medium, Dev.to, etc.). It should look at a \u0026ldquo;todo\u0026rdquo; or \u0026ldquo;check\u0026rdquo;-list. It may seem silly at first glance, but the list with checkboxes does its \u0026ldquo;cognitive magic\u0026rdquo;. When you go point by point, marking items as \u0026ldquo;done\u0026rdquo;, you feel the progress and this motivates you to keep going further. For example, you can make a plan from the resources I outlined below in this article.\nI encourage you to explore the Internet for something by yourself as well. Who knows, perhaps you will find some learning course that fits you better. And that is great! However, when you find it, take extra 5-10 minutes to go through its curriculum and create a list with lessons.\nIt feels so nice to cross out items off the todo list, believe me üòÑ Go through the official Study Guide Despite your findings on the Internet, I strongly suggest going through the official study guide\nStudy Guide - Terraform Associate Certification\nIt took me about 20 hours to complete it (including practice tasks based on topics in the guide), and it was the core of my studying. I did not buy or search for some third-party course intentionally because I did have some Terraform experience before starting the preparation. But give the official guide a chance even if you found some course. It is well-made and matches real exam questions very precisely.\nAlso, there is an official Exam Review. Someone might find this even better because it is a direct mapping of each exam objective to HashiCorp\u0026rsquo;s documentation and training.\nTake additional tutorials Here is a list of additional tutorials and materials I suggest adding into your learning program:\nOfficial guides / documentation:  Automate Terraform Collaborate using Terraform Cloud Terraform tutorials Reuse Configuration with Modules A Practitioner‚Äôs Guide to Using HashiCorp Terraform Cloud with GitHub Enforce Policy with Sentinel  Third-party articles and guides:  Using the terraform console to debug interpolation syntax YouTube playlist with exam-like questions review  Find yourself some practice Mockup a project You can greatly improve your practice by mocking some real business cases.\nIf you already work in some company you can set up the project you\u0026rsquo;re working with using Terraform. If you don‚Äôt have a real project or afraid to accidentally violate NDA, try this open-source demo project: Real World Example Apps.\nIt is a collection of different codebases for front-end and back-end used to build the same project. Just find the combination that suits your experience better and try to build the infrastructure for it using Terraform.\nAnswer forum topics Last but not least advice ‚Äî try to answer some questions on the official Terraform forum.\nThis is a nice way to test your knowledge, help others, and develop the community around Terraform. Just register there, look for the latest topics, and have fun!\nüçÄ I sincerely wish you exciting preparation and a successful exam! üçÄ ","permalink":"https://serhii.vasylenko.info/2020/09/15/terraform-certification-tips.html","summary":"I successfully passed the \u0026ldquo;HashiCorp Certified ‚Äî Terraform Associate\u0026rdquo; exam last Friday and decided to share some advice for exam preparation.\nMake yourself a plan Make a list of things you are going to go through: links to the study materials, practice tasks, some labs, some articles on relative blogs (Medium, Dev.to, etc.). It should look at a \u0026ldquo;todo\u0026rdquo; or \u0026ldquo;check\u0026rdquo;-list. It may seem silly at first glance, but the list with checkboxes does its \u0026ldquo;cognitive magic\u0026rdquo;.","title":"Terraform Certification Tips"},{"content":"Surprisingly, a lot of beginners skip over Terraform modules for the sake of simplicity, or so they think. Later, they find themselves going through hundreds of lines of configuration code.\nI assume you already know some basics about Terraform or even tried to use it in some way before reading the article.\nPlease note: I do not use real code examples with some specific provider like AWS or Google intentionally, just for the sake of simplicity.\nTerraform modules You already write modules even if you think you don‚Äôt.\nEven when you don\u0026rsquo;t create a module intentionally, if you use Terraform, you are already writing a module ‚Äì a so-called \u0026ldquo;root\u0026rdquo; module.\nAny number of Terraform configuration files (.tf) in a directory (even one) forms a module.\nWhat does the module do? A Terraform module allows you to create logical abstraction on the top of some resource set. In other words, a module allows you to group resources together and reuse this group later, possibly many times.\nLet\u0026rsquo;s assume we have a virtual server with some features hosted in the cloud. What set of resources might describe that server? For example: ‚Äì the virtual machine itself (created from some image) ‚Äì an attached block device of specified size (for additional storage) ‚Äì a static public IP mapped to the server\u0026rsquo;s virtual network interface ‚Äì a set of firewall rules to be attached to the server ‚Äì something else\u0026hellip; (i.e. another block device, additional network interface, etc)\nNow let\u0026rsquo;s assume that you need to create this server with a set of resources many times. This is where modules are really helpful ‚Äì you don\u0026rsquo;t want to repeat the same configuration code over and over again, do you?\nHere is an example that illustrates how our \u0026ldquo;server\u0026rdquo; module might be called. \u0026ldquo;To call a module\u0026rdquo; means to use it in the configuration file.\nHere we create 5 instances of the \u0026ldquo;server\u0026rdquo; using single set of configurations (in the module):\n1 2 3 4 5 6 7  module \u0026#34;server\u0026#34; { count = 5 source = \u0026#34;./module_server\u0026#34; some_variable = some_value }   Modules organisation: child and root Of course, you would probably want to create more than one module. Here are some common examples:\n for a network (i.e. VPC) for a static content hosting (i.e. buckets) for a load balancer and it\u0026rsquo;s related resources for a logging configuration and whatever else you consider a distinct logical component of the infrastructure  Let\u0026rsquo;s say we have two different modules: a \u0026ldquo;server\u0026rdquo; module and a \u0026ldquo;network\u0026rdquo; module. The module called \u0026ldquo;network\u0026rdquo; is where we define and configure our virtual network and place servers in it:\n1 2 3 4 5 6 7 8 9  module \u0026#34;server\u0026#34; { source = \u0026#34;./module_server\u0026#34; some_variable = some_value } module \u0026#34;network\u0026#34; { source = \u0026#34;./module_network\u0026#34; some_other_variable = some_other_value }   Once we have some custom modules, we can refer to them as \u0026ldquo;child\u0026rdquo; modules. And the configuration file where we call child modules relates to the root module.\nA child module can be sourced from a number of places:\n local paths official Terraform Registry (if you\u0026rsquo;re familiar with other registries, i.e. Docker Registry then you already understand the idea) Git repository (a custom one or GitHub/BitBucket) HTTP URL to .zip archive with module  But how can you pass resources details between modules?\nIn our example, the servers should be created in a network. So how can we tell the \u0026ldquo;server\u0026rdquo; module to create VMs in a network which was created in a module called \u0026ldquo;network\u0026rdquo;?\nThis is where encapsulation comes in.\nModule encapsulation Encapsulation in Terraform consists of two basic concepts: module scope and explicit resources exposure.\nModule Scope All resource instances, names, and therefore, resource visibility, are isolated in a module\u0026rsquo;s scope. For example, module \u0026ldquo;A\u0026rdquo; can\u0026rsquo;t see and does not know about resources in module \u0026ldquo;B\u0026rdquo; by default.\nResource visibility, sometimes called resource isolation, ensures that resources will have unique names within a module\u0026rsquo;s namespace. For example, with our 5 instances of the \u0026ldquo;server\u0026rdquo; module:\n1 2 3  module.server[0].resource_type.resource_name module.server[1].resource_type.resource_name module.server[2].resource_type.resource_name   On the other hand, we could create two instances of the same module with different names:\n1 2 3 4 5 6 7 8  module \u0026#34;server-alpha\u0026#34; { source = \u0026#34;./module_server\u0026#34; some_variable = some_value } module \u0026#34;server-beta\u0026#34; { source = \u0026#34;./module_server\u0026#34; some_variable = some_value }   In this case, the naming or address of resources would be as follows:\n1 2 3  module.server-alpha.resource_type.resource_name module.server-beta.resource_type.resource_name   Explicit resources exposure If you want to access some details for the resources in another module, you\u0026rsquo;ll need to explicitly configure that.\nBy default, our module \u0026ldquo;server\u0026rdquo; doesn\u0026rsquo;t know about the network that was created in the \u0026ldquo;network\u0026rdquo; module.\nSo we must declare an output value in the \u0026ldquo;network\u0026rdquo; module to export its resource, or an attribute of a resource, to other modules.\nThe module \u0026ldquo;server\u0026rdquo; must declare a variable to be used later as the input.\nThis explicit declaration of the output is the way to expose some resource (or information about it) outside ‚Äî to the scope of the \u0026lsquo;root\u0026rsquo; module, hence to make it available for other modules.\nNext, when we call the child module \u0026ldquo;server\u0026rdquo; in the root module, we should assign the output from the \u0026ldquo;network\u0026rdquo; module to the variable of the \u0026ldquo;server\u0026rdquo; module:\n1  network_id = module.network.network_id   Here is how the final code for calling our child modules will look like in result:\n1 2 3 4 5 6 7 8 9 10 11  module \u0026#34;server\u0026#34; { count = 5 source = \u0026#34;./module_server\u0026#34; some_variable = some_value network_id = module.network.network_id } module \u0026#34;network\u0026#34; { source = \u0026#34;./module_network\u0026#34; some_other_variable = some_other_value }   This example configuration would create 5 instances of the same server, with all the necessary resources, in the network we created with as a separate module.\nWrap up Now you should understand what modules are and what do they do.\nIf you\u0026rsquo;re at the beginning of your Terraform journey, here are some suggestions for the next steps.\nI encourage you to take this short tutorial from HashiCorp, the creators of Terraform, about modules: \u0026ldquo;Organize Configuration\u0026rdquo;\nAlso, there is a great comprehensive study guide which covers everything from beginner to advanced concepts about Terraform: \u0026ldquo;Study Guide - Terraform Associate Certification\u0026rdquo;\nThe modular code structure makes your configuration more flexible and yet easy to be understood by others. The latter is especially useful in teamwork.\n This article was originaly published on FreeCodeCamp paltform by me, but I still want to keep it here for the record. Canonical link to original publication was properly set in the page headers. ","permalink":"https://serhii.vasylenko.info/2020/09/09/terraform-modules-explained.html","summary":"Surprisingly, a lot of beginners skip over Terraform modules for the sake of simplicity, or so they think. Later, they find themselves going through hundreds of lines of configuration code.\nI assume you already know some basics about Terraform or even tried to use it in some way before reading the article.\nPlease note: I do not use real code examples with some specific provider like AWS or Google intentionally, just for the sake of simplicity.","title":"What are Terraform Modules and how do they work?"},{"content":"Here is some CLI shortcuts I use day-to-day to simplify and speed-up my Terraform workflow. Requirements \u0026mdash; bash-compatible interpreter, because aliases and functions described below will work with bash, zsh and ohmyzsh.\nIn order to use any of described aliases of functions, you need to place it in your ~/.bashrc or ~/.zshrc file (or any other configuration file you have for your shell).\nThen just source this file, for example: source ~/.zshrc\nFunction: list outputs and variables of given module You need to provide the path to module directory, and this function will list all declared variables and outputs module has. It comes very useful when you don\u0026rsquo;t remember them all and just need to take a quick look.\n1 2 3 4 5 6 7  ## TerraForm MOdule Explained function tfmoe { echo -e \u0026#34;\\nOutputs:\u0026#34; grep -r \u0026#34;output \\\u0026#34;.*\\\u0026#34;\u0026#34; $1 |awk \u0026#39;{print \u0026#34;\\t\u0026#34;,$2}\u0026#39; |tr -d \u0026#39;\u0026#34;\u0026#39; echo -e \u0026#34;\\nVariables:\u0026#34; grep -r \u0026#34;variable \\\u0026#34;.*\\\u0026#34;\u0026#34; $1 |awk \u0026#39;{print \u0026#34;\\t\u0026#34;,$2}\u0026#39; |tr -d \u0026#39;\u0026#34;\u0026#39; }   Example usage:\n1 2 3 4 5 6 7 8 9 10 11  user@localhost $: tfmoe ./module_alb Outputs: alb_arn Variables: acm_certificate_arn lb_name alb_sg_list subnets_id_list tags   Function: pre-fill module directory with configuration files You need to provide a path to the module directory and this function will create a bunch of empty \u0026lsquo;default\u0026rsquo; .tf files in it.\n1 2 3 4 5 6 7  #TerraForm MOdule Initialize function tfmoi { touch $1/variables.tf touch $1/outputs.tf touch $1/versions.tf touch $1/main.tf }   Example usage:\n1 2 3 4  user@localhost $: mkdir ./module_foo \u0026amp;\u0026amp; temoi $_ user@localhost $: ls ./module_foo main.tf outputs.tf variables.tf versions.tf   Aliases The purpose of these aliases is just to keep you from typing long commands when you want to do a simple action.\n1 2 3 4 5 6 7  alias tf=\u0026#39;terraform\u0026#39; alias tfv=\u0026#39;terraform validate\u0026#39; alias tfi=\u0026#39;terraform init\u0026#39; alias tfp=\u0026#39;terraform plan\u0026#39;   This one is useful because it makes format tool to go in-depth (recursively) through directories.\n1  alias tfm=\u0026#39;terraform fmt -recursive\u0026#39;   Example usage:\n1 2 3  user@localhost $: tfm module_ecs_cluster/ecs.tf module_alb/alb.tf   ","permalink":"https://serhii.vasylenko.info/2020/08/25/terraform-cli-shortcuts.html","summary":"Here is some CLI shortcuts I use day-to-day to simplify and speed-up my Terraform workflow. Requirements \u0026mdash; bash-compatible interpreter, because aliases and functions described below will work with bash, zsh and ohmyzsh.\nIn order to use any of described aliases of functions, you need to place it in your ~/.bashrc or ~/.zshrc file (or any other configuration file you have for your shell).\nThen just source this file, for example: source ~/.","title":"Terraform CLI shortcuts"},{"content":"Lookup plugins for Ansible allow you to do a lot of cool things. One of them is to securely pass sensitive information to your playbooks. If you manage some apps in AWS with Ansible, then using Parameter Store or Secrets Manager along with it might greatly improve your security.\nVariables with SSM Parameter Store Let\u0026rsquo;s say you have some variables defined in \u0026lsquo;defaults/main.yaml\u0026rsquo; file of your role or maybe in group_vars.yaml file.\n1 2 3 4 5 6  ---# content of dev.vars.yaml to be included in your play or roleuse_tls:trueapplication_port:3000app_env:developmentstripe_api_key:1HGASU2eZvKYlo2CT5MEcnC39HqLyjWD  If you store such things locally on Ansible control node, you probably encrypt it with ansible-vault\nSSM Parameter Store gives you more flexibility and security by centralized storage and management of parameters and secrets, so let\u0026rsquo;s use it with Ansible:\n1 2 3 4 5 6  # content of dev.vars.yaml to be included in your play or roleuse_tls:\u0026#34;{{lookup(\u0026#39;aws_ssm\u0026#39;, \u0026#39;/dev/webserver/use_tls\u0026#39;)}}\u0026#34;application_port:\u0026#34;{{lookup(\u0026#39;aws_ssm\u0026#39;, \u0026#39;/dev/webserver/application_port\u0026#39;)}}\u0026#34;app_env:\u0026#34;{{lookup(\u0026#39;aws_ssm\u0026#39;, \u0026#39;/dev/webserver/app_env\u0026#39;)}}\u0026#34;stripe_api_key:\u0026#34;{{lookup(\u0026#39;aws_ssm\u0026#39;, \u0026#39;/dev/webserver/stripe_api_key\u0026#39;)}}\u0026#34;  The syntax is fairly simple:\nThe aws_ssm argument ‚Äì is the name of plugin.\nThe /dev/webserver/use_tls argument ‚Äì is the path to the key in Paramter Store.\nSurely you can do the same for a group of servers with group variables, for example:\nYou can use this anywhere you can use templating: in a play, in variables file, or a Jinja2 template.\nVariables with Secret Manager Another cool lookup plugin is Secrets Manager. In a nutshell, it has the same kind of functionality but it uses JSON format by feault.\nHere is a quick example of its functionality in a Playbook:\n1 2 3 4  ---- name:Extract something secrets from Secret Managerdebug:msg:\u0026#34;{{ lookup(\u0026#39;aws_secret\u0026#39;, \u0026#39;dev/some-secrets\u0026#39;)}}\u0026#34;  The above task will generate the following output\n1 2 3 4 5 6 7 8 9 10 11  TASK [Extract something secrets from Secret Manager] **************************************************** ok: [some_server] =\u0026gt; { \u0026#34;msg\u0026#34;: { \u0026#34;dbname\u0026#34;: \u0026#34;database\u0026#34;, \u0026#34;engine\u0026#34;: \u0026#34;mysql\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;password\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;3306\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;db_user\u0026#34; } }   This is nice if you want to insert a JSON as is, but you will need additional parsing in case you want to get only some of JSON elements.\nConclusion If you‚Äôre using Ansible in CI/CD, then having it on an EC2 Instance with the IAM role will make you avoid keeping any secrets on that instance at all.\nThe IAM role must allow at least the read access to SSM Parameter Store (+ KMS read access to be able to decrypt the keys) or the read access to Secrets Manager.\nYou can find documentation for described plugins here aws_ssm and here aws_secret.\nMore about lookup plugins: https://docs.ansible.com/ansible/latest/plugins/lookup.html\n","permalink":"https://serhii.vasylenko.info/2020/08/06/ansible-secrets-aws-ssm-sm.html","summary":"Lookup plugins for Ansible allow you to do a lot of cool things. One of them is to securely pass sensitive information to your playbooks. If you manage some apps in AWS with Ansible, then using Parameter Store or Secrets Manager along with it might greatly improve your security.\nVariables with SSM Parameter Store Let\u0026rsquo;s say you have some variables defined in \u0026lsquo;defaults/main.yaml\u0026rsquo; file of your role or maybe in group_vars.","title":"Manage Ansible playbook secrets with AWS services"},{"content":"This article was written a long time ago in a galaxy far, far away\u0026hellip; As a team leader, I have to speak with my teammates on the same language ‚Äî technical language\u0026hellip;\nFor example, I have a good technical background, yet sometimes I have a feeling that my teammates see that I don‚Äôt understand them when we discuss some project or a task in-depth. Moreover, I know they are right. Of course, there are plenty of managers who do not have a technical background and they perform great. And some might say that technical skills are not the priority for a manager.\nBut I think that +1 to skills is always better than +0. Of course, it is a question of time and personal interests, one way or another.\nThis is why I decided to share my experience and explain Terraform in one blog post.\nThe language of this article will be ‚Äòtechie‚Äô but not too much. This is because I want to highlight the main parts the Terraform consists of. Although, this is not technical documentation (I hope). Code examples will be based on AWS cloud configuration, although in-depth knowledge of AWS is not required to understand them.\nA few words about ‚ÄúInfrastructure as Code‚Äù IaC is when you describe and manage your infrastructure as‚Ä¶ (guess what?) ‚Ä¶code. Literally.\nIn a nutshell that means you can define all the elements (servers, networks, storage, etc.) and resources (memory, cpu, etc) of your infrastructure via configuration files in the version control system (Git, SVN, etc.), and manage it in a way similar to how you manage the source code of the applications: branches, releases, and all that stuff.\nAnd the main idea behind the IaC approach is that it manages the state of things and must be the single source of truth (configuration truth) for your infrastructure. You define the state via the code (at first) and then IaC tool (Terraform, for example) applies this state on the infrastructure: all that is missing according to the code will be created, all that differs from the code will be changed and all that exists in the infrastructure but is not described via code ‚Äî will be destroyed.\nWhy and when do you need the Terraform for a project? Terraform is a specific tool, hence like any other tool it has its particular application area. There is no strict definition of project kind that needs Terraform (surprise!) but in general, you need to consider using Terraform if you answer ‚Äòyes‚Äô to one of the following questions:\n Do you have multiple logical elements of the same kind (in plural) in your infrastructure, i.e. several web servers, several application servers, several database servers? Do you have numerous environments (or workspaces) where you run your applications, i.e. development, staging, QA, production? Do you spend some significant amount of time managing the changes in the environment(s) where you run your applications?  How does it work? Terraform works with the source code of configuration, and interprets the code into real resources inside on-premise or cloud platforms.\nTerraform supports a lot of platforms: from major cloud providers such as AWS, Azure, GCP, DigitalOcean, to more modest platforms such as OVH, 1\u0026amp;1, Hetzner, and others. It also supports infrastructure software such as Docker, Kubernetes, Chef, and even databases and monitoring software. This is why Terraform is so popular ‚Äî it is a real Swiss knife in the operations world.\nSo to create, change, or destroy the infrastructure Terraform needs the source code. The source code is a set of configuration files that defines your infrastructure state. The code uses its own syntax but it looks very user friendly. Here is an example: the following configuration block describes the virtual server (EC2 instance) in AWS\n1 2 3 4  resource \u0026#34;aws_instance\u0026#34; ‚Äúweb_server‚Äù {ami = \u0026#34;ami-a1b2c3d4\u0026#34;instance_type = \u0026#34;t3.micro\u0026#34;}  Terraform can automatically detect the dependencies between resources described in the code, and also allows you to add custom dependencies when needed.\nWhen you apply the code first time, Terraform creates a so-called ‚Äústate file‚Äù that works as a mapping of your code to real resources created in the hosting platform. With each next ‚Äúapply‚Äù action Terraform will use it to compare the code changes with the sate file to decide what should be done (and in what order) against real infrastructure.\nOne of the important functions of the state file is a description of dependencies between the resources. For example (some technical nuances are omitted for purpose of simplicity): if you have a server created inside some network and that network is going to be changed, then Terraform will know that server setting should be changed as well or server should be re-created inside the updated network.\nWhat is inside? Terraform configuration code consists of several elements: providers, resources, modules, input variables, output values, local values, expressions, functions.\nProvider is an entity that defines what exactly is possible to do with cloud or on-premises infrastructure platform you manage via Terraform.\nResource is the most important part of the configuration code. This is where the definition of infrastructure objects happens. Resources are the main building blocks of the whole code.\nEvery resource has a type and local name. For example here is how EC2 instance configuration may look like:\n1 2 3 4  resource ‚Äúaws_instance‚Äù ‚Äúweb_server‚Äù {ami = ‚Äúami-a1b2c3d4‚Äùinstance_type = ‚Äút3.micro‚Äù}  The aws_instance is a resource type and web_server is the resource local name. Later, when Terraform applies this code, it will create an EC2 instance with some particular ID in AWS. Once created, the ID will be stored in the state file with mapping information that logically connects it with web_server.\nThe ami, instance_type and private_ip are the arguments with values which define the actual state of the resource. There are plenty of value types, depending on the particular argument and particular resource type, so I will not focus on them here.\nModules are the kind of logical containers or groups for resources you define and use together. The purpose of modules is not only the grouping of resources but it is also the possibility to reuse the same code with different variables.\nLet‚Äôs get back to the example with EC2 instance and say you need to have a static public IP address with it. In such a case, here is how the module for web server may look like:\n1 2 3 4 5 6 7  resource ‚Äúaws_instance‚Äù ‚Äúweb_server‚Äù {ami = ‚Äúami-a1b2c3d4‚Äùinstance_type = ‚Äút3.micro‚Äù}resource ‚Äúaws_eip‚Äù ‚Äúweb_server_public_ip‚Äù {instance = ‚Äú${aws_instance.web_server.id}‚Äù}  Having these two resources together allows us to think of it as a stand-alone unit you can reuse later, for example in our development, staging, and production environments. And not by copying and pasting it, but via reference to the module defined only once.\nPlease note: we specified an instance argument inside the aws_eip resource as a reference to another resource details (the ID of an instance). This is possible because of a way how Terraform treats dependencies: when it detects the dependency (or you define it explicitly) it will create the main resource first, and only after it‚Äôs created and available it will create the dependent one.\nInput variables work as parameters for the modules so module code could be reusable. Let‚Äôs look at the previous example: it has some hardcoded values ‚Äî instance image ID and instance type. Here is how you can make it more abstract and reusable:\n1 2 3 4 5 6 7 8 9 10  variable ‚Äúimage_id‚Äù {type = string}variable ‚Äúinstance_type‚Äù {type = string}resource ‚Äúaws_instance‚Äù ‚Äúweb_server‚Äù {ami = var.image_idinstance_type = var.instance_type}  Values for the variables then can be passed either via CLI and environment variables (if you have only the one, so-called root module) or via explicit values in the block where you call a module, for example:\n1 2 3 4 5 6 7 8 9 10  module ‚Äúweb_server_production‚Äù {source. = ‚Äú./modules/web_server‚Äùimage_id = ‚Äúami-a1b2c3d4‚Äùinstance_type = ‚Äúm5.large‚Äù}module ‚Äúweb_server_development‚Äù {source = ‚Äú./modules/web_server‚Äùimage_id = ‚Äúami-a2b3c4d5‚Äùinstance_type = ‚Äút3.micro‚Äù}  Output values are similar to the ‚Äúreturn‚Äù of a function in development language. They can be used for dependencies management (for example, when a module require something from another module) and for the printing of the certain values at the end of Terraform work (for example to be used for notification in CI/CD process).\nLocal values, expressions, functions ‚Äî three more things that augment the capabilities of Terraform and make it more similar to a programming language (which is great by the way).\nThe local values are used inside modules for extended data manipulations in it.\nThe expressions are used to set the values (for many things), for example, to set the value of some argument in resource configuration. They used either to refer something (just as we referenced instance ID ‚Äú${aws_instance.web_server.id}‚Äù in the example above) or to compute the value within your configuration.\nThe functions in Terraform are built-int jobs you can call to transform and combine values. For example, the tolist() function converts its argument to a list value.\nAnd this is it? Yes, in a very very short words ‚Äî this is what Terraform is. Not a rocket science if it\u0026rsquo;s about to manage a small infastructure, but gets more complicated with bigger infrastctucture. As any other engineerign tool or development language, actually.\nOkay, what next? If you read down to this point (anybody?) then it means it worth ‚Äúget your hands dirty‚Äù and to try building your Infrastructure with Terraform. There are plenty of courses and books (and the ‚ÄúTerraform up and running‚Äù is one of the most popular), but my learning path started from the following: Official guide from Hashicorp ‚Äî great and free guide from Terraform developers. Just pick your favorite cloud (AWS, Azure, GCP) and go through the topics.\nOnce you finish this guide, I suggest jumping into the more real-world things and describe the infrastructure of the most common project you work with. For example, here is what I do: small github project ‚Äì I am trying to describe the Infrastructure for SPA website with services in docker containers at the backend. The variety and complexity of the code are limited only by your fantasy.\nAnother thing worth your attention is A Comprehensive Guide to Terraform.\nAnd I also encourage you to go through the collection of blog posts and talks they share.\n","permalink":"https://serhii.vasylenko.info/2020/05/02/Terraform-explained-for-managers.html","summary":"This article was written a long time ago in a galaxy far, far away\u0026hellip; As a team leader, I have to speak with my teammates on the same language ‚Äî technical language\u0026hellip;\nFor example, I have a good technical background, yet sometimes I have a feeling that my teammates see that I don‚Äôt understand them when we discuss some project or a task in-depth. Moreover, I know they are right. Of course, there are plenty of managers who do not have a technical background and they perform great.","title":"Terraform explained for managers"},{"content":"I am a team lead in an outsourcing company. That means I believe (or convince myself) that I lead the team (wait, wait, there will be even more obvious discoveries).\nI am sure that a team can achieve more than a single person or a group. And the story I want to tell is about making the team from a group of people with personal and non-connected tasks. I assume this is going to be a series of posts, and hopefully, I will add a table of contents here.\nA group or a team? It is quite rare to find a project that involves more than 10-15 people in a small or mid-size outsourcing company. With such project size, a single DevOps engineer is enough to handle the whole job. Or an engineer may work on several projects at the same time if projects are relatively small. This is because the scope of work to be done fits into the schedule of a single person. So it is either one-to-one or one-to-many connection in most cases: either the employee works with one project or the employee works with several projects simultaneously.\nOutsourcing a \u0026ldquo;DevOps\u0026rdquo; within this context usually means assigning an individual for the work with a client or/and a client\u0026rsquo;s team.\nNow imagine: you have several people in a room, all of them are DevOps engineers working in the same outsource company and all they work with their \u0026ldquo;own\u0026rdquo; projects separately.\nSo would you call that group of people \u0026ldquo;a team\u0026rdquo;?\nWikipedia states that \u0026ldquo;a team\u0026rdquo; is a group of individuals (human or non-human) working together to achieve their [common] goal. As well as common sense tells us that \u0026ldquo;a team\u0026rdquo; is something that groups people to make them work for a mutual outcome.\nAnd here comes an obvious but a difficult question: what common goal(s) a group of people can have whereas each member of that group has its own goal, which is not related to others'? A bit contradictory, isn\u0026rsquo;t it?\nA Goal Conventional thinking tells us that a goal for the for-profit company is to make money. This is true, and I don\u0026rsquo;t want to argue with it. Therefore, does it mean that a goal for a team of engineers inside the company is the same as for the company as a whole?\nYes\u0026hellip; Yet, not only, or almost.\nYes, because it\u0026rsquo;s the main reason why we work for an outsourcing company. We do the job for clients, clients pay the company, the company pays us.\nHowever, there is a long chain of events and processes that leads from the start of a contract to the paid invoice. A team of engineers is the inalienable part of that chain, with a focus on its work - deliver technical solutions following the client\u0026rsquo;s expectations and requirements.\nThis conclusion reveals the goal for the DevOps team in the outsourcing company: grow the quality of work by a process of ongoing improvement and refinement of technical and soft skills. So simple, so obvious.\nAnd this goal means nothing by itself. But to sustain it and adapt, to make it desirable for the team, and to make it a team\u0026rsquo;s foundation eventually, we need the following:\n Values Principles Common interests  One of the crucial responsibilities of a team lead is to foster these three pillars of a team.\nThe VALUES is something that lets us feel comfortable with the people surrounding us in the office. The list of values is not carved in stone, it can (and eventually should) change as the team evolves. But here is the most constant items for my team:\n openness in communication respect to each other\u0026rsquo;s opinion empathy feeling free to provide critic and being ready to receive it count on help from a teammate and be ready to help ability to work remotely and have flexible working hours  The PRINCIPLES is a set of beliefs and policies that keeps us on the required level of productivity and disciplines us, and stimulates the longing to improve. Again, this list can be and should be adjusted as the time goes and the team evolves, but here is the most up to date for us:\n we learn constantly we are the first who appraises the quality of our work we are proactive we are open: if we do not agree with something or something is worrying us, we should be brave enough to say it we may defend own point of view, but be ready to accept that it is incorrect we can make mistakes, but we must learn from it and do not repeat them we must help our teammates we keep our promises  The COMMON INTERESTS is more about informal things we share within a team. This includes (but not limited to) team building events, discussions and arguments about new technologies, so-called water-cooler chats, and so on. Being less formal, this pillar acts as a glue for the team. This is because we are all humans, first of all, and we generally work for 1/3 of our day (not including weekends), so we need to stay humans at work as well. I mean it is impossible to switch off the \u0026lsquo;human\u0026rsquo; for that 1/3 of the day, leaving only the engineer for this time. Hence, this must be admitted and we must cope with it by putting several informal things into formal work.\nThese pillars create a team culture where learning, creativity, and an open mindset are encouraged.\nThe Team This is how a group of people with separate goals may become a team. While we do not have a goal (\u0026ldquo;one for all\u0026rdquo;) that is measurable or has a fixed point of completion, we still have a common aim whose realization is based on pillars of teamwork. And with a set of sub-goals, this may be a specific, achievable (until you find something else to improve, which should happen quite often) and even realistic. So it\u0026rsquo;s kinda 60% S.M.A.R.T. goal :smile:\nUnfortunately or fortunately (if you like your team lead job) there is a lot of work that needs to be done and a lot of questions need to be answered to implement the described approach:\n How to translate own values to the team and create the new ones together? How to make people communicate in such a team and share their knowledge? How to define who does what? How to teach team members to learn? How to keep team members loyal to the team and company? How to mitigate conflicts and how to use them? How to keep team members motivated when the work becomes boring and the level of engagement goes down? and much much more  I wish I could answer all these questions at once, and I wish there were only correct answers. But my experience tells me that it is possible to answer them one by one. And I hope to share that here someday!\n","permalink":"https://serhii.vasylenko.info/2020/03/20/devops-team-in-outsource-a-team.html","summary":"I am a team lead in an outsourcing company. That means I believe (or convince myself) that I lead the team (wait, wait, there will be even more obvious discoveries).\nI am sure that a team can achieve more than a single person or a group. And the story I want to tell is about making the team from a group of people with personal and non-connected tasks. I assume this is going to be a series of posts, and hopefully, I will add a table of contents here.","title":"DevOps team in outsource. A team?"},{"content":"Although Github Actions service is generally available since November 13, 2020, and there are about 243,000,000 results for \u0026ldquo;github actions\u0026rdquo; in Google search already, I have just reached it\u0026hellip;\nIt\u0026rsquo;s half past midnight, it took me about 35 commits to make my first github automation work, but it finally works and this blog post was built and published automatically!\nActions everywhere One of the most (or maybe the most one) powerful things in Actions is \u0026hellip; Actions! Github made a simple but genius thing: they turned well-known snippets (we do with pipelines) into the marketplace of well-made (sometimes not) simple and complex applications you can use in your automation workflow. https://github.com/marketplace?type=actions\nSo now you can either re-invent your wheel or re-use someone else\u0026rsquo;s code to make the needed automation.\nI decided to automate publications to this blog via Actions in order to have some practice.\nThere are two workflows: one for the blog (website), and one for the CV (cv).\n actions/checkout@v2 actions/upload-artifact@v2 actions/download-artifact@v2  In both workflows, the build job is performed within a container, which is different per workflow: Ruby for the blog and Pandoc for CV.\nHere is how the build job looks like for the blog:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  jobs:build:runs-on:ubuntu-latestcontainer:image:ruby:2.6.4options:--workdir /src steps:- name:Checkoutuses:actions/checkout@v2 - name:Build blogrun:|bundle install bundle exec jekyll build --verbose --destination _site- name:Upload artifactsuses:actions/upload-artifact@v2with:name:_sitepath:_site  As you can see, I run the steps within the Ruby container. This simplifies things related to file permissions and directory mounting because checkout is made inside the container.\nThe deploy step is performed via shell run command for now, for better clearness (can be replaced to third-party action or custom-made one): it makes a commit to gh-pages branch which is configured for Github Pages.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  deploy:if:github.ref == \u0026#39;refs/heads/master\u0026#39;needs:buildruns-on:ubuntu-lateststeps:- name:Checkout gh-pages branchuses:actions/checkout@v2with:ref:\u0026#39;gh-pages\u0026#39;- name:Get the build artifactuses:actions/download-artifact@v2with:name:_sitepath:./- name:Deploy (push) to gh-pagesrun:|git config user.name \u0026#34;$GITHUB_ACTOR\u0026#34; git config user.email \u0026#34;${GITHUB_ACTOR}@bots.github.com\u0026#34; git add -A git commit -a -m \u0026#34;Updated Website\u0026#34; git remote set-url origin \u0026#34;https://x-access-token:${{ secrets.DEPLOY_TOKEN }}@github.com/vasylenko/serhii.vasylenko.info.git\u0026#34; git push --force-with-lease origin gh-pages  Old good things made better A lot of common things have been introduced to GitHubActions with some sweet additions:\n you can also specify different environments for your jobs in the same workflow; you can use environment variables with a different visibility scope: either workflow, or job, or step; you can use cache for dependencies and reuse it between workflow runs while keeping workflow directory clean; you can trigger a workflow by repo events and have a quite complex conditional logic or filters (if needed), external webhooks and by a schedule; you can pass artifacts between jobs inside a workflow with ease - Github provides simple actions for this, so you don\u0026rsquo;t need to dance around temporary directories or files; and much more  ","permalink":"https://serhii.vasylenko.info/2020/03/18/github-actions-first-impression.html","summary":"Although Github Actions service is generally available since November 13, 2020, and there are about 243,000,000 results for \u0026ldquo;github actions\u0026rdquo; in Google search already, I have just reached it\u0026hellip;\nIt\u0026rsquo;s half past midnight, it took me about 35 commits to make my first github automation work, but it finally works and this blog post was built and published automatically!\nActions everywhere One of the most (or maybe the most one) powerful things in Actions is \u0026hellip; Actions!","title":"Github Actions - First impression"},{"content":"926 out of 1000 Last week I\u0026rsquo;ve successfully passed AWS SAA exam with 926 points from 1000 possible. I can\u0026rsquo;t help saying this and showing off my verification page{:target=\u0026quot;_blank\u0026quot;}, just because I am very happy so please excuse me my bragging.\nWhat helped me But I would like to share some advices and tips with anyone who reads this and wants to pass the exam. I mean, I could just twit about it if that was only about saying \u0026ldquo;hey look at me!\u0026rdquo;, right?\nIt took me a month of intensive studying and here is what helped me:\n  Video course at CloudGuru - AWS Certified Solutions Architect Associate{:target=\u0026quot;_blank\u0026quot;}.\nPrice: $50 for a monthly subscription.\nTips: They have a 7 days free trial, which is actually quite enough to view the whole course. But I strongly recommend purchasing a full month, because it is better to view the lectures gradually during couple of weeks for better learning. Plus they have a nice exam simulator where you can practice several times.\n  Practice Tests set at Udemy - AWS Certified Solutions Architect Associate Practice Exams{:target=\u0026quot;_blank\u0026quot;}.\nPrice: $40 or only $12 if you\u0026rsquo;re lucky to get it during a sale. But they make sales quite often and they frequently provide discuounts for new students. I purchaced it for $12.\nTips: practice tests are very useful, do not skip buying them. You will find your weak spots and also learn a lot by passing these tests. This particular set has a quite good explanations for each question.\n  Exam Guide at O\u0026rsquo;relly Media AWS Certified Solutions Architect Associate All-in-One Exam Guide{:target=\u0026quot;_blank\u0026quot;}.\nPrice: this one can be easily read during 10 days free trial period :wink:\nTips: The new exam version is released on 23rd of March, so it is better to find a new updated version of exam guide. And I suggest reading the guide after the video course or vise versa, but do not mix them.\n  Making notes. Seriously, note taking helps you memorize better. Do not skip it, and note your video courses as well as exam guide. Later, you will find your notes very helpful before the exam day - they will fresh up your memory.\n  Thank you for reading down to this point. I hope my advices were helpful and you will pass the exam!\n","permalink":"https://serhii.vasylenko.info/2020/03/15/aws-solutions-architect-associate-exam-tips.html","summary":"926 out of 1000 Last week I\u0026rsquo;ve successfully passed AWS SAA exam with 926 points from 1000 possible. I can\u0026rsquo;t help saying this and showing off my verification page{:target=\u0026quot;_blank\u0026quot;}, just because I am very happy so please excuse me my bragging.\nWhat helped me But I would like to share some advices and tips with anyone who reads this and wants to pass the exam. I mean, I could just twit about it if that was only about saying \u0026ldquo;hey look at me!","title":"AWS SAA exam results"},{"content":"So you opened this page to read more about me? I am very pleased! ü§ó üòä\nI live in Kyiv, Ukraine, but I like to travel to new, unseen places whenever I have enough time for that.\nTechnical blogging is my hobby, and I am also fond of astronomy and history.\nI am the AWS Community Builder and Developer Experience Engineer at Grammarly.\nAlso, you can find me here and there:\nTwitter | Dev | LinkedIn | Github | Facebook\nSome highlights of the things I do Webinars, workshops, presentations:\n  Terraform webinar (RU): Start using Terraform if you still don\u0026rsquo;t\n  AWS webinar (RU): The path to the certification of AWS architect \n  AWS webinar (RU): Infrastructure for Software Engineers: Spin Up and Keep It Running\n  Terraform crash course (RU) in Hillel IT school. Three 2-hours classes about Terraform and Terraform modules:\n Terraform ‚Äî Introduction (RU) Terraform ‚Äî Modules (RU) Terraform ‚Äî Modules practice (RU)    And this is me chasing the ocean waves on Iceland\u0026rsquo;s volcanic sand beach\n  ","permalink":"https://serhii.vasylenko.info/about/","summary":"So you opened this page to read more about me? I am very pleased! ü§ó üòä\nI live in Kyiv, Ukraine, but I like to travel to new, unseen places whenever I have enough time for that.\nTechnical blogging is my hobby, and I am also fond of astronomy and history.\nI am the AWS Community Builder and Developer Experience Engineer at Grammarly.\nAlso, you can find me here and there:","title":"About me"},{"content":"Developer Experience engineer, DevOps enthusiast. In 2020 I switched from team management to an individual contributor role to feel the engineering world on the tip of my fingers again.\nCertified: AWS Solutions Architect Associate, Terraform Associate.\n From Nov'20 Developer Experience Engineer at Grammarly Making day-to-day work of developers more productive and agile by:\n Providing automation and tooling for routine SDLC procedures Enriching experience in Infrastructure, Automation, Observability areas Standardizing commonly used infrastructure components   From July'20 DevOps Coach at Hillel IT School (part-time) Teaching DevOps and doing workshops about AWS and Terraform.\n Mar'17 ‚Äì Jun'20 DevOps Teamlead at ITCraft My biggest project ‚Äî my team. A skilled and highly motivated team of engineers that sincerely cares about its work outcomes.\nKey accomplishments:  Created a team of engineers that completed more than 40 successful projects of different sizes; Team size growth: from 1 to 8; Tripled the number of shared projects with other departments by boosting department reputation and team visibility within the company; Introduced DevOps as a Service business model in the department; Mentored senior and middle engineers from junior newcomers; Fostered the culture of ownership.  Challenges:  Transform team members' mindset toward ownership culture; Keep the team motivated despite workload level: during peaks and drawdowns; Justification of the advantages of my team to the clients on the pre-sales stage.   Nov'14 ‚Äì Sep'16 Chief Technologist at YourServerAdmin I was responsible for the technical side of processes inside the department and researching the new technologies we could implement for our clients.\nIntroduced ITIL practices together with our COO and, what I like the most, could change how our sysadmin\u0026rsquo;s department worked with the development department. The work became more coordinated and more integrated.\nAlso, I managed the projects using practices from PRINCE2 methodology and the Theory of Constraints.\nKey accomplishments:  Improved technical expertise of a support team; Improved task management; Increased team size; Fostered the change of team mindset to Agile thinking.   Jun'11 ‚Äì Nov'14 System Administrator / Support Engineer at YourServerAdmin Started my career in IT: grew up from a Level-1 tech support engineer (communication with customers and initial problem analysis) to Level-3 System Administrator responsible for complex technical tasks and daily shift management.\n  Skills  Leader Team motivation, mentorship, cultivation of soft/hard skills of team members. Team Fostering team values and working principles and developing the new ones together with the team.\nHiring new team members and forming the required team skillset. PM Managing stand-alone DevOps projects and operations parts of big projects with dev teams.\nResources and capacity management. Clouds Amazon Web Services. Design and fine-tuning of resilient and HA infrastructures; costs optimization and security hardening. 4 years of hands-on experience. Techs Jenkins, GitlabCI, Github Actions, Bitrise, CircleCI.\nTerraform, Docker, Ansible, Nginx, Apache, Databases. English C1 (CEFR) / Advanced.  Activities and interests  Travel I like to explore new cities and countries, and I like trekking. I\u0026rsquo;ve been to the Annapurna base camp, and now I want to visit Everest\u0026rsquo;s base camp. Blogging I love to write tutorials and articles about the technologies I use and learn. Science Fond of History, Astronomy, and Physics. I wish to see the Betelgeuse supernova explosion someday, even though the expected explosion date is somewhere between today and 100k years.  ","permalink":"https://serhii.vasylenko.info/cv/","summary":"Serhii Vasylenko ‚Äî professional experience","title":"Serhii Vasylenko"}]