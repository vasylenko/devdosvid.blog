[{"content":"Here I want to share two Apple Shortcuts that I created for myself and use to process images for this blog:\nImage Optimization\nand\nImage Resize\nAbout a year ago, I posted the blog about the Automator quick action to optimize PNG and JPEG images with TinyPNG service and save the processed images next to the original ones.\nWhile that Automator-based solution still works, macOS Monterey now supports Shortcuts ‚Äî a new automating tool that seems to substitute the old fellow Automator.\nSo I decided to create a couple of automation with Shortcuts: one for image optimization (reduce file size but not the size in pixels), and one for image scaling (change its size in pixels).\nI have used them for several months to prepare images for this blog, and I really like how they work!\nImage Optimization with Monterey Shortcuts This Shortcut replicates the functionality of the Automator quick action and also uses TinyPNG service as a back-end. There are tons of other similar services, but I like TinyPNG for its simplicity: it just does one thing, and it does it well.\nSo first, you need to get yourself an API key for TinyPNG.\nThe simplest way to reuse my Shortcut is to import it from iCloud using the following URL:\n‚û°Ô∏è Click here to import the Image Optimization Shortcut ‚¨ÖÔ∏è\nThe import will work only when the link is opened in Safari.\n  Shortcut Import Dialog\n  To make the Shortcut work from the context menu of the Finder, set the options on the Details panel of the Shortcut setting as displayed on the screenshot:\n  Shortcut Settings\n  Here is what this Image Optimization Shortcut does:\n The Shortcut receives image files. Then, for every image file received, the Shotcut does the following:\n Gets the file\u0026rsquo;s name and parent directory Sends the original file to TinyPNG Process the response with URL to download the optimized image Downloads the optimized image using the URL from the response and replaces the original image with the optimized   And here is how this Shortcuts looks if you want to create it from scratch:\n  Image Optimization Shortcut\n  Unfortunately, this Shortcut won\u0026rsquo;t work on iOS or watchOS because they do not support the \u0026ldquo;File Storage\u0026rdquo; actions used in the Shortcut.\nüåü Demo üåü   Shortcut Demo\n  Image Resize with Monterey Shortcuts Another Shortcut I actively use is the image resizer. Most of the images on my blog are 1600px width fitted into an 800px frame to look sharp on the high-res displays (e.g., Retina).\nAnd when I have many images in my folder, I want to make them all be 1600px width at once or don\u0026rsquo;t change their own size if they were created smaller intentionally (no upscale, in other words).\nHere is the link to Shortcut import (again, the import will work only when the link is opened in Safari):\n‚û°Ô∏è Click here to import the Image Resize Shortcut ‚¨ÖÔ∏è\nHere is how the Image Resize Shortcut looks if you want to create it from scratch:\n  Image Resizing Shortcut\n  Fun with Shortcuts I love the way Apple works on routine automation. This Shortcuts app, ported from iOS, brings a lot of cool and fun possibilities to Mac.\nDo you use Shortcuts? What is your favorite? I would love to know!\n","permalink":"https://serhii.vasylenko.info/2022/01/31/monterey-shortcuts-for-easy-and-fast-image-processing/","summary":"Here I want to share two Apple Shortcuts that I created for myself and use to process images for this blog:\nImage Optimization\nand\nImage Resize\nAbout a year ago, I posted the blog about the Automator quick action to optimize PNG and JPEG images with TinyPNG service and save the processed images next to the original ones.\nWhile that Automator-based solution still works, macOS Monterey now supports Shortcuts ‚Äî a new automating tool that seems to substitute the old fellow Automator.","title":"Monterey Shortcuts for Easy and Fast Image Processing"},{"content":"Terraform built-in functionality is very feature-rich: functions, expressions, and meta-arguments provide many ways to shape the code and fit it to a particular use case. I want to share a few valuable practices to boost your Terraform expertise in this blog.\nSome code examples in this article will work with Terraform version 0.15 and onwards. But if you\u0026rsquo;re still using 0.14 or lower, here\u0026rsquo;s another motivation for you to upgrade.  Conditional resources creation   Let\u0026rsquo;s start from the most popular one (although, still may be new for somebody): whether to create a resource depending on some fact, e.g., the value of a variable. Terraform meta-argument count helps to describe that kind of logic.\nHere is how it may look like:\ndata \u0026#34;aws_ssm_parameter\u0026#34; \u0026#34;ami_id\u0026#34; { count = var.ami_channel == \u0026#34;\u0026#34; ? 0 : 1 name = local.ami_channels[var.ami_channel] } The notation var.ami_channel == \u0026quot;\u0026quot; ? 0 : 1 is called conditional expression and means the following: if my variable is empty (var.ami_channel == \u0026quot;\u0026quot; ‚Äî hence, true) then set the count to 0, otherwise set to 1.\ncondition ? true_val : false_val In this illustration, I want to get the AMI ID from the SSM Parameter only if the AMI channel (e.g., beta or alpha) is specified. Otherwise, providing that the ami_channel variable is an empty string by default (\u0026quot;\u0026quot;), the data source should not be created.\nWhen following this method, keep in mind that the resource address will contain the index identifier. So when I need to use the value of the SSM parameter from our example, I need to reference it the following way:\nami_id = data.aws_ssm_parameter.ami_id[0].value The count meta-argument can also be used when you need to conditionally create a Terraform module.\nmodule \u0026#34;bucket\u0026#34; { count = var.create_bucket == true ? 1 : 0 source = \u0026#34;./modules/s3_bucket\u0026#34; name = \u0026#34;my-unique-bucket\u0026#34; ... } The var.create_bucket == true ? 1 : 0 expression can be written even shorter: var.create_bucket ? 1 : 0 because the create_bucket variable has boolean type, apparently.\nBut what if you need to produce more than one instance of a resource or module? And still be able to avoid their creation.\nAnother meta-argument ‚Äî for_each ‚Äî will do the trick.\nFor example, this is how it looks for a module:\nmodule \u0026#34;bucket\u0026#34; { for_each = var.bucket_names == [] ? [] : var.bucket_names source = \u0026#34;./modules/s3_bucket\u0026#34; name = \u0026#34;${each.key}\u0026#34; enable_encryption = true ... } In this illustration, I also used a conditional expression that makes Terraform iterate through the set of values of var.bucket_names if it\u0026rsquo;s not empty and create several modules. Otherwise, do not iterate at all and do not create anything.\nThe same can be done for the resources. For example, when you need to create an arbitrary number of security group rules, e.g., to allowlist some IPs for your bastion host:\nresource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;allowlist\u0026#34; { for_each = var.cidr_blocks == [] ? [] : var.cidr_blocks type = \u0026#34;ingress\u0026#34; from_port = 22 to_port = 22 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [each.value] security_group_id = aws_security_group.bastion.id } And just like with the count meta-argument, with the for_each, resource addresses will have the identifier named by the values provided to for_each. For example, here is how I would reference a resource created in the module with for_each described earlier:\nbucket_name = module.bucket[\u0026#34;photos\u0026#34;].name Conditional resource arguments (attributes) setting   Now let\u0026rsquo;s go deeper and see how resource arguments can be conditionally set (or not). First, let\u0026rsquo;s review the conditional argument value setting with the null data type:\nresource \u0026#34;aws_launch_template\u0026#34; \u0026#34;this\u0026#34; { name = \u0026#34;my-launch-template\u0026#34; ... key_name = var.use_default_keypair ? var.keypair_name : null ... Here I want to skip the usage of the EC2 Key Pair for the Launch Template in some instances and Terraform allows me to write the conditional expression that will set the null value for the argument. It means the absence or omission and Terraform would behave the same as if you did not specify the argument at all.\nDynamic blocks are another case where conditional creation suits best. Take a look at the following piece of CloudFront resource code where I want to either describe the configuration for the custom error response or omit that completely:\nresource \u0026#34;aws_cloudfront_distribution\u0026#34; \u0026#34;cdn\u0026#34; { enabled = true ... dynamic \u0026#34;custom_error_response\u0026#34; { for_each = var.custom_error_response == null ? [] : [var.custom_error_response] iterator = cer content { error_code =lookup(cer.value, \u0026#34;error_code\u0026#34;, null) error_caching_min_ttl =lookup(cer.value, \u0026#34;error_caching_min_ttl\u0026#34;, null) response_code =lookup(cer.value, \u0026#34;response_code\u0026#34;, null) response_page_path =lookup(cer.value, \u0026#34;response_page_path\u0026#34;, null) } } ... } The custom_error_response variable is null by default, but it has the object type, and users can assign the variable with the required nested specifications if needed. And when they do it, Terraform will add the custom_error_response block to the resource configuration. Otherwise, it will be omitted entirely.\nConvert types with ease   Ok, let\u0026rsquo;s move to the less conditional things now üòÖ\nTerraform has several type conversion functions: tobool(), tolist(),tomap(), tonumber(), toset(), and tostring(). Their purpose is to convert the input values to the compatible types.\nFor example, suppose I need to pass the set to the for_each (it accepts only sets and maps types of value), but I got the list as an input; let\u0026rsquo;s say I got it as an output from another module. In such a case, I would do something like this:\nfor_each = toset(var.remote_access_ports) However, I can make my code cleaner and avoid the explicit conversion ‚Äî I just need to define the value type in the configuration block of the my_list variable. Terraform will do the conversion automatically when the value is assigned.\nvariable \u0026#34;remote_access_ports\u0026#34; { description = \u0026#34;Ports for remote access\u0026#34; type = set(string) } While Terraform can do a lot of implicit conversions for you, explicit type conversions are practical during values normalization or when you need to calculate some complex value for a variable. For example, the Local Values, known as locals, are the most suitable place for doing that.\nBy the way, although there is a tolist() function, there is no such thing as the tostring() function. But what if you need to convert the list to string in Terraform?\nThe one() function can help here: it takes a list, set, or tuple value with either zero or one element and returns either null or that one element in the form of string.\nIt\u0026rsquo;s useful in cases when a resource created using conditional expression is represented as either a zero- or one-element list, and you need to get a single value which may be either null or string, for example:\nresource \u0026#34;aws_kms_key\u0026#34; \u0026#34;main\u0026#34; { count = var.ebs_encrypted ? 1 : 0 enable_key_rotation = true tags = var.tags } resource \u0026#34;aws_kms_alias\u0026#34; \u0026#34;main\u0026#34; { count = var.ebs_encrypted ? 1 : 0 name = \u0026#34;alias/encrypt-ebs\u0026#34; target_key_id = one(aws_kms_key.main[*]key_id) } Write YAML or JSON as Terraform code (HCL)   Sometimes you need to supply JSON or YAML files to the services you manage with Terraform. For example, if you want to create something with CloudFormation using Terraform (and I am not kidding). Sometimes the AWS Terraform provider does not support the needed resource, and you want to maintain the whole infrastructure code using only one tool.\nInstead of maintaining another file in JSON or YAML format, you can embed JSON or YAML code management into HCL by taking benefit of the jsonencode() or yamlencode() functions.\nThe attractiveness of this approach is that you can reference other Terraform resources or their attributes right in the code of your object, and you have more freedom in terms of the code syntax and its formatting comparable to native JSON or YAML.\nHere is how it looks like:\nlocals { some_string = \u0026#34;ult\u0026#34; myjson_object =jsonencode({ \u0026#34;Hashicorp Products\u0026#34;: { Terra: \u0026#34;form\u0026#34; Con: \u0026#34;sul\u0026#34; Vag: \u0026#34;rant\u0026#34; Va: local.some_string } }) } The value of the myjson_object local variable would look like this:\n{ \u0026#34;Hashicorp Products\u0026#34;: { \u0026#34;Con\u0026#34;: \u0026#34;sul\u0026#34;, \u0026#34;Terra\u0026#34;: \u0026#34;form\u0026#34;, \u0026#34;Va\u0026#34;: \u0026#34;ult\u0026#34;, \u0026#34;Vag\u0026#34;: \u0026#34;rant\u0026#34; } } And here is a piece of real-world example:\nlocals { cf_template_body =jsonencode({ Resources : { DedicatedHostGroup : { Type : \u0026#34;AWS::ResourceGroups::Group\u0026#34; Properties : { Name : var.service_name Configuration : [ { Type : \u0026#34;AWS::EC2::HostManagement\u0026#34; Parameters : [ { Name : \u0026#34;auto-allocate-host\u0026#34; Values : [var.auto_allocate_host] }, ... ... Templatize stuff   The last case in this blog but not the least by its efficacy ‚Äî render source file content as a template in Terraform.\nLet\u0026rsquo;s review the following scenario: you launch an EC2 instance and want to supply it with a bash script (via the user-data parameter) for some additional configuration at launch.\nSuppose we have the following bash script instance-init.sh that sets the hostname and registers our instance in a monitoring system:\n#!/bin/bash  hostname example.com bash /opt/system-init/register-monitoring.sh But what if you want to set a different hostname per instance, and some instances should not be registered in the monitoring system?\nIn such a case, here is how the script file content will look:\n#!/bin/bash hostname ${system_hostname} %{ if register_monitoring } bash /opt/system-init/register-monitoring.sh %{endif} And when you supply this file as an argument for the EC2 instance resource in Terraform, you will use the templatefile() function to make the magic happen:\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;web\u0026#34; { ami = var.my_ami_id instance_type = var.instance_type ... user_data = templatefile(\u0026#34;${path.module}/instance-init.tftpl\u0026#34;, { system_hostname = var.system_hostname register_monitoring = var.add_to_monitoring }) ... } And of course, you can create a template from any file type. The only requirement here is that the template file must exist on the disk at the beginning of the Terraform execution.\nKey takeaways Terraform is far beyond the standard resource management operations. With the power of built-in functions, you can write more versatile code and reusable Terraform modules.\n‚úÖ Use conditional expressions with count and for_each meta-arguments, when the creation of a resource depends on some context or user input.\n‚úÖ Take advantage of implicit type conversion when working with input variables and their values to keep your code cleaner.\n‚úÖ Embed YAML and JSON-based objects right into your Terraform code using built-in encoding functions.\n‚úÖ And when you need to pass some files to the managed service, you can treat them as templates and make them multipurpose.\nThank you for reading down to this point! ü§ó\nIf you have some favorite Terraform tricks ‚Äî I would love to know!\n","permalink":"https://serhii.vasylenko.info/2022/01/16/some-techniques-to-enhance-your-terraform-proficiency/","summary":"Terraform built-in functionality is very feature-rich: functions, expressions, and meta-arguments provide many ways to shape the code and fit it to a particular use case. I want to share a few valuable practices to boost your Terraform expertise in this blog.\nSome code examples in this article will work with Terraform version 0.15 and onwards. But if you\u0026rsquo;re still using 0.14 or lower, here\u0026rsquo;s another motivation for you to upgrade.  Conditional resources creation   Let\u0026rsquo;s start from the most popular one (although, still may be new for somebody): whether to create a resource depending on some fact, e.","title":"Some Techniques to Enhance Your Terraform Proficiency"},{"content":"Terraform by itself automates a lot of things: it creates, changes, and versions your cloud resources. Although many teams run Terraform locally (sometimes with wrapper scripts), running Terraform in CI/CD can boost the organization\u0026rsquo;s performance and ensure consistent deployments.\nIn this article, I would like to review different approaches to integrating Terraform into generic deployment pipelines.\nWhere to store the Terraform code Storing Terraform code in the same repository as the application code or maintaining a separate repository for the infrastructure?\nThis question has no strict and clear answer, but here are some insights that may help you decide:\n The Terraform and application code coupled together represent one unit, so it\u0026rsquo;s simple to maintain by one team; Conversely, if you have a dedicated team that manages infrastructure (e.g., platform team), a separate repository for infrastructure is more convenient because it\u0026rsquo;s a standalone project in that case. When infrastructure code is stored with the application, sometimes you have to deal with additional rules for the pipeline to separate triggers for these code parts. But sometimes (e.g., serverless apps) changes to either part (app/infra) should trigger the deployment.  There is no right or wrong approach, but whichever you choose, remember to follow the Don‚Äôt Repeat Yourself (DRY) principle: make the infrastructure code modular by logically grouping resources into higher abstractions and reusing these modules.  Preparing Terraform execution environment Running Terraform locally generally means that all dependencies are already in-place: you have the binary installed and present in the user\u0026rsquo;s PATH and perhaps even some providers already stored in the .terraform directory.\nBut when you shift Terraform runs from your local machine to stateless pipelines, this is not the case. However, you can still have a pre-built environment ‚Äî this will speed up the pipeline execution and provide control over the process.\nDocker image with a Terraform binary is one of the popular solutions that address this. Once created, you can execute Terraform within a container context with configuration files mounted as a Docker volume.\nYou can use the official image from Hashicorp, but sometimes it makes sense to maintain your own Docker images with additional tools you may need. For instance, you can bake the tfsec tool into the image to use it for security inspection and have it ready inside the Docker container without the need to install it every time.\nHere is an example of a Dockerfile that builds an image with a custom Terraform version (you can override it as a build argument) and a tfsec tool. This example also shows how to verify the installed Terraform binary to make sure it\u0026rsquo;s signed by HashiCorp before we run it.\nFROMalpine:3.14ARG TERRAFORM_VERSION=1.0.11ARG TFSEC_VERSION=0.59.0RUN apk add --no-cache --virtual .sig-check gnupgRUN wget -O /usr/bin/tfsec https://github.com/aquasecurity/tfsec/releases/download/v${TFSEC_VERSION}/tfsec-linux-amd64 \\  \u0026amp;\u0026amp; chmod +x /usr/bin/tfsecRUN cd /tmp \\  \u0026amp;\u0026amp; wget \u0026#34;https://releases.hashicorp.com/terraform/${TERRAFORM_VERSION}/terraform_${TERRAFORM_VERSION}_linux_amd64.zip\u0026#34; \\  \u0026amp;\u0026amp; wget https://keybase.io/hashicorp/pgp_keys.asc \\  \u0026amp;\u0026amp; gpg --import pgp_keys.asc \\  \u0026amp;\u0026amp; gpg --fingerprint --list-signatures \u0026#34;HashiCorp Security\u0026#34; | grep -q \u0026#34;C874 011F 0AB4 0511 0D02 1055 3436 5D94 72D7 468F\u0026#34; || exit 1 \\  \u0026amp;\u0026amp; gpg --fingerprint --list-signatures \u0026#34;HashiCorp Security\u0026#34; | grep -q \u0026#34;34365D9472D7468F\u0026#34; || exit 1 \\  \u0026amp;\u0026amp; wget https://releases.hashicorp.com/terraform/${TERRAFORM_VERSION}/terraform_${TERRAFORM_VERSION}_SHA256SUMS \\  \u0026amp;\u0026amp; wget https://releases.hashicorp.com/terraform/${TERRAFORM_VERSION}/terraform_${TERRAFORM_VERSION}_SHA256SUMS.sig \\  \u0026amp;\u0026amp; gpg --verify terraform_${TERRAFORM_VERSION}_SHA256SUMS.sig terraform_${TERRAFORM_VERSION}_SHA256SUMS || exit 1 \\  \u0026amp;\u0026amp; sha256sum -c terraform_${TERRAFORM_VERSION}_SHA256SUMS 2\u0026gt;\u0026amp;1 | grep -q \u0026#34;terraform_${TERRAFORM_VERSION}_linux_amd64.zip: OK\u0026#34; || exit 1 \\  \u0026amp;\u0026amp; unzip terraform_${TERRAFORM_VERSION}_linux_amd64.zip -d /bin \\  \u0026amp;\u0026amp; rm -rf /tmp/* \u0026amp;\u0026amp; apk del .sig-checkBut the main functionality of Terraform is delivered by provider plugins. It takes time to download the provider: for example, the AWS provider is about 250MB, and in a large scale, with hundreds of Terraform runs per day, this makes a difference.\nThere are two common ways to deal with it: either use a shared cache available to your pipeline workloads or bake provider binaries into the runtime environment (i.e., Docker image).\nThe critical element for both approaches is the configuration of the plugin cache directory path. By default, Terraform looks for plugins and downloads them in the .terraform directory, which is local to the main project directory. But you can override this, and you can leverage the TF_PLUGIN_CACHE_DIR environment variable to do that.\nIf supported by your CI/CD tool, the shared cache can significantly reduce the operational burden because all your pipeline runtime environments can use it to get the needed provider versions.\nSo all you have to do is to maintain the provider versions in the shared cache and instruct Terraform to use it:\n Mount the cache directory to the pipeline runtime (i.e., docker container) and specify its internal path Set the value of the TF_PLUGIN_CACHE_DIR environment variable accordingly  On the other hand, you can bake the provider binaries into the Docker image and inject the value for the TF_PLUGIN_CACHE_DIR environment variable right into the Dockerfile.\nThis approach takes more operational effort but makes the Terraform environment self-sufficient and stateless. It also allows you to set strict boundaries around permitted provider versions as a security measure.  Planning and Applying changes Now let\u0026rsquo;s review the ways to automate planning and applying of changes. Although terraform apply can do both, it\u0026rsquo;s sometimes useful to separate these actions.\nInitialization CI/CD pipelines generally run in stateless environments. Thus, every subsequent run of Terraform looks like a fresh start, so the project needs to be initialized before other actions can be performed.\nThe usage of the init command in CI/CD slightly differs from its common local usage:\n\u0026gt; terraform init -input=false The -input=false option prevents Terraform CLI from asking for user actions (it will throw an error if the input was required).\nAlso, there is -no-color option that prevents the usage of color codes in a shell, so the output will look much cleaner if your CI/CD logging system cannot render the terminal formatting.\nAnother option of the init command that is useful in CI ‚Äî is the -backend-config. That option allows you to override the backend configuration in your code or define it if you prefer to use partial configuration, thus creating more uniform pipelines.\nFor example, here is how you can use the same code with different roles in different environments on AWS:\n\u0026gt; terraform init -input=false \\ -backend-config=\u0026#34;role_arn=arn:aws:iam::012345678901:role/QADeploymentAutomation\u0026#34; Terraform init produces two artifacts:\n .terraform directory, which Terraform uses to manage cached provider plugins and modules, and record backend information .terraform.lock.hcl file, which Terraform uses to track provider dependencies  They both must be present in the project directory to successfully run the subsequent plan and apply commands.\nHowever, I suggest checking in .terraform.lock.hcl to your repository as suggested by HashiCorp (Dependency Lock File): this way you will be able to control dependencies more thoroughly, and you will not worry about transferring this file between build stages.\nPlan The terraform plan command helps you validate the changes manually. However, there are ways to use it in automation as well.\nBy default, Terraform prints the plan output in a human-friendly format but also supports machine-readable JSON. With additional command-line options, you can extend your CI experience.\nFor example, you can use your validation conditions to decide whether to apply the changes automatically; or you can parse the plan details and integrate the summary into a Pull Request description. Let‚Äôs review a simple example that illustrates it.\nFirst, you need to save the plan output to the file:\n\u0026gt; terraform plan -input=false -compact-warnings -out=plan.file The main point here is the -out option ‚Äî it tells Terraform to save its output into a binary plan file, and we will talk about it in the next paragraph.\nThe -compact-warnings option suppresses the warning-level messages produced by Terraform.\nAlso, the plan command has the -detailed-exitcode option that returns detailed exit codes when the command exits. For example, you can leverage this in a script that wraps Terraform and adds more conditional logic to its execution, because CIs will generally fail the pipeline on a command‚Äôs non-zero exit code. However, that may add complexity to the pipeline logic.\nSo if you need to get detailed info about the plan, I suggest parsing the plan output.\nWhen you have a plan file, you can read it in JSON format and parse it. Here is a code snippet that illustrates that:\n\u0026gt; terraform show -json plan.file| jq -r \u0026#39;([.resource_changes[]?.change.actions?]|flatten)|{\u0026#34;create\u0026#34;:(map(select(.==\u0026#34;create\u0026#34;))|length),\u0026#34;update\u0026#34;:(map(select(.==\u0026#34;update\u0026#34;))|length),\u0026#34;delete\u0026#34;:(map(select(.==\u0026#34;delete\u0026#34;))|length)}\u0026#39; { \u0026#34;create\u0026#34;: 1, \u0026#34;update\u0026#34;: 0, \u0026#34;delete\u0026#34;: 0 } Another way to see the information about changes, is to run the plan command with -json option and parse its output to stdout (available starting from Terraform 1.0.5):\n\u0026gt; terraform plan -json|jq \u0026#39;select( .type == \u0026#34;change_summary\u0026#34;)|.\u0026#34;@message\u0026#34;\u0026#39; \u0026#34;Plan: 1 to add, 0 to change, 0 to destroy.\u0026#34; This technique can make your Pull Request messages more informative and improve your collaboration with teammates.  You can write a custom script/function that sends a Pull Request comment to VCS using its API. Or you can try the existing features of your VCS: with GitHub Actions, you can use the Terraform PR Commenter or similar action to achieve that; for GitLab, there is a built-in functionality that integrates plan results into the Merge Request ‚Äî Terraform integration in Merge Requests.\nYou can find more information about the specification of the JSON output here ‚Äî Terraform JSON Output Format.\nApply When the plan file is ready, and the proposed changes are expected and approved, it\u0026rsquo;s time to apply them.\nHere is how the apply command may look like in automation:\nterraform apply -input=false -compact-warnings plan.file Here, the plan.file is the file we got from the previous plan step.\nAlternatively, you might want to omit the planning phase at all. In that case, the following command will apply the configuration immediately, without the need for a plan:\nterraform apply -input=false -compact-warnings -auto-approve Here, the -auto-approve option tells Terraform to create the plan implicitly and skip the interactive approval of that plan before applying.\nWhichever way you choose, keep in mind the destructive nature of the apply command. Hence, the fully automated apply of configuration generally works well with environments that tolerate unexpected downtimes, such as development or testing. Whereas plan review is recommended for production-grade environments, and in that case, the apply job is configured for a manual trigger.\nDealing with stateless environments If you run init, plan, and apply commands in different environments, you need to care for some artifacts produced by Terraform:\n The .terraform directory with information about modules, providers, and the state file (even in the case of remote state). The .terraform.lock.hcl file ‚Äî the dependency lock file which Terraform uses to check the integrity of provider versions used for the project. If your VCS does not track it, you\u0026rsquo;ll need to pass that file to the plan and apply commands to make them work after init. The output file of the plan command is essential for the apply command, so treat it as a vital artifact. This file includes a full copy of the project configuration, the state, and variables passed to the plan command (if any). Therefore, mind the security precautions because sensitive information may be present there.  There is one shortcut, though. You can execute the init and plan commands within the same step/stage and transfer the artifacts only once ‚Äî to the apply execution.\nUsing the command-line and environments variables Last but not least, a few words about ways to maximize the advantage of variables when running Terraform in CI.\nThere are two common ways how you can pass values for the variables used in the configuration:\n Using a -var-file option with the variable definitions file ‚Äî a filename ending in .tfvars or .tfvars.json. For example: terraform apply -var-file=development.tfvars -input=false -no-color -compact-warnings -auto-approve Also, Terraform can automatically load the variables from files named exactly terraform.tfvars or terraform.tfvars.json: with that approach, you don‚Äôt need to specify the tfvar file as a command option explicitly. Using environment variables with the prefix TF_VAR_. Implicitly, Terraform always looks for the environment variables (within its process context) with that prefix, so the same \u0026ldquo;instance_type\u0026rdquo; variables from the example above can be passed as follows: export TF_VAR_instance_type=t3.nano terraform -input=false -no-color -compact-warnings -auto-approve   The latter method is widely used in CI because modern CI/CD tools support the management of the environment variables for automation jobs.\nPlease refer to the following official documentation if you want to know more about variables ‚Äî Terraform Input Variables.\nAlong with that, Terraform supports several configuration parameters in the form of environment variables. These parameters are optional; however, they can simplify the automation management and streamline its code.\n TF_INPUT ‚Äî when set to \u0026ldquo;false\u0026rdquo; or \u0026ldquo;0\u0026rdquo;, this tells Terraform to behave the same way as with the -input=false flag; TF_CLI_ARGS ‚Äî can contain a set of command-line options that will be passed to one or another Terraform command. Therefore, the following notation can simplify the execution of apply and plan commands by unifying their options for CI: export TF_CLI_ARGS=\u0026#34;-input=false -no-color -compact-warnings\u0026#34; terraform plan ... terraform apply ... You can advantage this even more when using this variable as the environment configuration of stages or jobs in a CI/CD tool. TF_IN_AUTOMATION ‚Äî when set to any non-empty value (e.g., \u0026ldquo;true\u0026rdquo;), Terraform stops suggesting commands run after the one you execute, hence producing less output.  Key takeaways There are two primary outcomes from automating Terraform executions: consistent results and integrating with the code or project management solutions. Although the exact implementation of Terraform in CI may vary per project or team, try to aim the following goals when working on it:\n Ease of code management A secure and controlled execution environment Coherent runs of init, plan, apply phases Leveraging of built-in Terraform capabilities  I originally wrote this article for the Spacelift.io technical blog. But I decided to keep it here as well, for the history. The canonical link to their blog has been set accordingly. ","permalink":"https://serhii.vasylenko.info/2021/11/24/guide-to-using-terraform-in-ci/cd/","summary":"Terraform by itself automates a lot of things: it creates, changes, and versions your cloud resources. Although many teams run Terraform locally (sometimes with wrapper scripts), running Terraform in CI/CD can boost the organization\u0026rsquo;s performance and ensure consistent deployments.\nIn this article, I would like to review different approaches to integrating Terraform into generic deployment pipelines.\nWhere to store the Terraform code Storing Terraform code in the same repository as the application code or maintaining a separate repository for the infrastructure?","title":"Guide to Using Terraform in CI/CD"},{"content":"In November 2021, AWS announced Response Headers Policies ‚Äî native support of response headers in CloudFront. You can read the full announcement here: Amazon CloudFront introduces Response Headers Policies\nI said \u0026ldquo;native\u0026rdquo; because previously you could set response headers either using CloudFront Functions or Lambda@Edge.\nAnd one of the common use cases for that was to set security headers. Now you don\u0026rsquo;t need to add intermediate requests processing to modify the headers: CloudFront does that for you with no additional fee.\nManage Security Headers as Code Starting from the 3.64.0 version of Terraform AWS provider, you can create the security headers policies and apply them for your distribution.\nLet\u0026rsquo;s see how that looks!\nFirst, you need to describe the aws_cloudfront_response_headers_policy resource:\nresource \u0026#34;aws_cloudfront_response_headers_policy\u0026#34; \u0026#34;security_headers_policy\u0026#34; { name = \u0026#34;my-security-headers-policy\u0026#34; security_headers_config { content_type_options { override = true } frame_options { frame_option = \u0026#34;DENY\u0026#34; override = true } referrer_policy { referrer_policy = \u0026#34;same-origin\u0026#34; override = true } xss_protection { mode_block = true protection = true override = true } strict_transport_security { access_control_max_age_sec = \u0026#34;63072000\u0026#34; include_subdomains = true preload = true override = true } content_security_policy { content_security_policy = \u0026#34;frame-ancestors \u0026#39;none\u0026#39;; default-src \u0026#39;none\u0026#39;; img-src \u0026#39;self\u0026#39;; script-src \u0026#39;self\u0026#39;; style-src \u0026#39;self\u0026#39;; object-src \u0026#39;none\u0026#39;\u0026#34; override = true } } } List of security headers used:\n X-Content-Type-Options X-Frame-Options Referrer Policy X-XSS-Protection Strict Transport Security Content Security Policy  The values for the security headers can be different, of course. However, the provided ones cover the majority of cases. And you can always get the up to date info about these headers and possible values here: Mozilla web Security Guidelines\nAlso, you could notice that provided example uses the override argument a lot. The override argument tells CloudFront to set these values for specified headers despite the values received from the origin. This way, you can enforce your security headers configuration.\nOnce you have the aws_cloudfront_response_headers_policy resource, you can refer to it in the code of aws_cloudfront_distribution resource inside cache behavior block (default or ordered). For example, in your default_cache_behavior:\nresource \u0026#34;aws_cloudfront_distribution\u0026#34; \u0026#34;test\u0026#34; { default_cache_behavior { target_origin_id = aws_s3_bucket.my_origin.id allowed_methods = [\u0026#34;GET\u0026#34;, \u0026#34;HEAD\u0026#34;, \u0026#34;OPTIONS\u0026#34;] cached_methods = [\u0026#34;GET\u0026#34;, \u0026#34;HEAD\u0026#34;] viewer_protocol_policy = \u0026#34;redirect-to-https\u0026#34;# some arguments skipped from listing for the sake of simplicity  response_headers_policy_id = aws_cloudfront_response_headers_policy.security_headers_policy.id }# some arguments skipped from listing for the sake of simplicity } Security Scan Results Here is what Mozilla Observatory reports about my test CF distribution where I enabled the policy described above:\n  Scan summary for CloudFront distribution with security headers policy\n  So with just minimum effort, you can greatly boost your web application security posture.\nMore to read:  Terraform Resource: aws_cloudfront_response_headers_policy Creating response headers policies - Amazon CloudFront Using the managed response headers policies - Amazon CloudFront Understanding response headers policies - Amazon CloudFront  ","permalink":"https://serhii.vasylenko.info/2021/11/05/apply-cloudfront-security-headers-with-terraform/","summary":"In November 2021, AWS announced Response Headers Policies ‚Äî native support of response headers in CloudFront. You can read the full announcement here: Amazon CloudFront introduces Response Headers Policies\nI said \u0026ldquo;native\u0026rdquo; because previously you could set response headers either using CloudFront Functions or Lambda@Edge.\nAnd one of the common use cases for that was to set security headers. Now you don\u0026rsquo;t need to add intermediate requests processing to modify the headers: CloudFront does that for you with no additional fee.","title":"Apply Cloudfront Security Headers With Terraform"},{"content":"It‚Äôs been almost a year since I started using macOS EC2 instances on AWS: there were ups and downs in service offerings and a lot of discoveries with macOS AMI build automation.\nAnd I like this small but so helpful update of EC2 service very much: with mac1.metal instances, seamless integration of Apple-oriented CI/CD with other AWS infrastructure could finally happen.\nWhile management of a single mac1.metal node (or a tiny number of ones) is not a big deal (especially when Dedicated Host support was added to Terraform provider), governing the fleet of instances is still complicated. Or it has been complicated until recent days.\nOfficial / Unofficial Auto Scaling for macOS With a growing number of instances, the following challenges arise:\n Scale mac1.metal instances horizontally Automatically allocate and release Dedicated Hosts needed for instances Automatically replace unhealthy instances  If you have worked with AWS before, you know that Auto Scaling Group service can solve such things.\nHowever, official documentation (as of October 2021) states: ‚ÄúYou cannot use Mac instances with Amazon EC2 Auto Scaling‚Äù.\nBut in fact, you can.\nCombining services to get real power So how does all that work?\nLet‚Äôs review the diagram that illustrates the interconnection between involved services:\n  Services logical interconnection\n  With the help of Licence Manager service and Launch Templates, you can set up EC2 Auto Scaling Group for mac1.metal and leave the automated instance provisioning to the service.\nLicense Configuration First, you need to create a License Configuration so that the Host resource group can allocate the hots.\nGo to AWS License Manager -\u0026gt; Customer managed licenses -\u0026gt; Create customer-managed license.\nSpecify Sockets as the Licence type. You may skip setting the Number of Sockets. However, the actual limit of mac1.metal instances per account is regulated by Service Quota. The default number of mac instances allowed per account is 3. Therefore, consider increasing this to a more significant number.\n  Licence configuration values\n  Host resource group Second, create the Host resource group: AWS License Manager -\u0026gt; Host resource groups -\u0026gt; Create host resource group.\nWhen creating the Host resource group, check ‚ÄúAllocate hosts automatically‚Äù and ‚ÄúRelease hosts automatically‚Äù but leave ‚ÄúRecover hosts automatically‚Äù unchecked. Dedicated Host does not support host recovery for mac1.metal. However, Auto Scaling Group will maintain the desired number of instances if one fails the health check (which assumes the case of host failure as well).\nAlso, I recommend specifying ‚Äúmac1‚Äù as an allowed Instance family for the sake of transparent resource management: only this instance type is permitted to allocate hosts in the group.\n  Host resource group configuration values\n  Optionally, you may specify the license association here (the Host group will pick any compatible license) or select the license you created on step one.\nLaunch Template Create Launch Template: EC2 -\u0026gt; Launch templates -\u0026gt; Create launch template.\nI will skip the description of all Launch Template parameters (but here is a nice tutorial), if you don‚Äôt mind, and keep focus only on the items relevant to the current case.\nSpecify mac1.metal as the Instance type. Later, in Advanced details: find the Tenancy parameter and set it to ‚ÄúDedicated host‚Äù; for Target host by select ‚ÄúHost resource group‚Äù, and once selected the new parameter Tenancy host resource group will appear where you should choose your host group; select your license in License configurations parameter.\n  Launch Template configuration values\n  Auto Scaling Group Finally, create the Auto Scaling Group: EC2 -\u0026gt; Auto Scaling groups -\u0026gt; Create Auto Scaling group.\nThe vital thing to note here ‚Äî is the availability of the mac1.metal instance in particular AZ.\nMac instances are available in us-east-1 and 7 more regions, but not every Availability Zone in the region supports it. So you must figure out which AZ supports the needed instance type.\nThere is no documentation for that, but there is an AWS CLI command that can answer this question: describe-instance-type-offerings ‚Äî AWS CLI 2.3.0 Command Reference\nHere is an example for the us-east-1 region: ‚û°Ô∏è Click here to see the code snippet ‚¨ÖÔ∏è \u0026gt; aws ec2 describe-instance-type-offerings --location-type availability-zone-id --filters Name=instance-type,Values=mac1.metal --region us-east-1 --output text INSTANCETYPEOFFERINGS\tmac1.metal\tuse1-az6\tavailability-zone-id INSTANCETYPEOFFERINGS\tmac1.metal\tuse1-az4\tavailability-zone-id   Keep that nuance in mind when selecting a subnet for the mac1.metal instances.\nWhen you know the AZ, specify the respective Subnet in the Auto Scaling Group settings, and you\u0026rsquo;re ready to go!\nBring Infrastructure as Code here I suggest describing all that as a code. I prefer Terraform, and its AWS provider supports the needed resources. Except one.\nAs of October 2021, resources supported :\n aws_servicequotas_service_quota aws_licensemanager_license_configuration aws_launch_template aws_autoscaling_group  The Host resource group is not yet supported by the provider, unfortunately. However, we can use CloudFormation in Terraform to overcome that: describe the Host resource group as aws_cloudformation_stack Terraform resource using CloudFormation template from a file.\nHere is how it looks like: ‚û°Ô∏è Click here to see the code snippet ‚¨ÖÔ∏è resource \u0026#34;aws_licensemanager_license_configuration\u0026#34; \u0026#34;this\u0026#34; { name = local.full_name license_counting_type = \u0026#34;Socket\u0026#34; } resource \u0026#34;aws_cloudformation_stack\u0026#34; \u0026#34;this\u0026#34; { name = local.full_name# the name of CloudFormation stack  template_body = file(\u0026#34;${path.module}/resource-group-cf-stack-template.json\u0026#34;) parameters = { GroupName = local.full_name# the name for the Host group, passed to CloudFormation template  } on_failure = \u0026#34;DELETE\u0026#34; }   And the next code snippet explains the CloudFromation template (which is the resource-group-cf-stack-template.json file in the code snippet above) ‚û°Ô∏è Click here to see the code snippet ‚¨ÖÔ∏è { \u0026#34;Parameters\u0026#34; : { \u0026#34;GroupName\u0026#34; : { \u0026#34;Type\u0026#34; : \u0026#34;String\u0026#34;, \u0026#34;Description\u0026#34; : \u0026#34;The name of Host Group\u0026#34; } }, \u0026#34;Resources\u0026#34; : { \u0026#34;DedicatedHostGroup\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;AWS::ResourceGroups::Group\u0026#34;, \u0026#34;Properties\u0026#34;: { \u0026#34;Name\u0026#34;: { \u0026#34;Ref\u0026#34;: \u0026#34;GroupName\u0026#34; }, \u0026#34;Configuration\u0026#34;: [ { \u0026#34;Type\u0026#34;: \u0026#34;AWS::ResourceGroups::Generic\u0026#34;, \u0026#34;Parameters\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;allowed-resource-types\u0026#34;, \u0026#34;Values\u0026#34;: [\u0026#34;AWS::EC2::Host\u0026#34;] }, { \u0026#34;Name\u0026#34;: \u0026#34;deletion-protection\u0026#34;, \u0026#34;Values\u0026#34;: [\u0026#34;UNLESS_EMPTY\u0026#34;] } ] }, { \u0026#34;Type\u0026#34;: \u0026#34;AWS::EC2::HostManagement\u0026#34;, \u0026#34;Parameters\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;allowed-host-families\u0026#34;, \u0026#34;Values\u0026#34;: [\u0026#34;mac1\u0026#34;] }, { \u0026#34;Name\u0026#34;: \u0026#34;auto-allocate-host\u0026#34;, \u0026#34;Values\u0026#34;: [\u0026#34;true\u0026#34;] }, { \u0026#34;Name\u0026#34;: \u0026#34;auto-release-host\u0026#34;, \u0026#34;Values\u0026#34;: [\u0026#34;true\u0026#34;] }, { \u0026#34;Name\u0026#34;: \u0026#34;any-host-based-license-configuration\u0026#34;, \u0026#34;Values\u0026#34;: [\u0026#34;true\u0026#34;] } ] } ] } } }, \u0026#34;Outputs\u0026#34; : { \u0026#34;ResourceGroupARN\u0026#34; : { \u0026#34;Description\u0026#34;: \u0026#34;ResourceGroupARN\u0026#34;, \u0026#34;Value\u0026#34; : { \u0026#34;Fn::GetAtt\u0026#34; : [\u0026#34;DedicatedHostGroup\u0026#34;, \u0026#34;Arn\u0026#34;] } } } }   The aws_cloudformation_stack resource will export the DedicatedHostGroup attribute (see the code of CloudFromation template), which you will use later in the Launch Template resource.\nPro tips If you manage an AWS Organization, I have good news: Host groups and Licenses are supported by Resource Access Manager service. Hence, you can host all mac instances in one account and share them with other accounts ‚Äî it might be helpful for costs allocation, for example. Also, check out my blog about AWS RAM if you are very new to this service.\nTo solve the ‚Äúwhich AZ supports mac metal‚Äù puzzle, you can leverage the aws_ec2_instance_type_offerings and aws_subnet_ids data sources.\nCosts considerations License Manager is a free of charge service, as well as Auto Scaling, and Launch Template.\nSo it‚Äôs all about the price for mac1.metal Dedicated Host which is $1.083 per hour as of October 2021. However, Saving Plans can be applied.\nPlease note that the minimum allocation time for that type of host is 24 hours. Maybe someday AWS will change that to 1-hour minimum someday (fingers crossed).\nOh. So. ASG. The Auto Scaling for mac1.metal opens new possibilities for CI/CD: you can integrate that to your favorite tool (GitLab, Jenkins, whatsoever) using AWS Lambda and provision new instances when your development/testing environments need that. Or you can use other cool ASG stuff, such as Lifecycle hooks, to create even more custom scenarios.\nConsidering the ‚Äúhidden‚Äù (undocumented) nature of the described setup, I suggest treating it as rather testing than production-ready for now. However, my tests show that everything works pretty well: hosts are allocated, instances are spawned, and the monthly bill grows.\nI suppose AWS will officially announce all this in the nearest future. Along with that, I am looking forward to the announcement of Monterey-based AMIs and maybe even M1 chip-based instances (will it be mac2.metal?).\nAnd I want to say thanks (thanks, pal!) to OliverKoo, who started digging into that back in April'21.\n","permalink":"https://serhii.vasylenko.info/2021/10/24/auto-scaling-group-for-your-macos-ec2-instances-fleet/","summary":"It‚Äôs been almost a year since I started using macOS EC2 instances on AWS: there were ups and downs in service offerings and a lot of discoveries with macOS AMI build automation.\nAnd I like this small but so helpful update of EC2 service very much: with mac1.metal instances, seamless integration of Apple-oriented CI/CD with other AWS infrastructure could finally happen.\nWhile management of a single mac1.metal node (or a tiny number of ones) is not a big deal (especially when Dedicated Host support was added to Terraform provider), governing the fleet of instances is still complicated.","title":"Auto Scaling Group for your macOS EC2 Instances fleet"},{"content":"With a multi-account approach of building the infrastructure, there is always a challenge of provision and governance of the resources to subordinate accounts within the Organization. Provision resources, keep them up to date, and decommission them properly ‚Äî that\u0026rsquo;s only a part of them.\nAWS has numerous solutions that help make this process reliable and secure, and the Resource Access Manager (RAM) is one of them. In a nutshell, the RAM service allows you to share the AWS resources you create in one AWS account with other AWS accounts. They can be your organizations' accounts, organizational units (OU), or even third-party accounts.\nSo let\u0026rsquo;s see what the RAM is and review some of its usage examples.\nWhy using RAM There are several benefits of using the RAM service:\n  Reduced operational overhead: eliminate the need of provisioning the same kind of resource multiple times ‚Äî RAM does that for you\n  Simplified security management: AWS RAM-managed permissions (at least one per resource type) define the actions that principals with access to the resources (i.e., resource users) can perform on those resources.\n  Consistent experience: you share the resource in its state and with its security configuration with an arbitrary number of accounts.\nThat plays incredibly well in the case of organization-wide sharing: new accounts get the resources automatically. And the shared resource itself looks like a native resource in the account that accepts your sharing.\n  Audit and visibility: RAM integrates with the CloudWatch and CloudTrail.\n  How to share a resource When you share a resource, the AWS account that owns that resource retains full ownership of the resource.\nSharing of the resource doesn\u0026rsquo;t change any permissions or quotas that apply to that resource. Also, you can share the resource only if you own it.\nAvailability of the shared resources scopes to the Region: the users of your shared resources can access these resources only in the same Region where resources belong.\nCreation of resource share consists of three steps:   Specify the share name and the resource(s) you want to share. It can be either one resource type or several. You can also skip the resources selection and do that later.\nIt\u0026rsquo;s possible to modify the resource share later (e.g., you want to add some resources to the share).\n  Associate permissions with resource types you share. Some resources can have only one managed permission (will be attached automatically), and some can have multiple.\nYou can check the Permissions Library in the AWS RAM Console to see what managed permissions are available.\n  Select who can use the resources you share: either external or Organization account or IAM role/user. If you share the resource with third parties, they will have to accept the sharing explicitly.\nOrganization-wide resource share is accepted implicitly if resource sharing is enabled for the Organization.\n  Finally, review the summary page of the resource share and create it.\nOnly specific actions are available to the users of shared resources. These actions mostly have the \u0026ldquo;read-only\u0026rdquo; nature and vary by resource type.\nAlso, the RAM service is supported by Terraform, so the resource sharing configuration may look like that, for example:\nresource \u0026#34;aws_ram_resource_share\u0026#34; \u0026#34;example\u0026#34; { name = \u0026#34;example\u0026#34; allow_external_principals = false tags = { Environment = \u0026#34;Production\u0026#34; } } resource \u0026#34;aws_ram_resource_association\u0026#34; \u0026#34;example\u0026#34; { resource_arn = aws_subnet.example.arn resource_share_arn = aws_ram_resource_share.example.arn } Example use cases One of the trivial but valuable examples of RAM service usage is sharing a Manged Prefix List. Suppose you have some service user across your Organization, a self-hosted VPN server, for example. And you have a static set of IPs for that VPN: you trust these IPs and would like them to be allow-listed in your other services. How to report these IPs to all organization accounts/users? And if the IP set changes, how to announce that change, and what should be done to reflect that change in services that depend on it, for example, Security Groups?\nThe answer is a shared Managed Prefix List. You create the list once in the account and share it across your Organization. Other accounts automatically get access to that list and can reference the list in their Security Groups. And when the list entry is changed, they do not need to perform any actions: their Security Groups will get the updated IPs implicitly.\nAnother everyday use case of RAM is the VPC sharing that can form the foundation of the multi-account AWS architectures.\n Of course, the RAM service is not the only way to organize and centralize resource management in AWS. There are Service Catalog, Control Tower, Systems Manager, Config, and others. However, the RAM is relatively simple to adopt but is capable of providing worthy outcomes.\n","permalink":"https://serhii.vasylenko.info/2021/09/25/aws-resource-access-manager-multi-account-resource-governance/","summary":"With a multi-account approach of building the infrastructure, there is always a challenge of provision and governance of the resources to subordinate accounts within the Organization. Provision resources, keep them up to date, and decommission them properly ‚Äî that\u0026rsquo;s only a part of them.\nAWS has numerous solutions that help make this process reliable and secure, and the Resource Access Manager (RAM) is one of them. In a nutshell, the RAM service allows you to share the AWS resources you create in one AWS account with other AWS accounts.","title":"AWS Resource Access Manager ‚Äî Multi Account Resource Governance"},{"content":"In days of containers and serverless applications, Ansible looks not such a trendy thing.\nBut still, there are cases when it helps, and there are cases when it combines very well with brand new product offerings, such as EC2 Mac instances.\nThe more I use mac1.metal in AWS, the more I see that Ansible becomes a bedrock of software customization in my case.\nAnd when you have a large instances fleet, the AWS Systems Manager becomes your best friend (the sooner you get along together, the better).\nSo is it possible to use Ansible playbooks for mac1.metal on a big scale, with the help of AWS Systems Manager?\n(Not) Available out of the box AWS Systems Manager (SSM hereafter) has a pre-defined, shared Document that allows running Ansible playbooks.\nIt‚Äôs called ‚ÄúAWS-RunAnsiblePlaybook,‚Äù and you can find it in AWS SSM ‚Üí Documents ‚Üí Owned by Amazon.\nHowever, this Document is not quite ‚Äúfriendly‚Äù to macOS. When the SSM agent calls Ansible on the Mac EC2 instance, it does not recognize the Ansible installed with Homebrew (de-facto most used macOS package manager).\nSo if you try to run a command on the mac1.metal instance using this Document, you will get the following error:\nAnsible is not installed. Please install Ansible and rerun the command. The root cause is trivial: the path to Ansible binary is not present on the list of paths available to the SSM agent by default.\nThere are several ways to solve that, but I believe that the most convenient one would be to create your custom Document ‚Äî a slightly adjusted version of the default one provided by AWS.\nCreating own SSM Document for Ansible installed with Homebrew All you need to do is clone the Document provided by AWS and change its code a little ‚Äî replace the callouts of ansible with the full path to the binary.\nNavigate to AWS SSM ‚Üí Documents ‚Üí Owned by Amazon and type AWS-RunAnsiblePlaybook in the search field.\nSelect the Document by pressing the circle on its top-right corner and then click Actions ‚Üí Clone document.\nGive the new SSM Document a name, e.g., macos-arbitrary-ansible-playbook, and change the ansible callouts (at the end of the code) with the full path to the ansible symlink made by Homebrew which is /usr/local/bin/ansible\nHere is the complete source code of the Document with adjusted Ansible path:\n‚û°Ô∏è Click here to see the code snippet ‚¨ÖÔ∏è { \u0026#34;schemaVersion\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Use this document to run arbitrary Ansible playbooks on macOS EC2 instances. Specify either YAML text or URL. If you specify both, the URL parameter will be used. Use the extravar parameter to send runtime variables to the Ansible execution. Use the check parameter to perform a dry run of the Ansible execution. The output of the dry run shows the changes that will be made when the playbook is executed.\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;playbook\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;(Optional) If you don\u0026#39;t specify a URL, then you must specify playbook YAML in this field.\u0026#34;, \u0026#34;default\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;displayType\u0026#34;: \u0026#34;textarea\u0026#34; }, \u0026#34;playbookurl\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;(Optional) If you don\u0026#39;t specify playbook YAML, then you must specify a URL where the playbook is stored. You can specify the URL in the following formats: http://example.com/playbook.yml or s3://examplebucket/plabook.url. For security reasons, you can\u0026#39;t specify a URL with quotes.\u0026#34;, \u0026#34;default\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;allowedPattern\u0026#34;: \u0026#34;^\\\\s*$|^(http|https|s3)://[^\u0026#39;]*$\u0026#34; }, \u0026#34;extravars\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;(Optional) Additional variables to pass to Ansible at runtime. Enter a space separated list of key/value pairs. For example: color=red or fruits=[apples,pears]\u0026#34;, \u0026#34;default\u0026#34;: \u0026#34;foo=bar\u0026#34;, \u0026#34;displayType\u0026#34;: \u0026#34;textarea\u0026#34;, \u0026#34;allowedPattern\u0026#34;: \u0026#34;^((^|\\\\s)\\\\w+=(\\\\S+|\u0026#39;.*\u0026#39;))*$\u0026#34; }, \u0026#34;check\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34; (Optional) Use the check parameter to perform a dry run of the Ansible execution.\u0026#34;, \u0026#34;allowedValues\u0026#34;: [ \u0026#34;True\u0026#34;, \u0026#34;False\u0026#34; ], \u0026#34;default\u0026#34;: \u0026#34;False\u0026#34; }, \u0026#34;timeoutSeconds\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;(Optional) The time in seconds for a command to be completed before it is considered to have failed.\u0026#34;, \u0026#34;default\u0026#34;: \u0026#34;3600\u0026#34; } }, \u0026#34;mainSteps\u0026#34;: [ { \u0026#34;action\u0026#34;: \u0026#34;aws:runShellScript\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;runShellScript\u0026#34;, \u0026#34;inputs\u0026#34;: { \u0026#34;timeoutSeconds\u0026#34;: \u0026#34;{{ timeoutSeconds }}\u0026#34;, \u0026#34;runCommand\u0026#34;: [ \u0026#34;#!/bin/bash\u0026#34;, \u0026#34;/usr/local/bin/ansible --version\u0026#34;, \u0026#34;if [ $? -ne 0 ]; then\u0026#34;, \u0026#34; echo \\\u0026#34;Ansible is not installed. Please install Ansible and rerun the command\\\u0026#34; \u0026gt;\u0026amp;2\u0026#34;, \u0026#34; exit 1\u0026#34;, \u0026#34;fi\u0026#34;, \u0026#34;execdir=$(dirname $0)\u0026#34;, \u0026#34;cd $execdir\u0026#34;, \u0026#34;if [ -z \u0026#39;{{playbook}}\u0026#39; ] ; then\u0026#34;, \u0026#34; if [[ \\\u0026#34;{{playbookurl}}\\\u0026#34; == http* ]]; then\u0026#34;, \u0026#34; wget \u0026#39;{{playbookurl}}\u0026#39; -O playbook.yml\u0026#34;, \u0026#34; if [ $? -ne 0 ]; then\u0026#34;, \u0026#34; echo \\\u0026#34;There was a problem downloading the playbook. Make sure the URL is correct and that the playbook exists.\\\u0026#34; \u0026gt;\u0026amp;2\u0026#34;, \u0026#34; exit 1\u0026#34;, \u0026#34; fi\u0026#34;, \u0026#34; elif [[ \\\u0026#34;{{playbookurl}}\\\u0026#34; == s3* ]] ; then\u0026#34;, \u0026#34; aws --version\u0026#34;, \u0026#34; if [ $? -ne 0 ]; then\u0026#34;, \u0026#34; echo \\\u0026#34;The AWS CLI is not installed. The CLI is required to process Amazon S3 URLs. Install the AWS CLI and run the command again.\\\u0026#34; \u0026gt;\u0026amp;2\u0026#34;, \u0026#34; exit 1\u0026#34;, \u0026#34; fi\u0026#34;, \u0026#34; aws s3 cp \u0026#39;{{playbookurl}}\u0026#39; playbook.yml\u0026#34;, \u0026#34; if [ $? -ne 0 ]; then\u0026#34;, \u0026#34; echo \\\u0026#34;Error while downloading the document from S3\\\u0026#34; \u0026gt;\u0026amp;2\u0026#34;, \u0026#34; exit 1\u0026#34;, \u0026#34; fi\u0026#34;, \u0026#34; else\u0026#34;, \u0026#34; echo \\\u0026#34;The playbook URL is not valid. Verify the URL and try again.\\\u0026#34;\u0026#34;, \u0026#34; fi\u0026#34;, \u0026#34;else\u0026#34;, \u0026#34; echo \u0026#39;{{playbook}}\u0026#39; \u0026gt; playbook.yml\u0026#34;, \u0026#34;fi\u0026#34;, \u0026#34;if [[ \\\u0026#34;{{check}}\\\u0026#34; == True ]] ; then\u0026#34;, \u0026#34; /usr/local/bin/ansible-playbook -i \\\u0026#34;localhost,\\\u0026#34; --check -c local -e \\\u0026#34;{{extravars}}\\\u0026#34; playbook.yml\u0026#34;, \u0026#34;else\u0026#34;, \u0026#34; /usr/local/bin/ansible-playbook -i \\\u0026#34;localhost,\\\u0026#34; -c local -e \\\u0026#34;{{extravars}}\\\u0026#34; playbook.yml\u0026#34;, \u0026#34;fi\u0026#34; ] } } ] }   Applying Ansible playbook to the fleet of mac1.metal Let‚Äôs give our new SSM Document a try! (I suppose you have at least one mac1 instance running, right?)\nIn AWS SSM, go to the Run Command feature, then click on the Run Command button.\nOn the new panel, type the name of your Document (macos-arbitrary-ansible-playbook in this example) in the search field and press enter.\nSelect the Document, and you‚Äôll see its parameters and settings.\nThe rest is self-explanatory. Enter either a playbook code or a link to the source file, add extra variables if needed, and select the target host or a filtered bunch (I like that feature with tags filtering!). Finally, click on the ‚ÄúRun‚Äù orange button to apply your playbook.\nThat‚Äôs it! Now you can make all your ansible-playbook dreams come true! üòÅ\n","permalink":"https://serhii.vasylenko.info/2021/05/27/run-ansible-playbook-on-mac1.metal-instances-fleet-with-aws-systems-manager/","summary":"In days of containers and serverless applications, Ansible looks not such a trendy thing.\nBut still, there are cases when it helps, and there are cases when it combines very well with brand new product offerings, such as EC2 Mac instances.\nThe more I use mac1.metal in AWS, the more I see that Ansible becomes a bedrock of software customization in my case.\nAnd when you have a large instances fleet, the AWS Systems Manager becomes your best friend (the sooner you get along together, the better).","title":"Run Ansible playbook on mac1.metal instances fleet with AWS Systems Manager"},{"content":"In November 2021, AWS has added this functionality as a native CloudFront feature.\nI suggest switching to the native implementation. I have described how to configure Security Response Headers for CloudFront in the following article:\nApply Cloudfront Security Headers With Terraform\n A couple of weeks ago, AWS released CloudFront Functions ‚Äî a ‚Äútrue edge‚Äù compute capability for the CloudFront.\nIt is ‚Äútrue edge‚Äù because Functions work on 200+ edge locations (link to doc) while its predecessor, the Lambda@Edge, runs on a small number of regional edge caches.\nOne of the use cases for Lambda@Edge was adding security HTTP headers (it‚Äôs even listed on the product page), and now there is one more way to make it using CloudFront Functions.\nWhat are security headers, and why it matters Security Headers are one of the web security pillars.\nThey specify security-related information of communication between a web application (i.e., website) and a client (i.e., browser) and protect the web app from different types of attacks. Also, HIPAA and PCI, and other security standard certifications generally include these headers in their rankings.\nWe will use CloudFront Functions to set the following headers:\n Content Security Policy Strict Transport Security X-Content-Type-Options X-XSS-Protection X-Frame-Options Referrer Policy  You can find a short and detailed explanation for each security header on Web Security cheatsheet made by Mozilla\nCloudFront Functions overview In a nutshell, CloudFront Functions allow performing simple actions against HTTP(s) request (from the client) and response (from the CloudFront cache at the edge). Functions take less than one millisecond to execute, support JavaScript (ECMAScript 5.1 compliant), and cost $0.10 per 1 million invocations.\nEvery CloudFront distribution has one (default) or more Cache behaviors, and Functions can be associated with these behaviors to execute upon a specific event.\nThat is how the request flow looks like in general, and here is where CloudFront Functions execution happens:\nCloudFront Functions support Viewer Request (after CloudFront receives a request from a client) and Viewer Response (before CloudFront forwards the response to the client) events.\nYou can read more about the events types and their properties here ‚Äî CloudFront Events That Can Trigger a Lambda Function - Amazon CloudFront.\nAlso, the CloudFront Functions allow you to manage and operate the code and lifecycle of the functions directly from the CloudFront web interface.\nSolution overview CloudFront distribution should exist before Function creation so you could associate the Function with the distribution.\nCreation and configuration of the CloudFront Function consist of the following steps:\nCreate Function In the AWS Console, open CloudFront service and lick on the Functions on the left navigation bar, then click Create function button. Enter the name of your Function (e.g., ‚Äúsecurity-headers‚Äù) and click Continue.\nBuild Function On the function settings page, you will see four tabs with the four lifecycle steps: Build, Test, Publish, Associate.\nPaste the function code into the editor and click ‚ÄúSave.‚Äù\nHere is the source code of the function:\nfunction handler(event) { var response = event.response; var headers = response.headers; headers[\u0026#39;strict-transport-security\u0026#39;] = { value: \u0026#39;max-age=63072000; includeSubdomains; preload\u0026#39;}; headers[\u0026#39;content-security-policy\u0026#39;] = { value: \u0026#34;default-src \u0026#39;none\u0026#39;; img-src \u0026#39;self\u0026#39;; script-src \u0026#39;self\u0026#39;; style-src \u0026#39;self\u0026#39;; object-src \u0026#39;none\u0026#39;; frame-ancestors \u0026#39;none\u0026#39;\u0026#34;}; headers[\u0026#39;x-content-type-options\u0026#39;] = { value: \u0026#39;nosniff\u0026#39;}; headers[\u0026#39;x-xss-protection\u0026#39;] = {value: \u0026#39;1; mode=block\u0026#39;}; headers[\u0026#39;referrer-policy\u0026#39;] = {value: \u0026#39;same-origin\u0026#39;}; headers[\u0026#39;x-frame-options\u0026#39;] = {value: \u0026#39;DENY\u0026#39;}; return response; } Test Function Open the ‚ÄúTest‚Äù tab ‚Äî let‚Äôs try our function first before it becomes live!\nSelect Viewer Response event type and Development Stage, then select ‚ÄúViewer response with headers‚Äù as a Sample test event (you will get a simple set of headers automatically).\nNow click the blue ‚ÄúTest‚Äù button and observe the output results:\n Compute utilization represents the relative amount of time (on a scale between 0 and 100) your function took to run Check the Response headers tab and take a look at how the function added custom headers.  Publish Function Let‚Äôs publish our function. To do that, open the Publish tab and click on the blue button ‚ÄúPublish and update.‚Äù Associate your Function with CloudFront distribution Now, you can associate the function with the CloudFront distribution.\nTo do so, open the Associate tab, select the distribution and event type (Viewer Response), and select the Cache behavior of your distribution which you want to use for the association.\nOnce you associate the function with the CloudFront distribution, you can test it in live mode.\nI will use curl here to demonstrate it:\n\u0026gt; curl -i https://d30i87a4ss9ifz.cloudfront.net HTTP/2 200 content-type: text/html content-length: 140 date: Sat, 22 May 2021 00:22:18 GMT last-modified: Tue, 27 Apr 2021 23:07:14 GMT etag: \u0026#34;a855a3189f8223db53df8a0ca362dd62\u0026#34; accept-ranges: bytes server: AmazonS3 via: 1.1 50f21cb925e6471490e080147e252d7d.cloudfront.net (CloudFront) content-security-policy: default-src \u0026#39;none\u0026#39;; img-src \u0026#39;self\u0026#39;; script-src \u0026#39;self\u0026#39;; style-src \u0026#39;self\u0026#39;; object-src \u0026#39;none\u0026#39;; frame-ancestors \u0026#39;none\u0026#39; strict-transport-security: max-age=63072000; includeSubdomains; preload x-xss-protection: 1; mode=block x-frame-options: DENY referrer-policy: same-origin x-content-type-options: nosniff x-cache: Miss from cloudfront x-amz-cf-pop: WAW50-C1 x-amz-cf-id: ud3qH8rLs7QmbhUZ-DeupGwFhWLpKDSD59vr7uWC65Hui5m2U8o2mw== You can also test your results here ‚Äî Mozilla Observatory\nRead more That was a simplified overview of the CloudFront Functions capabilities.\nBut if you want to get deeper, here is a couple of useful links to start:\n Another overview from AWS ‚Äî CloudFront Functions Launch Blog More about creating, testing, updating and publishing of CloudFront Functions ‚Äî Managing functions in CloudFront Functions - Amazon CloudFront  So what to choose? CloudFront Functions are simpler than Lambda@Edge and run faster with minimal latency and minimal time penalty for your web clients.\nLambda@Edge takes more time to invoke, but it can run upon Origin Response event so that CloudFront can cache the processed response (including headers) and return it faster afterward.\nBut again, the CloudFront Functions invocations are much cheaper (6x times) than Lambda@Edge, and you do not pay for the function execution duration.\nThe final decision would also depend on the dynamic/static nature of the content you have at your origin.\nTo make a wise and deliberate decision, try to analyze your use case using these two documentation articles:\n Choosing between CloudFront Functions and Lambda@Edge How to Decide Which CloudFront Event to Use to Trigger a Lambda Function  ","permalink":"https://serhii.vasylenko.info/2021/05/21/configure-http-security-headers-with-cloudfront-functions.html","summary":"In November 2021, AWS has added this functionality as a native CloudFront feature.\nI suggest switching to the native implementation. I have described how to configure Security Response Headers for CloudFront in the following article:\nApply Cloudfront Security Headers With Terraform\n A couple of weeks ago, AWS released CloudFront Functions ‚Äî a ‚Äútrue edge‚Äù compute capability for the CloudFront.\nIt is ‚Äútrue edge‚Äù because Functions work on 200+ edge locations (link to doc) while its predecessor, the Lambda@Edge, runs on a small number of regional edge caches.","title":"Configure HTTP Security headers with CloudFront Functions"},{"content":"I just wanted to compress one image, but went to far\u0026hellip;\nor \u0026ldquo;How to add TinyPNG image compression to your macOS Finder contextual menu.\u0026rdquo;\nWhat is it and how it works You select needed files or folders, then right-click on them, click on the Services menu item and choose TinyPNG.\nAfter a moment, the new optimized versions of images will appear near to original files.\nIf you selected a folder along with the files, the script would process all png and jpeg files in it.\n Prerequisites You need to register at TinyPNG and get your API key here ‚Äî Developer API.\nThey sometimes block some countries (for example, Ukraine) from registration; in that case, try to use a web-proxy or VPN.\nHow to create Quick Action Workflow Open Automator application. If you never used this app before, please read about it on the official user guide website.\nOn the New Action screen, chose Quick Action\nAfter you click the \u0026ldquo;Choose\u0026rdquo; button, you\u0026rsquo;ll see the workflow configuration window.\nWorkflow configuration Find the Run Shell Script action on the Utilities list in Library on the left, and drag it onto the right side of the panel.\nSet the following workflow configuration options as described below:\nWorkflow receives current files and folders in Finder\nShell /bin/zsh\nPass input as arguments\nClick the Option button at the bottom of the Action window and Uncheck Show this action when the workflow runs.\nPut the following script into the Run Shell Script window, replacing the YOUR_API_KEY_HERE string with your API key obtained from TinyPNG.\n Utilities used in the script ‚Äî explained curl ‚Äî used to make web requests (like your browser does)\ngrep ‚Äî used to parse the response for the needed header (i.e., field) with the file download link\ncut ‚Äî used to extract the URL from the parsed result\nsed ‚Äî used to remove the trailing \u0026ldquo;carriage return\u0026rdquo; symbol at the end of extracted string\nThe response body also contains a JSON object that includes the download URL; you can parse it with jq, for example. But I intentionally refused to use the jq tool because it is not pre-installed in MacOS.\nConclusion It is simple, and it does its job fine. And you don\u0026rsquo;t need to install anything to make it work.\nTo make this a bit fancier, you might also like to add a \u0026ldquo;Display Notification\u0026rdquo; (from the Utilities library on the left) after the \u0026ldquo;Run Shell Script\u0026rdquo;. The action will display a notification once image processing is completed.\nThank you for reading!\n","permalink":"https://serhii.vasylenko.info/2021/02/14/using-tinypng-image-compression-from-macos-finder-contextual-menu/","summary":"I just wanted to compress one image, but went to far\u0026hellip;\nor \u0026ldquo;How to add TinyPNG image compression to your macOS Finder contextual menu.\u0026rdquo;\nWhat is it and how it works You select needed files or folders, then right-click on them, click on the Services menu item and choose TinyPNG.\nAfter a moment, the new optimized versions of images will appear near to original files.\nIf you selected a folder along with the files, the script would process all png and jpeg files in it.","title":"Using TinyPNG Image Compression From MacOS Finder Contextual Menu"},{"content":"I guess macOS was designed for a user, not for the ops or engineers, so this is why its customization and usage for CI/CD are not trivial (compared to something Linux-based). A smart guess, huh?\nConfiguration Management Native Apple\u0026rsquo;s Mobile device management (a.k.a MDM) and Jamf is probably the most potent combination for macOS configuration. But as much as it\u0026rsquo;s mighty, it is a cumbersome combination, and Jamf is not free.\nThen we have Ansible, Chef, Puppet, SaltStack ‚Äî they all are good with Linux, but what about macOS?\nI tried to search for use cases of mentioned CM tools for macOS. However, I concluded that they wrap the execution of native macOS command-line utilities most of the time.\nAnd if you search for the \u0026lsquo;macos\u0026rsquo; word in Chef Supermarket or Puppet Forge, you won\u0026rsquo;t be impressed by the number of actively maintained packages. Although, here is a motivating article about using Chef automating-macos-provisioning-with-chef if you prefer it. I could not find something similar and fresh for Puppet, so I am sorry, Puppet fans.\nThat is why I decided to follow the KISS principle and chose Ansible.\nIt\u0026rsquo;s easy to write and read the configuration, it allows to group tasks and to add execution logic , and it feels more DevOps executing shell commands inside Ansible tasks instead of shell scripts; I know you know that üòÇ\nBy the way, Ansible Galaxy does not have many management packages for macOS, either. But thankfully, it has the basics:\n homebrew with homebrew_cask and homebrew_tap ‚Äî to install software launchd ‚Äî to manage services osx_defaults ‚Äî to manage some user settings (not all!)  I used Ansible to build the macOS AMI for CI/CD, so here are some tips for such a case.\nSome values are hardcoded intentionally in the code examples for the sake of simplicity and easy reading. You would probably want to parametrize them.\nXcode installation example The following tasks will help you to automate the basics.\n- name:Install Xcodeshell:\u0026#34;xip --expand Xcode.xip\u0026#34;args:chdir:/Applications- name:Accept License Agreementshell:\u0026#34;/Applications/Xcode.app/Contents/Developer/usr/bin/xcodebuild -license accept\u0026#34;- name:Accept License Agreementshell:\u0026#34;/Applications/Xcode.app/Contents/Developer/usr/bin/xcodebuild -runFirstLaunch\u0026#34;- name:Switch into newly installed Xcode contextshell:\u0026#34;xcode-select --switch /Applications/Xcode.app/Contents/Developer\u0026#34;Example of software installation with Brew {% raw %}\n- name:Install common build softwarecommunity.general.homebrew:name:\u0026#34;{{ item }}\u0026#34;state:latestloop:- swiftlint- swiftformat- wget{% endraw %}\nScreenSharing (remote desktop) configuration example - name:Turn On Remote Managementshell:\u0026#34;./kickstart -activate -configure -allowAccessFor -specifiedUsers\u0026#34;args:chdir:/System/Library/CoreServices/RemoteManagement/ARDAgent.app/Contents/Resources/- name:Enable Remote Management for CI usershell:\u0026#34;./kickstart -configure -users ec2-user -access -on -privs -all\u0026#34;args:chdir:/System/Library/CoreServices/RemoteManagement/ARDAgent.app/Contents/Resources/Shell rulez, yes.\nBuilding the AMI  Packer by HashiCorp, of course.\nI would love to compare Packer with EC2 Image Builder, but it does not support macOS yet (as of Feb'21).\nPacker configuration is straightforward, so I want to highlight only the things specific to the \u0026ldquo;mac1.metal\u0026rdquo; use case.\nTimeouts As I mentioned in the previous article, the creation and deletion time of the \u0026ldquo;mac1.metal\u0026rdquo; Instance is significantly bigger than Linux. That is why you should raise the polling parameters for the builder.\nExample:\n\u0026#34;aws_polling\u0026#34;: { \u0026#34;delay_seconds\u0026#34;: 30, \u0026#34;max_attempts\u0026#34;: 60 } And it would be best if you also increased the SSH timeout:\n\u0026#34;ssh_timeout\u0026#34;: \u0026#34;1h\u0026#34; Fortunately, Packer\u0026rsquo;s AMI builder does not require an explicit declaration of the Dedicated Host ID. So you can just reference the same subnet where you allocated the Host, assuming you did it with the enabled \u0026ldquo;Auto placement\u0026rdquo; parameter during the host creation.\nExample:\n\u0026#34;tenancy\u0026#34;: \u0026#34;host\u0026#34;, \u0026#34;subnet_id\u0026#34;: \u0026#34;your-subnet-id\u0026#34; Provisioning Packer has Ansible Provisioner that I used for the AMI. Its documentation is also very clean and straightforward.\nBut it is still worth mentioning that if you want to parametrize the Ansible playbook, then the following configuration example will be handy:\n\u0026#34;extra_arguments\u0026#34;: [ \u0026#34;--extra-vars\u0026#34;, \u0026#34;your-variable-foo=your-value-bar]\u0026#34; ], \u0026#34;ansible_env_vars\u0026#34;: [ \u0026#34;ANSIBLE_PYTHON_INTERPRETER=auto_legacy_silent\u0026#34;, \u0026#34;ANSIBLE_OTHER_ENV_VARIABLE=other_value\u0026#34; ] Configuration at launch If you\u0026rsquo;re familiar with AWS EC2, you probably know what the Instance user data is.\nA group of AWS developers made something similar for the macOS: EC2 macOS Init.\nIt does not support cloud-init as on Linux-based Instances, but it can run shell scripts, which is quite enough.\nEC2 macOS Init utility is a Launch Daemon (macOS terminology) that runs on behalf of the root user at system boot. It executes the commands according to the so-called Priority Groups, or the sequence in other words.\nThe number of the group corresponds to the execution order. You can put several tasks into a single Priority Group, and the tool will execute them simultaneously.\nEC2 macOS Init uses a human-readable configuration file in toml format.\nExample:\n[[Module]] Name = \u0026quot;Create-some-folder\u0026quot; PriorityGroup = 3 FatalOnError = false RunPerInstance = true [Module.Command] Cmd = [\u0026quot;mkdir\u0026quot;, \u0026quot;/Users/ec2-user/my-directory\u0026quot;] RunAsUser = \u0026quot;ec2-user\u0026quot; EnvironmentVars = [\u0026quot;MY_VAR_FOO=myValueBar\u0026quot;] I should clarify some things here.\nModules ‚Äî a set of pre-defined modules for different purposes. It is something similar to the Ansible modules.\nYou can find the list of available modules here ec2-macos-init/lib/ec2macosinit\nThe RunPerInstance directive controls whether a module should run. There are three of such directives, and here is what they mean:\n RunPerBoot ‚Äî module will run at every system boot RunPerInstance ‚Äî module will run once for the Instance. Each Instance has a unique ID; the init tool fetches it from the AWS API before the execution and keeps its execution history per Instance ID. When you create a new Instance from the AMI, it will have a unique ID, and the module will run again. RunOnce ‚Äî module will run only once, despite the instance ID change  I mentioned the execution history above. When EC2 macOS Init runs on the Instance first time, it creates a unique directory with the name per Instance ID to store the execution history and user data copy.\nRunPerInstance and RunOnce directives depend on the execution history, and modules with those directives will run again on the next boot if the previous execution failed. It was not obvious to me why RunOnce keeps repeating itself every boot until I dug into the source code.\nFinally, there is a module for user data. It runs at the end by default (priority group #4) and pulls the user data script from AWS API before script execution.\nI suggest looking into the default init.toml configuration file to get yourself more familiar with the capabilities of the tool.\nThe init tool can also clear its history, which is useful for the new AMI creation.\nExample:\nec2-macos-init clean -all And you can run the init manually for debugging purposes.\nExample:\nec2-macos-init run You can also combine the EC2 macOS Init actions (made by modules) with your script in user data for more accurate nontrivial configurations.\nWrapping up As a whole, building and operating macOS-based AMI does not differ from AMI management for other platforms.\nThere are the same principle stages: prepare, clear, build, execute deployment script (if necessary). Though, the particular implementation of each step has its nuances and constraints.\nSo the whole process may look as follows:\n Provision and configure needed software with Ansible playbook Clean-up system logs and EC2 macOS Init history (again, with Ansible task) Create the AMI Add more customizations at launch with EC2 macOS Init modules and user data (that also executes your Ansible playbook or shell commands)  Getting into all this was both fun and interesting. Sometimes painful, though. üòÜ\nI sincerely hope this article was helpful to you. Thank you for reading!\n","permalink":"https://serhii.vasylenko.info/2021/02/01/customizing-mac1-metal-ec2-ami.html","summary":"I guess macOS was designed for a user, not for the ops or engineers, so this is why its customization and usage for CI/CD are not trivial (compared to something Linux-based). A smart guess, huh?\nConfiguration Management Native Apple\u0026rsquo;s Mobile device management (a.k.a MDM) and Jamf is probably the most potent combination for macOS configuration. But as much as it\u0026rsquo;s mighty, it is a cumbersome combination, and Jamf is not free.","title":"Customizing mac1.metal EC2 AMI ‚Äî new guts, more glory"},{"content":"Updated on the 23rd of October, 2021: Terraform AWS provider now supports Dedicated Hosts natively  In November 2021, AWS announced the support for Mac mini instances.\nI believe this is huge, even despite the number of constraints this solution has. This offering opens the door to seamless macOS CI/CD integration into existing AWS infrastructure.\nSo here is a quick-start example of creating the dedicated host and the instance altogether using Terraform.\nI intentionally used some hardcoded values for the sake of simplicity in the example.\nresource \u0026#34;aws_ec2_host\u0026#34; \u0026#34;example_host\u0026#34; { instance_type = \u0026#34;mac1.metal\u0026#34; availability_zone = \u0026#34;us-east-1a\u0026#34; } resource \u0026#34;aws_instance\u0026#34; \u0026#34;example_instance\u0026#34; { ami = data.aws_ami.mac1metal.id host_id = aws_ec2_host.example_host.id instance_type = \u0026#34;mac1.metal\u0026#34; subnet_id = data.aws_subnet.example_subnet.id } data \u0026#34;aws_subnet\u0026#34; \u0026#34;example_subnet\u0026#34; { availability_zone = \u0026#34;us-east-1a\u0026#34; filter { name = \u0026#34;tag:Tier\u0026#34;# you should omit this filter if you don\u0026#39;t distinguish your subnets on private and public  values = [\u0026#34;private\u0026#34;] } } data \u0026#34;aws_ami\u0026#34; \u0026#34;mac1metal\u0026#34; { owners = [\u0026#34;amazon\u0026#34;] most_recent = true filter { name = \u0026#34;name\u0026#34; values = [\u0026#34;amzn-ec2-macos-11*\u0026#34;]# get latest BigSur AMI  } } Simple as that, yes. Now, you can integrate it into your CI system and have the Mac instance with the underlying host in a bundle.\nüí° Pro tip: you can leverage the aws_ec2_instance_type_offerings Data Source and use its output with aws_subnet source to avoid availability zone hardcoding.\nTo make the code more uniform and reusable, you can wrap it into a Terraform module that accepts specific parameters (such as instance_type or availability_zone) as input variables.\n","permalink":"https://serhii.vasylenko.info/2021/01/20/terraforming-mac1-metal-at-AWS.html","summary":"Updated on the 23rd of October, 2021: Terraform AWS provider now supports Dedicated Hosts natively  In November 2021, AWS announced the support for Mac mini instances.\nI believe this is huge, even despite the number of constraints this solution has. This offering opens the door to seamless macOS CI/CD integration into existing AWS infrastructure.\nSo here is a quick-start example of creating the dedicated host and the instance altogether using Terraform.","title":"Terraforming mac1.metal at AWS"},{"content":"Amazon EC2 Mac Instances Something cool and powerful with inevitable trade-offs. As everything in this world.\nAWS announced EC2 macOS-based instances on the 30th of November 2020, and after more than a month of tests, I would like to share some findings and impressions about it.\nFirst of all, the things you can easily find, but it\u0026rsquo;s still worth to say:\n The new instance family is called mac1.metal. Guess we should expect mac2 or mac3; otherwise, why did they put a number in the name? They added AWS Nitro System to integrate them with many AWS services. The Instance must be placed onto a Dedicated Host. Only one Instance per Host is allowed because the Host is an actual Mac Mini in that case. You don\u0026rsquo;t pay anything for the Instance itself, but you pay for the Dedicated Host leasing ‚Äî $1.083, and the minimum lease time is 24 hours. So the launch of the \u0026ldquo;mac1.metal\u0026rdquo; Instance costs $26 at minimum. Prices provided for the cheapest region ‚Äî North Virginia. You can apply Saving Plans to save some money. Mojave (10.14) and Catalina (10.15) are supported at the moment, with \u0026ldquo;support for macOS Big Sur (11.0) coming soon\u0026rdquo;. I expect it to be in 2021, though.  What can it do Here is a list of some features that the \u0026ldquo;mac1.metal\u0026rdquo; instance has:\n It lives in your VPC because it is an EC2 Instance so that you can access many other services. It supports the new gp3 EBS type (and other types as well). It supports SSM Agent and Session Manager. It has several AWS tools pre-installed. It has pre-installed Enhanced Network Interface drivers. My test upload/download to S3 was about 300GB/s. It can report CPU metrics to CloudWatch (if you ever need it, though).  What can\u0026rsquo;t it do  It can\u0026rsquo;t be used in Auto Scaling because of a Dedicated Host. It can\u0026rsquo;t recognize the attached EBS if you connected it while the Instance was running ‚Äî you must reboot the Instance to make it visible. It does not support several services that rely on additional custom software, such as \u0026ldquo;EC2 Instance Connect\u0026rdquo; and \u0026ldquo;AWS Inspect.\u0026rdquo; But I think that AWS will add macOS distros for those soon.  Launching the Instance Jeff Bar published an excellent how-to about kickstart of the \u0026ldquo;mac1.metal\u0026rdquo;, so I will focus on things he did not mention.\nOnce you allocated the Dedicated Host and launched an Instance on it, the underlying system connects the EBS with a root file system to the Mac Mini.\nIt is an AMI with 32G EBS (as per Jan'21) with macOS pre-installed.\nThat means two things:\n The built-it physical SSD is still there and still yours to use; however, AWS does not manage or support the Apple hardware\u0026rsquo;s internal SSD. You must resize the disk manually (if you specified the EBS size to be more than 32G)[1].  The time from the Instance launch till you\u0026rsquo;re able to SSH into it varies between 15 and 20 minutes.\nYou have the option to access it over SSH with your private key. If you need to set up Screen Sharing, you have to allow it through the \u0026ldquo;kickstart\u0026rdquo; command-line utility and setting the user password [2].\nDestroying the Instance What an easy thing to do, right? Well, it depends.\nWhen you click on the \u0026ldquo;Terminate\u0026rdquo; item in the Instance actions menu, the complex Instance scrubbing process begins.\nAWS wants to make sure that anyone who uses the Host (Mac mini) after you will get your data stored neither on disks (including physical SSD mentioned earlier), nor inside memory or NVRAM, nor anywhere else. They do not share the info about this scrubbing process\u0026rsquo;s details, but it takes more than an hour to complete.\nWhen scrubbing is started, the Dedicated Host transitions to the Pending state. Dedicated Host transitions to Available state once scrubbing is finished. But you must wait for another 10-15 minutes to be able to release it finally.\nI don\u0026rsquo;t know why they set the Available state value earlier than the Host is available for operations, but this is how it works now (Jan'21).\nTherefore, you can launch the next Instance on the same Host not earlier than ~1,5 hours after you terminated the previous. That doesn\u0026rsquo;t seem very pleasant in the first couple of weeks, but you will get used to it. üòÑ\nAnd again: you can release the \u0026ldquo;mac1.metal\u0026rdquo; Dedicated Host not earlier than 24 hours after it was allocated. So plan your tests wisely.\nLegal things I could not find it on a documentation page, but A Cloud Guru folks say that you must use new Instances solely for developer services, and you must agree to all of the EULAs.\nSounds reasonable to me, but that could be written somewhere in the docs still, at least. Please let me know if you found it there.\nSome more cool stuff to check: EC2 macOS Init launch daemon, which is used to initialize Mac instances. EC2 macOS Homebrew Tap (Third-Party Repository) with several management tools which come pre-installed into macOS AMI from AWS.\n Indeed it is powerful, and it has its trade-offs, such as price and some technical constraints. But it is a real MacOS device natively integrated into the AWS environment. So I guess it worth to be tried!\nThanks for reading this! Stay tuned for more user experience feedback about baking custom AMI\u0026rsquo;s, automated software provisioning with Ansible, and other adventures with mac1.metal!\n [1] How to resize the EBS at mac1.metal in Terminal\nGet the identifier of EBS (look for the first one with GUID_partition_scheme):\ndiskutil list physical external\nOr here is a more advanced version to be used in a script:\nDISK_ID=$(diskutil list physical external | grep \u0026#39;GUID_partition_scheme\u0026#39;| tr -s \u0026#39; \u0026#39; | cut -d\u0026#39; \u0026#39; -f6) It would probably be disk0 if you did not attach additional EBS.\nThen run the repair job for the disk, using its identifier: diskutil repairDisk disk0\nAdvanced version:\nyes | diskutil repairDisk $DISK_ID Now get the APFS container identifier (look for Apple_APFS): diskutil list physical external\nAdvanced version:\nAPFS_ID=$(diskutil list physical external | grep \u0026#39;Apple_APFS\u0026#39; | tr -s \u0026#39; \u0026#39; | cut -d\u0026#39; \u0026#39; -f8) It would probably be disk0s2 if you did not attach additional EBS.\nFinally, resize the APFS container: diskutil apfs resizeContainer disk0s2\nAdvanced version\ndiskutil apfs resizeContainer $APFS_ID [2]How to setup Screen Sharing at mac1.metal in Terminal\nThe kickstart command-line tool resides in /System/Library/CoreServices/RemoteManagement/ARDAgent.app/Contents/Resources/ so you\u0026rsquo;ll better to cd into that directory for convenience:\n# Turn On Remote Management for a user to be specified later sudo ./kickstart -activate -configure -allowAccessFor -specifiedUsers # Enable Remote Management for ec2-user user sudo ./kickstart -configure -users ec2-user -access -on -privs -all # Set the user password  sudo passwd ec2-user ","permalink":"https://serhii.vasylenko.info/2021/01/19/mac1-metal-EC2-Instance-user-experience.html","summary":"Amazon EC2 Mac Instances Something cool and powerful with inevitable trade-offs. As everything in this world.\nAWS announced EC2 macOS-based instances on the 30th of November 2020, and after more than a month of tests, I would like to share some findings and impressions about it.\nFirst of all, the things you can easily find, but it\u0026rsquo;s still worth to say:\n The new instance family is called mac1.metal. Guess we should expect mac2 or mac3; otherwise, why did they put a number in the name?","title":"mac1.metal EC2 Instance ‚Äî user experience"},{"content":"A simple but cool announcement from AWS ‚Äî AWS CloudShell. A tool for ad-hoc AWS management via CLI directly in your browser.\nI like when AWS releases something simple to understand and yet powerful.\nSo it is not another DevOps Guru, believe me :)\n Yes, this is similar to the shells that GCE and Azure have. No, you can‚Äôt access your instances from it, so it‚Äôs not a jump server (bastion host). Yes, it has AWS CLI and other tools pre-installed. Even Python and Node.js. No, you can‚Äôt (well, you can, but should not) use it as an alternative to the day-to-day console on your laptop. Yes, you can manage all resources from that shell as much as your IAM permissions allow you (even with SSO, which is pretty cool). No, it does not support Docker. Yes, you have 1 GB of permanent storage and the ability to transfer files in and out.  More Yes and No‚Äôs here: https://docs.aws.amazon.com/cloudshell/latest/userguide/faq-list.html\nhttps://aws.amazon.com/cloudshell/faqs/\n","permalink":"https://serhii.vasylenko.info/2020/12/16/aws-cloudshell.html","summary":"A simple but cool announcement from AWS ‚Äî AWS CloudShell. A tool for ad-hoc AWS management via CLI directly in your browser.\nI like when AWS releases something simple to understand and yet powerful.\nSo it is not another DevOps Guru, believe me :)\n Yes, this is similar to the shells that GCE and Azure have. No, you can‚Äôt access your instances from it, so it‚Äôs not a jump server (bastion host).","title":"AWS CloudShell"},{"content":"The work with Terraform code may become tangled sometimes. Here are some guides on how to streamline it and make it transparent for you and your team.\nIt is extremely helpful in a team, and can benefit you even if you work individually. A good workflow enables you to streamline a process, organize it, and make it less error-prone.\nThis article summaries several approaches when working with Terraform, both individually and in a team. I tried to gather the most common ones, but you might also want to develop your own.\nThe common requirement for all of them is a version control system (such as Git). This is how you ensure nothing is lost and all your code changes are properly versioned tracked.\nTable of contents:\n Basic Concepts Core individual workflow Core team workflow Team workflow with automation Import workflow  Basic Concepts Let‚Äôs define the basic actions first.\nAll described workflows are built on top of three key steps: Write, Plan, and Apply. Nevertheless, their details and actions vary between workflows.\nIt\u0026rsquo;s a piece of cake, isn\u0026rsquo;t it? üòÜ\nWrite ‚Äì this is where you make changes to the code.\nPlan ‚Äì this is where you review changes and decide whether to accept them.\nApply ‚Äì this is where you accept changes and apply them against real infrastructure.\nIt\u0026rsquo;s a simple idea with a variety of possible implementations.\nCore individual workflow This is the most simple workflow if you work alone on a relatively small TF project. This workflow suits both local and remote backends well.\nLet\u0026rsquo;s add a bit of Git\nWrite You clone the remote code repo or pull the latest changes, edit the configuration code, then run the terraform validate and terraform fmt commands to make sure your code works well.\nPlan This is where you run the terraform plan command to make sure that your changes do what you need. This is a good time to commit your code changes changes (or you can do it in the next step).\nApply This is when you run terraform apply and introduce the changes to real infrastructure objects. Also, this is when you push committed changes to the remote repository.\nCore team workflow This workflow is good for when you work with configuration code in a team and want to use feature branches to manage the changes accurately.\nDon\u0026rsquo;t get scared, it is still simple, just follow the lines\nWrite Start by checking out a new branch, make your changes, and run the terraform validate and terraform fmt commands to make sure your code works well.\nRunning terraform plan at this step will help ensure that you\u0026rsquo;ll get what you expect.\nPlan This is where code and plan reviews happen.\nAdd the output of the terraform plan command to the Pull Request with your changes. It would be a good idea to add only the changed parts of the common output, which is the part that starts with \u0026ldquo;Terraform will perform the following actions\u0026rdquo; string.\nApply Once the PR is reviewed and merged to the upstream branch, it is safe to finally pull the upstream branch locally and apply the configuration with terraform apply.\nTeam workflow with automation In a nutshell, this workflow allows you to introduce a kind of smoke test for your infrastructure code (using plan) and also to automate the feedback in the CI process.\nThe automated part of this workflow consists of a speculative plan on commit and/or Pull Request (PR ), along with adding the output of plan to the comment of the PR. A speculative plan mean just to show the changes, and not apply them afterward.\nI like when TF plan output is included to PR, but nobody likes to read others TF plans for some reason\u0026hellip;\nWrite This step is the same as in the previous workflow.\nPlan This is where your CI tool does its job.\nLet‚Äôs review this step by step:\n You create a PR with the code changes you wish to implement. The CI pipeline is triggered by an event from your code repository (such as webhook push) and it runs a speculative plan against your code. The list of changes (a so-called \u0026ldquo;plan diff\u0026rdquo;) is added to PR for review by the CI. Once merged, the CI pipeline runs again and you get the final plan that\u0026rsquo;s ready to be applied to the infrastructure.  Apply Now that you have a branch (i.e. main) with the fresh code to apply, you need to pull it locally and run terraform apply.\nYou can also add the automated apply here ‚Äì step 5 in the picture below. This may be very useful for disposable environments such as testing, staging, development, and so on.\nThe exact CI tool to be used here is up to you: Jenkins, GitHub Actions, and Travis CI all work well.\nAn important thing to note is that the CI pipeline must be configured in a bi-directional way with your repository to get the code from it and report back with comments to PR.\nAs an option, you may consider using Terraform Cloud which has a lot of functionality, including the above mentioned repo integration, even with the free subscription.\nIf you have never worked with Terraform Cloud before and want to advice to get started, I\u0026rsquo;ll provide the links at the end of this article.\nImport workflow This workflow refers to a situation when you have some objects already created (i.e., up and running), and you need to manage them with Terraform.\nSuppose we already have an S3 bucket in AWS called \u0026ldquo;someassetsbucket\u0026rdquo; and we want to include it into our configuration code.‚Äå‚Äå\nPrepare You should create a resource block to be used later for the real object you‚Äôre going to import.\nYou don‚Äôt need to fill the arguments in it at the start, so it may be just a blank resource block, for example:\nresource \u0026quot;aws_s3_bucket\u0026quot; \u0026quot;someassetsbucket\u0026quot; { ‚Äå‚Äå } Import Now you need to import the information about the real object into your existing Terraform state file.\nThis can be done with the terraform import command, for example:\nterraform import aws_s3_bucket.assets \u0026quot;someassetsbucket\u0026quot; ‚ÄåBe sure to also check the list of possible options import accepts with terraform import -h\nWrite Now you need to write the corresponding Terraform code for this bucket.\nTo avoid modifying your real object on the terraform apply action, you should specify all needed arguments with the exact values from the import phase.\nYou can see the details by running the terraform state show command, for example:\nterraform state show aws_s3_bucket.assets The output of this command will be very similar to the configuration code. But it contains both arguments and attributes of the resource, so you need to clean it up before applying it.\nYou can use one of the following tactics:\n either copy/paste it, and then run terraform validate and terraform plan several times to make sure there are no errors like \u0026ldquo;argument is not expected here\u0026rdquo; or \u0026ldquo;this field cannot be set\u0026rdquo; or you can pick and write only the necessary arguments  In any case, be sure to refer to the documentation of the resource during this process.\nPlan The goal is to have a terraform plan output showing \u0026ldquo;~ update in-place\u0026rdquo; changes only.\nHowever, it is not always clear whether the real object will be modified or only the state file will be updated. This is why you should understand how a real object works and know its life cycle to make sure it is safe to apply the plan.\nApply This is usual the terraform apply action.\nOnce applied, your configuration and state file will correspond to the real object configuration.\nWrapping up Here is an overview of Terraform Cloud for those who never worked with it before: ‚Äå‚ÄåOverview of Terraform Cloud Features\nAnd here is a nice tutorial to start with: Get Started - Terraform Cloud\nAlso, here is an overview of workflows at scale from the HashiCorp CTO which might be useful for more experienced Terraform users: Terraform Workflow Best Practices at Scale\nThank you for reading. I hope you will try one of these workflows, or develop your own!‚Äå‚Äå\n This article was originaly published on FreeCodeCamp paltform by me, but I still want to keep it here for the record. Canonical link to original publication was properly set in the page headers. ","permalink":"https://serhii.vasylenko.info/2020/09/16/terraform-workflow-working-individually-and-in-a-team.html","summary":"The work with Terraform code may become tangled sometimes. Here are some guides on how to streamline it and make it transparent for you and your team.\nIt is extremely helpful in a team, and can benefit you even if you work individually. A good workflow enables you to streamline a process, organize it, and make it less error-prone.\nThis article summaries several approaches when working with Terraform, both individually and in a team.","title":"Terraform Workflow ‚Äî Working Individually and in a Team"},{"content":"I successfully passed the \u0026ldquo;HashiCorp Certified ‚Äî Terraform Associate\u0026rdquo; exam last Friday and decided to share some advice for exam preparation.\nMake yourself a plan Make a list of things you are going to go through: links to the study materials, practice tasks, some labs, some articles on relative blogs (Medium, Dev.to, etc.). It should look at a \u0026ldquo;todo\u0026rdquo; or \u0026ldquo;check\u0026rdquo;-list. It may seem silly at first glance, but the list with checkboxes does its \u0026ldquo;cognitive magic\u0026rdquo;. When you go point by point, marking items as \u0026ldquo;done\u0026rdquo;, you feel the progress and this motivates you to keep going further. For example, you can make a plan from the resources I outlined below in this article.\nI encourage you to explore the Internet for something by yourself as well. Who knows, perhaps you will find some learning course that fits you better. And that is great! However, when you find it, take extra 5-10 minutes to go through its curriculum and create a list with lessons.\nIt feels so nice to cross out items off the todo list, believe me üòÑ Go through the official Study Guide Despite your findings on the Internet, I strongly suggest going through the official study guide\nStudy Guide - Terraform Associate Certification\nIt took me about 20 hours to complete it (including practice tasks based on topics in the guide), and it was the core of my studying. I did not buy or search for some third-party course intentionally because I did have some Terraform experience before starting the preparation. But give the official guide a chance even if you found some course. It is well-made and matches real exam questions very precisely.\nAlso, there is an official Exam Review. Someone might find this even better because it is a direct mapping of each exam objective to HashiCorp\u0026rsquo;s documentation and training.\nTake additional tutorials Here is a list of additional tutorials and materials I suggest adding into your learning program:\nOfficial guides / documentation:  Automate Terraform Collaborate using Terraform Cloud Terraform tutorials Reuse Configuration with Modules A Practitioner‚Äôs Guide to Using HashiCorp Terraform Cloud with GitHub Enforce Policy with Sentinel  Third-party articles and guides:  Using the terraform console to debug interpolation syntax YouTube playlist with exam-like questions review  Find yourself some practice Mockup a project You can greatly improve your practice by mocking some real business cases.\nIf you already work in some company you can set up the project you\u0026rsquo;re working with using Terraform. If you don‚Äôt have a real project or afraid to accidentally violate NDA, try this open-source demo project: Real World Example Apps.\nIt is a collection of different codebases for front-end and back-end used to build the same project. Just find the combination that suits your experience better and try to build the infrastructure for it using Terraform.\nAnswer forum topics Last but not least advice ‚Äî try to answer some questions on the official Terraform forum.\nThis is a nice way to test your knowledge, help others, and develop the community around Terraform. Just register there, look for the latest topics, and have fun!\nüçÄ I sincerely wish you exciting preparation and a successful exam! üçÄ ","permalink":"https://serhii.vasylenko.info/2020/09/15/terraform-certification-tips.html","summary":"I successfully passed the \u0026ldquo;HashiCorp Certified ‚Äî Terraform Associate\u0026rdquo; exam last Friday and decided to share some advice for exam preparation.\nMake yourself a plan Make a list of things you are going to go through: links to the study materials, practice tasks, some labs, some articles on relative blogs (Medium, Dev.to, etc.). It should look at a \u0026ldquo;todo\u0026rdquo; or \u0026ldquo;check\u0026rdquo;-list. It may seem silly at first glance, but the list with checkboxes does its \u0026ldquo;cognitive magic\u0026rdquo;.","title":"Terraform Certification Tips"},{"content":"Surprisingly, a lot of beginners skip over Terraform modules for the sake of simplicity, or so they think. Later, they find themselves going through hundreds of lines of configuration code.\nI assume you already know some basics about Terraform or even tried to use it in some way before reading the article.\nPlease note: I do not use real code examples with some specific provider like AWS or Google intentionally, just for the sake of simplicity.\nTerraform modules You already write modules even if you think you don‚Äôt.\nEven when you don\u0026rsquo;t create a module intentionally, if you use Terraform, you are already writing a module ‚Äì a so-called \u0026ldquo;root\u0026rdquo; module.\nAny number of Terraform configuration files (.tf) in a directory (even one) forms a module.\nWhat does the module do? A Terraform module allows you to create logical abstraction on the top of some resource set. In other words, a module allows you to group resources together and reuse this group later, possibly many times.\nLet\u0026rsquo;s assume we have a virtual server with some features hosted in the cloud. What set of resources might describe that server? For example: ‚Äì the virtual machine itself (created from some image) ‚Äì an attached block device of specified size (for additional storage) ‚Äì a static public IP mapped to the server\u0026rsquo;s virtual network interface ‚Äì a set of firewall rules to be attached to the server ‚Äì something else\u0026hellip; (i.e. another block device, additional network interface, etc)\nNow let\u0026rsquo;s assume that you need to create this server with a set of resources many times. This is where modules are really helpful ‚Äì you don\u0026rsquo;t want to repeat the same configuration code over and over again, do you?\nHere is an example that illustrates how our \u0026ldquo;server\u0026rdquo; module might be called. \u0026ldquo;To call a module\u0026rdquo; means to use it in the configuration file.\nHere we create 5 instances of the \u0026ldquo;server\u0026rdquo; using single set of configurations (in the module):\nmodule \u0026quot;server\u0026quot; { count = 5 source = \u0026quot;./module_server\u0026quot; some_variable = some_value } Modules organisation: child and root Of course, you would probably want to create more than one module. Here are some common examples:\n for a network (i.e. VPC) for a static content hosting (i.e. buckets) for a load balancer and it\u0026rsquo;s related resources for a logging configuration and whatever else you consider a distinct logical component of the infrastructure  Let\u0026rsquo;s say we have two different modules: a \u0026ldquo;server\u0026rdquo; module and a \u0026ldquo;network\u0026rdquo; module. The module called \u0026ldquo;network\u0026rdquo; is where we define and configure our virtual network and place servers in it:\nmodule \u0026quot;server\u0026quot; { source = \u0026quot;./module_server\u0026quot; some_variable = some_value } module \u0026quot;network\u0026quot; { source = \u0026quot;./module_network\u0026quot; some_other_variable = some_other_value } Once we have some custom modules, we can refer to them as \u0026ldquo;child\u0026rdquo; modules. And the configuration file where we call child modules relates to the root module.\nA child module can be sourced from a number of places:\n local paths official Terraform Registry (if you\u0026rsquo;re familiar with other registries, i.e. Docker Registry then you already understand the idea) Git repository (a custom one or GitHub/BitBucket) HTTP URL to .zip archive with module  But how can you pass resources details between modules?\nIn our example, the servers should be created in a network. So how can we tell the \u0026ldquo;server\u0026rdquo; module to create VMs in a network which was created in a module called \u0026ldquo;network\u0026rdquo;?\nThis is where encapsulation comes in.\nModule encapsulation Encapsulation in Terraform consists of two basic concepts: module scope and explicit resources exposure.\nModule Scope All resource instances, names, and therefore, resource visibility, are isolated in a module\u0026rsquo;s scope. For example, module \u0026ldquo;A\u0026rdquo; can\u0026rsquo;t see and does not know about resources in module \u0026ldquo;B\u0026rdquo; by default.\nResource visibility, sometimes called resource isolation, ensures that resources will have unique names within a module\u0026rsquo;s namespace. For example, with our 5 instances of the \u0026ldquo;server\u0026rdquo; module:\nmodule.server[0].resource_type.resource_name module.server[1].resource_type.resource_name module.server[2].resource_type.resource_name On the other hand, we could create two instances of the same module with different names:\nmodule \u0026quot;server-alpha\u0026quot; { source = \u0026quot;./module_server\u0026quot; some_variable = some_value } module \u0026quot;server-beta\u0026quot; { source = \u0026quot;./module_server\u0026quot; some_variable = some_value } In this case, the naming or address of resources would be as follows:\nmodule.server-alpha.resource_type.resource_name module.server-beta.resource_type.resource_name Explicit resources exposure If you want to access some details for the resources in another module, you\u0026rsquo;ll need to explicitly configure that.\nBy default, our module \u0026ldquo;server\u0026rdquo; doesn\u0026rsquo;t know about the network that was created in the \u0026ldquo;network\u0026rdquo; module.\nSo we must declare an output value in the \u0026ldquo;network\u0026rdquo; module to export its resource, or an attribute of a resource, to other modules.\nThe module \u0026ldquo;server\u0026rdquo; must declare a variable to be used later as the input.\nThis explicit declaration of the output is the way to expose some resource (or information about it) outside ‚Äî to the scope of the \u0026lsquo;root\u0026rsquo; module, hence to make it available for other modules.\nNext, when we call the child module \u0026ldquo;server\u0026rdquo; in the root module, we should assign the output from the \u0026ldquo;network\u0026rdquo; module to the variable of the \u0026ldquo;server\u0026rdquo; module:\nnetwork_id = module.network.network_id Here is how the final code for calling our child modules will look like in result:\nmodule \u0026quot;server\u0026quot; { count = 5 source = \u0026quot;./module_server\u0026quot; some_variable = some_value network_id = module.network.network_id } module \u0026quot;network\u0026quot; { source = \u0026quot;./module_network\u0026quot; some_other_variable = some_other_value } This example configuration would create 5 instances of the same server, with all the necessary resources, in the network we created with as a separate module.\nWrap up Now you should understand what modules are and what do they do.\nIf you\u0026rsquo;re at the beginning of your Terraform journey, here are some suggestions for the next steps.\nI encourage you to take this short tutorial from HashiCorp, the creators of Terraform, about modules: \u0026ldquo;Organize Configuration\u0026rdquo;\nAlso, there is a great comprehensive study guide which covers everything from beginner to advanced concepts about Terraform: \u0026ldquo;Study Guide - Terraform Associate Certification\u0026rdquo;\nThe modular code structure makes your configuration more flexible and yet easy to be understood by others. The latter is especially useful in teamwork.\n This article was originaly published on FreeCodeCamp paltform by me, but I still want to keep it here for the record. Canonical link to original publication was properly set in the page headers. ","permalink":"https://serhii.vasylenko.info/2020/09/09/terraform-modules-explained.html","summary":"Surprisingly, a lot of beginners skip over Terraform modules for the sake of simplicity, or so they think. Later, they find themselves going through hundreds of lines of configuration code.\nI assume you already know some basics about Terraform or even tried to use it in some way before reading the article.\nPlease note: I do not use real code examples with some specific provider like AWS or Google intentionally, just for the sake of simplicity.","title":"What are Terraform Modules and how do they work?"},{"content":"Here is some CLI shortcuts I use day-to-day to simplify and speed-up my Terraform workflow. Requirements \u0026mdash; bash-compatible interpreter, because aliases and functions described below will work with bash, zsh and ohmyzsh.\nIn order to use any of described aliases of functions, you need to place it in your ~/.bashrc or ~/.zshrc file (or any other configuration file you have for your shell).\nThen just source this file, for example: source ~/.zshrc\nFunction: list outputs and variables of given module You need to provide the path to module directory, and this function will list all declared variables and outputs module has. It comes very useful when you don\u0026rsquo;t remember them all and just need to take a quick look.\n## TerraForm MOdule Explained function tfmoe { echo -e \u0026#34;\\nOutputs:\u0026#34; grep -r \u0026#34;output \\\u0026#34;.*\\\u0026#34;\u0026#34; $1 |awk \u0026#39;{print \u0026#34;\\t\u0026#34;,$2}\u0026#39; |tr -d \u0026#39;\u0026#34;\u0026#39; echo -e \u0026#34;\\nVariables:\u0026#34; grep -r \u0026#34;variable \\\u0026#34;.*\\\u0026#34;\u0026#34; $1 |awk \u0026#39;{print \u0026#34;\\t\u0026#34;,$2}\u0026#39; |tr -d \u0026#39;\u0026#34;\u0026#39; } Example usage:\nuser@localhost $: tfmoe ./module_alb Outputs: alb_arn Variables: acm_certificate_arn lb_name alb_sg_list subnets_id_list tags Function: pre-fill module directory with configuration files You need to provide a path to the module directory and this function will create a bunch of empty \u0026lsquo;default\u0026rsquo; .tf files in it.\n#TerraForm MOdule Initialize function tfmoi { touch $1/variables.tf touch $1/outputs.tf touch $1/versions.tf touch $1/main.tf } Example usage:\nuser@localhost $: mkdir ./module_foo \u0026amp;\u0026amp; temoi $_ user@localhost $: ls ./module_foo main.tf outputs.tf variables.tf versions.tf Aliases The purpose of these aliases is just to keep you from typing long commands when you want to do a simple action.\nalias tf=\u0026#39;terraform\u0026#39; alias tfv=\u0026#39;terraform validate\u0026#39; alias tfi=\u0026#39;terraform init\u0026#39; alias tfp=\u0026#39;terraform plan\u0026#39; This one is useful because it makes format tool to go in-depth (recursively) through directories.\nalias tfm=\u0026#39;terraform fmt -recursive\u0026#39; Example usage:\nuser@localhost $: tfm module_ecs_cluster/ecs.tf module_alb/alb.tf ","permalink":"https://serhii.vasylenko.info/2020/08/25/terraform-cli-shortcuts.html","summary":"Here is some CLI shortcuts I use day-to-day to simplify and speed-up my Terraform workflow. Requirements \u0026mdash; bash-compatible interpreter, because aliases and functions described below will work with bash, zsh and ohmyzsh.\nIn order to use any of described aliases of functions, you need to place it in your ~/.bashrc or ~/.zshrc file (or any other configuration file you have for your shell).\nThen just source this file, for example: source ~/.","title":"Terraform CLI shortcuts"},{"content":"Lookup plugins for Ansible allow you to do a lot of cool things. One of them is to securely pass sensitive information to your playbooks.\nIf you manage some apps in AWS with Ansible, then using Parameter Store or Secrets Manager along with it might greatly improve your security.\nDescribed plugins are the part of amazon.aws collection in Ansible Galaxy.\nRun the following command to install this collection:\nansible-galaxy collection install amazon.aws Variables with SSM Parameter Store Let\u0026rsquo;s say you have some variables defined in \u0026lsquo;defaults/main.yaml\u0026rsquo; file of your role or maybe in group_vars.yaml file.\n# content of dev.vars.yaml to be included in your play or roleuse_tls:trueapplication_port:3000app_env:developmentstripe_api_key:1HGASU2eZvKYlo2CT5MEcnC39HqLyjWDIf you store such things locally on Ansible control node, you probably encrypt it with ansible-vault\nSSM Parameter Store gives you more flexibility and security by centralized storage and management of parameters and secrets, so let\u0026rsquo;s use it with Ansible:\n# content of dev.vars.yaml to be included in your play or roleuse_tls:\u0026#34;{{ lookup(\u0026#39;amazon.aws.aws_ssm\u0026#39;, \u0026#39;/dev/webserver/use_tls\u0026#39;, bypath=true) }}\u0026#34;application_port:\u0026#34;{{ lookup(\u0026#39;amazon.aws.aws_ssm\u0026#39;, \u0026#39;/dev/webserver/application_port\u0026#39;, bypath=true) }}\u0026#34;app_env:\u0026#34;{{ lookup(\u0026#39;amazon.aws.aws_ssm\u0026#39;, \u0026#39;/dev/webserver/app_env\u0026#39;, bypath=true) }}\u0026#34;stripe_api_key:\u0026#34;{{ lookup(\u0026#39;amazon.aws.aws_ssm\u0026#39;, \u0026#39;/dev/webserver/stripe_api_key\u0026#39;, bypath=true) }}\u0026#34;The syntax is fairly simple:\nThe aws_ssm ‚Äì is the name of the lookup plugin.\nThe /dev/webserver/use_tls ‚Äì is the path to the key in the SSM Paramter Store.\nYou can use this anywhere you can use templating: in a play, in variables file, or a Jinja2 template.\nVariables with Secret Manager Another lookup plugin is Secrets Manager. Secrets Manager stores sensitive information in JSON format, supports rotation, encryption and other cool stuff.\nHere is a quick example of its functionality in a Playbook:\n- name:Extract something secrets from Secret Managerdebug:msg:\u0026#34;{{ lookup(\u0026#39;amazon.aws.aws_secret\u0026#39;, \u0026#39;DatabaseConnectionSettings\u0026#39;)}}\u0026#34;The above task will generate the following output\nTASK [Extract something secrets from Secret Manager] **************************************************** ok: [some_server] =\u0026gt; { \u0026#34;msg\u0026#34;: { \u0026#34;dbname\u0026#34;: \u0026#34;database\u0026#34;, \u0026#34;engine\u0026#34;: \u0026#34;mysql\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;p@$$w0rd\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;3306\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;db_user\u0026#34; } } This is nice if you want to insert a JSON as is, but you will need additional parsing in case you want to get only some of JSON elements.\nBenefits from using lookup plugins The biggest win here is the security ‚Äî no sensitive information in the source code.\nAnother benefit is the convenience of data management: instead using built-in local vault, you can manage the secrets in centralized way.\nOne of the common use cases for this kind of setup is CI/CD pipelines that generally run in stateless environments.\nWhen using lookup plugins for Secrets Manager and Parameter Store, mind the access permissions. The assumed IAM role must allow at least the read access to SSM Parameter Store (+ KMS read access to be able to decrypt the keys) or the read access to Secrets Manager.\nYou can find documentation for described plugins here aws_ssm and here aws_secret.\nMore about lookup plugins: https://docs.ansible.com/ansible/latest/plugins/lookup.html\n","permalink":"https://serhii.vasylenko.info/2020/08/06/manage-ansible-playbook-secrets-with-aws-services/","summary":"Lookup plugins for Ansible allow you to do a lot of cool things. One of them is to securely pass sensitive information to your playbooks.\nIf you manage some apps in AWS with Ansible, then using Parameter Store or Secrets Manager along with it might greatly improve your security.\nDescribed plugins are the part of amazon.aws collection in Ansible Galaxy.\nRun the following command to install this collection:\nansible-galaxy collection install amazon.","title":"Manage Ansible playbook secrets with AWS services"},{"content":"You might have heard about Terraform before, but if you have never tried it, this blog can help you to get the main point.\nA few words about ‚ÄúInfrastructure as Code\u0026quot; First of all, Terraform is the way to manage the infrastructure in the form of code. The same way developers write the code to create applications, Terraform code can create the resources in virtual data centers (i.e., clouds).\nInfrastructure as Code, or IaC, is when you describe and manage your infrastructure as‚Ä¶ (guess what?) ‚Ä¶code, literally.\nIn a nutshell, that means you can define all the elements (servers, networks, storage, etc.) and resources (memory, CPU, etc.) of your infrastructure in configuration files, and manage it in a way similar to how you handle the source code of the applications: branches, releases, and all that stuff.\nAnd the main idea behind the IaC approach is that it manages the state of things and must remain the single source of truth (configuration truth) for your infrastructure.\nFirst, you define the state via the code. Then IaC tool (Terraform, for example) applies this state to the infrastructure: all that is missing according to the code will be created, all that differs from the code will be changed, and all that exists in the infrastructure but is not described via code ‚Äî will be destroyed.\nWhy and when do you need the Terraform for a project? Terraform is a specific tool, hence like any other tool, it has its particular application area. There is no strict definition of project kind that needs Terraform (surprise!), but in general, you need to consider using Terraform if you answer ‚Äòyes‚Äô to one of the following questions:\n Do you have multiple logical elements of the same kind (in plural) in your infrastructure, i.e., several web servers, several application servers, several database servers? Do you have numerous environments (or workspaces) where you run your applications, i.e., development, staging, QA, production? Do you spend a significant amount of time managing the changes in the environment(s) where you run your applications?  How does it work? Terraform works with the source code of configuration and interprets the code into real objects inside on-premise or cloud platforms.\n  How Terraform works in a nutshell\n  Terraform supports many platforms: cloud providers such as AWS, Azure, GCP, DigitalOcean, and other platforms such as OVH, 1\u0026amp;1, Hetzner, etc. It also supports infrastructure software such as Docker, Kubernetes, Chef, and even databases and monitoring software. That is why Terraform is so popular ‚Äî it is an actual Swiss knife in the operations world.\nSo to create, change, or destroy the infrastructure, Terraform needs the source code.\nThe source code is a set of configuration files that defines your infrastructure state. The code uses its syntax, but it looks very user-friendly. Here is an example: the following configuration block describes the virtual server (EC2 instance) in AWS.\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;web_server\u0026#34; { ami = \u0026#34;ami-a1b2c3d4\u0026#34; instance_type = \u0026#34;t3.micro\u0026#34; } Terraform can automatically detect the dependencies between resources described in the code and also allows you to add custom dependencies when needed.\nWhen you apply the code the first time, Terraform creates a so-called ‚Äústate file,\u0026quot; which Terraform uses to map your code to resources created in the hosting platform. Terraform will use each subsequent ‚Äúapply\u0026quot; action to compare the code changes with the sate file to decide what should be done (and in what order) against real infrastructure.\nOne of the essential functions of the state file is the management of dependencies between the resources. For example (some technical nuances are omitted for simplicity): if you have a server created inside some network and you are going to change the network configuration in Terraform code, Terraform will know it should change that server configuration, or the server should be re-created inside the updated network.\nWhat does Terraform consist of: Terraform configuration code consists of several elements: providers, resources, modules, input variables, output values, local values, expressions, functions.\nProvider Provider is an entity that defines what exactly is possible to do with the cloud or on-premises infrastructure platform you manage via Terraform.\nIt translates your code into proper API calls to the hosting provider, transforming your configuration into real object: servers, networks, databases, and so on.\nResource Resource is the essential part of the configuration code. That is where the definition of infrastructure objects happens.\nResources are the main building blocks of the whole code. A resource can represent some object in the hosting provider (example: server) or the part of a compound object (example: attachable storage for a server)\nEvery resource has a type and local name. For example, here is how EC2 instance configuration may look like:\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;web_server\u0026#34; { ami = \u0026#34;ami-a1b2c3d4\u0026#34; instance_type = \u0026#34;t3.micro\u0026#34; } The aws_instance is a resource type, and web_server is the local resource name. Later, when Terraform applies this code, it will create an EC2 instance with some particular ID in AWS.\nOnce created, Terraform will store the ID in the state file with mapping information that logically connects it with web_server.\nThe ami, instance_type, and private_ip are the arguments with values that define the actual state of the resource. However, there are many value types, depending on the particular argument and particular resource type, so I will not focus on them here.\nModules   Terraform module\n  Modules is the kind of logical containers or groups for resources you define and use together. The purpose of modules is the grouping of resources and the possibility of reusing the same code with different variables.\nLet‚Äôs get back to the example with the EC2 instance and say you need to have a static public IP address with it. In such a case, here is how the module for web server may look like:\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;web_server\u0026#34; { ami = \u0026#34;ami-a1b2c3d4\u0026#34; instance_type = \u0026#34;t3.micro\u0026#34; } resource \u0026#34;aws_eip\u0026#34; \u0026#34;web_server_public_ip\u0026#34; { instance = \u0026#34;${aws_instance.web_server.id}\u0026#34; } Having these two resources together allows us to think of it as a stand-alone unit you can reuse later, for example, in our development, staging, and production environments. And not by copying and pasting it, but via reference to the module defined only once.\nPlease note: we specified an instance argument inside the aws_eip resource to reference another resource details (the ID of an instance). It is possible because of the way how Terraform treats dependencies. For example, when it detects the dependency (or you define it explicitly), Terraform creates the leading resource first. Only after the resource is created and available Terraform will create the dependent one.\nThe modules is a kind of standalone topic in Terraform. There is a separate article in my blog that explains what modules are and how do they work.\nVariables Input variables work as parameters for the modules so module code could be reusable. Let‚Äôs look at the previous example: it has some hardcoded values ‚Äî instance image ID and instance type. Here is how you can make it more abstract and reusable:\nvariable \u0026#34;image_id\u0026#34; { type = string } variable \u0026#34;instance_type\u0026#34; { type = string } resource \u0026#34;aws_instance\u0026#34; \u0026#34;web_server\u0026#34; { ami = var.image_id instance_type = var.instance_type } Values for the variables then can be passed either via CLI and environment variables (if you have only the one, so-called root module) or via explicit values in the block where you call a module, for example:\nmodule \u0026#34;web_server_production\u0026#34; { source = \u0026#34;./modules/web_server\u0026#34; image_id = \u0026#34;ami-a1b2c3d4\u0026#34; instance_type = \u0026#34;m5.large\u0026#34; } module \u0026#34;web_server_development\u0026#34; { source = \u0026#34;./modules/web_server\u0026#34; image_id = \u0026#34;ami-a2b3c4d5\u0026#34; instance_type = \u0026#34;t3.micro\u0026#34; } Output values are similar to the \u0026ldquo;return\u0026rdquo; of a function in development language. You can use them for dependencies management (for example, when a module requires something from another module) and print specific values at the end of Terraform work (for example, to be used for notification in the CI/CD process).\nLocal values, expressions, functions ‚Äî three more things that augment the capabilities of Terraform and make it more similar to a programming language (which is excellent, by the way).\nThe local values are used inside modules for extended data manipulations.\nThe expressions are used to set the values (for many things), such as the value of some argument in resource configuration. For example, they used either to refer something (just as we referenced instance ID \u0026quot;${aws_instance.web_server.id}\u0026quot; in the example above) or to compute the value within your configuration.\nThe functions in Terraform are built-in jobs you can call to transform and combine values. For example, the tolist() function converts its argument to a list value.\nAnd this is it?   Yes, in short words ‚Äî this is what Terraform is. Not rocket science if it\u0026rsquo;s about to manage a small infrastructure, but it gets more complicated with bigger infrastructure. Like any other engineering tool, though.\nOkay, what next? If you read down to this point, then it means it is worth \u0026ldquo;get your hands dirty\u0026rdquo; and to try building your Infrastructure with Terraform. There are plenty of courses and books (and the \u0026ldquo;Terraform up and running\u0026rdquo; is one of the most popular). Still, my learning path started from the following: Official guide from Hashicorp ‚Äî comprehensive and free guide from Terraform developers. Just pick your favorite cloud (AWS, Azure, GCP) and go through the topics.\nAnother thing worth your attention is A Comprehensive Guide to Terraform.\nOnce you finish this guide, I suggest jumping into the more real-world things and describing the infrastructure of the most common project you work with.\nYour hands-on experience is the best way to learn Terraform!\n","permalink":"https://serhii.vasylenko.info/2020/05/02/Terraform-explained-for-managers.html","summary":"You might have heard about Terraform before, but if you have never tried it, this blog can help you to get the main point.\nA few words about ‚ÄúInfrastructure as Code\u0026quot; First of all, Terraform is the way to manage the infrastructure in the form of code. The same way developers write the code to create applications, Terraform code can create the resources in virtual data centers (i.e., clouds).\nInfrastructure as Code, or IaC, is when you describe and manage your infrastructure as‚Ä¶ (guess what?","title":"Terraform explained in English"},{"content":"Although Github Actions service is generally available since November 13, 2020, and there are about 243,000,000 results for \u0026ldquo;github actions\u0026rdquo; in Google search already, I have just reached it\u0026hellip;\nIt\u0026rsquo;s half past midnight, it took me about 35 commits to make my first github automation work, but it finally works and this blog post was built and published automatically!\nActions everywhere One of the most (or maybe the most one) powerful things in Actions is \u0026hellip; Actions! Github made a simple but genius thing: they turned well-known snippets (we do with pipelines) into the marketplace of well-made (sometimes not) simple and complex applications you can use in your automation workflow. https://github.com/marketplace?type=actions\nSo now you can either re-invent your wheel or re-use someone else\u0026rsquo;s code to make the needed automation.\nI decided to automate publications to this blog via Actions in order to have some practice.\nThere are two workflows: one for the blog (website), and one for the CV (cv).\n actions/checkout@v2 actions/upload-artifact@v2 actions/download-artifact@v2  In both workflows, the build job is performed within a container, which is different per workflow: Ruby for the blog and Pandoc for CV.\nHere is how the build job looks like for the blog:\njobs:build:runs-on:ubuntu-latestcontainer:image:ruby:2.6.4options:--workdir /src steps:- name:Checkoutuses:actions/checkout@v2 - name:Build blogrun:|bundle install bundle exec jekyll build --verbose --destination _site- name:Upload artifactsuses:actions/upload-artifact@v2with:name:_sitepath:_siteAs you can see, I run the steps within the Ruby container. This simplifies things related to file permissions and directory mounting because checkout is made inside the container.\nThe deploy step is performed via shell run command for now, for better clearness (can be replaced to third-party action or custom-made one): it makes a commit to gh-pages branch which is configured for Github Pages.\ndeploy:if:github.ref == \u0026#39;refs/heads/master\u0026#39;needs:buildruns-on:ubuntu-lateststeps:- name:Checkout gh-pages branchuses:actions/checkout@v2with:ref:\u0026#39;gh-pages\u0026#39;- name:Get the build artifactuses:actions/download-artifact@v2with:name:_sitepath:./- name:Deploy (push) to gh-pagesrun:|git config user.name \u0026#34;$GITHUB_ACTOR\u0026#34; git config user.email \u0026#34;${GITHUB_ACTOR}@bots.github.com\u0026#34; git add -A git commit -a -m \u0026#34;Updated Website\u0026#34; git remote set-url origin \u0026#34;https://x-access-token:${{ secrets.DEPLOY_TOKEN }}@github.com/vasylenko/serhii.vasylenko.info.git\u0026#34; git push --force-with-lease origin gh-pagesOld good things made better A lot of common things have been introduced to GitHubActions with some sweet additions:\n you can also specify different environments for your jobs in the same workflow; you can use environment variables with a different visibility scope: either workflow, or job, or step; you can use cache for dependencies and reuse it between workflow runs while keeping workflow directory clean; you can trigger a workflow by repo events and have a quite complex conditional logic or filters (if needed), external webhooks and by a schedule; you can pass artifacts between jobs inside a workflow with ease - Github provides simple actions for this, so you don\u0026rsquo;t need to dance around temporary directories or files; and much more  ","permalink":"https://serhii.vasylenko.info/2020/03/18/github-actions-first-impression.html","summary":"Although Github Actions service is generally available since November 13, 2020, and there are about 243,000,000 results for \u0026ldquo;github actions\u0026rdquo; in Google search already, I have just reached it\u0026hellip;\nIt\u0026rsquo;s half past midnight, it took me about 35 commits to make my first github automation work, but it finally works and this blog post was built and published automatically!\nActions everywhere One of the most (or maybe the most one) powerful things in Actions is \u0026hellip; Actions!","title":"Github Actions - First impression"},{"content":"926 out of 1000 Last week I\u0026rsquo;ve successfully passed AWS SAA exam with 926 points from 1000 possible. I can\u0026rsquo;t help saying this and showing off my verification page{:target=\u0026quot;_blank\u0026quot;}, just because I am very happy so please excuse me my bragging.\nWhat helped me But I would like to share some advices and tips with anyone who reads this and wants to pass the exam. I mean, I could just twit about it if that was only about saying \u0026ldquo;hey look at me!\u0026rdquo;, right?\nIt took me a month of intensive studying and here is what helped me:\n  Video course at CloudGuru - AWS Certified Solutions Architect Associate{:target=\u0026quot;_blank\u0026quot;}.\nPrice: $50 for a monthly subscription.\nTips: They have a 7 days free trial, which is actually quite enough to view the whole course. But I strongly recommend purchasing a full month, because it is better to view the lectures gradually during couple of weeks for better learning. Plus they have a nice exam simulator where you can practice several times.\n  Practice Tests set at Udemy - AWS Certified Solutions Architect Associate Practice Exams{:target=\u0026quot;_blank\u0026quot;}.\nPrice: $40 or only $12 if you\u0026rsquo;re lucky to get it during a sale. But they make sales quite often and they frequently provide discuounts for new students. I purchaced it for $12.\nTips: practice tests are very useful, do not skip buying them. You will find your weak spots and also learn a lot by passing these tests. This particular set has a quite good explanations for each question.\n  Exam Guide at O\u0026rsquo;relly Media AWS Certified Solutions Architect Associate All-in-One Exam Guide{:target=\u0026quot;_blank\u0026quot;}.\nPrice: this one can be easily read during 10 days free trial period :wink:\nTips: The new exam version is released on 23rd of March, so it is better to find a new updated version of exam guide. And I suggest reading the guide after the video course or vise versa, but do not mix them.\n  Making notes. Seriously, note taking helps you memorize better. Do not skip it, and note your video courses as well as exam guide. Later, you will find your notes very helpful before the exam day - they will fresh up your memory.\n  Thank you for reading down to this point. I hope my advices were helpful and you will pass the exam!\n","permalink":"https://serhii.vasylenko.info/2020/03/15/aws-solutions-architect-associate-exam-tips.html","summary":"926 out of 1000 Last week I\u0026rsquo;ve successfully passed AWS SAA exam with 926 points from 1000 possible. I can\u0026rsquo;t help saying this and showing off my verification page{:target=\u0026quot;_blank\u0026quot;}, just because I am very happy so please excuse me my bragging.\nWhat helped me But I would like to share some advices and tips with anyone who reads this and wants to pass the exam. I mean, I could just twit about it if that was only about saying \u0026ldquo;hey look at me!","title":"AWS SAA exam results"},{"content":"So you opened this page to read more about me? I am very pleased!\nI live in Kyiv, Ukraine, but I like to travel to new, unseen places whenever I have enough time for that.\nTechnical blogging is my hobby, and I am also fond of astronomy and history.\nI am the AWS Community Builder and Developer Experience Engineer at Grammarly.\nAlso, you can find me here and there:\nTwitter | Dev | LinkedIn | Github | Facebook\nSome highlights of the things I do Webinars, workshops, presentations:\n  Terraform webinar (RU): Start using Terraform if you still don\u0026rsquo;t\n  AWS webinar (RU): The path to the certification of AWS architect \n  AWS webinar (RU): Infrastructure for Software Engineers: Spin Up and Keep It Running\n  Terraform crash course (RU) in Hillel IT school. Three 2-hours classes about Terraform and Terraform modules:\n Terraform ‚Äî Introduction (RU) Terraform ‚Äî Modules (RU) Terraform ‚Äî Modules practice (RU)  ","permalink":"https://serhii.vasylenko.info/about/","summary":"So you opened this page to read more about me? I am very pleased!\nI live in Kyiv, Ukraine, but I like to travel to new, unseen places whenever I have enough time for that.\nTechnical blogging is my hobby, and I am also fond of astronomy and history.\nI am the AWS Community Builder and Developer Experience Engineer at Grammarly.\nAlso, you can find me here and there:\nTwitter | Dev | LinkedIn | Github | Facebook","title":"About me"},{"content":"Developer Experience engineer, DevOps enthusiast. In 2020 I switched from team management to an individual contributor role to feel the engineering world on the tip of my fingers again.\nCertified: AWS Solutions Architect Associate, Terraform Associate.\n From Nov'20 Developer Experience Engineer at Grammarly (full-time) Making day-to-day work of developers more productive and agile by:\n Providing automation and tooling for routine SDLC procedures Enriching experience in Infrastructure, Automation, Observability areas Standardizing commonly used infrastructure components   Jul'20 ‚Äî Sep'21 DevOps Coach at Hillel IT School (part-time) Teaching DevOps and doing workshops about AWS and Terraform.\n Mar'17 ‚Äì Jun'20 DevOps Teamlead at ITCraft (full-time) My biggest project ‚Äî my team. A skilled and highly motivated team of engineers that sincerely cares about its work outcomes.\nKey accomplishments:  Created a team of engineers that completed more than 40 successful projects of different sizes; Team size growth: from 1 to 8; Tripled the number of shared projects with other departments by boosting department reputation and team visibility within the company; Introduced DevOps as a Service business model in the department; Mentored senior and middle engineers from junior newcomers; Fostered the culture of ownership.  Challenges:  Transform team members' mindset toward ownership culture; Keep the team motivated despite workload level: during peaks and drawdowns; Justification of the advantages of my team to the clients on the pre-sales stage.   Nov'14 ‚Äì Sep'16 Chief Technologist at YourServerAdmin (full-time) I was responsible for the technical side of processes inside the department and researching the new technologies we could implement for our clients.\nIntroduced ITIL practices together with our COO and, what I like the most, could change how our sysadmin\u0026rsquo;s department worked with the development department. The work became more coordinated and more integrated.\nAlso, I managed the projects using practices from PRINCE2 methodology and the Theory of Constraints.\nKey accomplishments:  Improved technical expertise of a support team; Improved task management; Increased team size; Fostered the change of team mindset to Agile thinking.   Jun'11 ‚Äì Nov'14 System Administrator / Support Engineer at YourServerAdmin (full-time) Started my career in IT: grew up from a Level-1 tech support engineer (communication with customers and initial problem analysis) to Level-3 System Administrator responsible for complex technical tasks and daily shift management.\n  Skills  Leader Team motivation, mentorship, cultivation of soft/hard skills of team members. Team Fostering team values and working principles and developing the new ones together with the team.\nHiring new team members and forming the required team skillset. PM Managing stand-alone DevOps projects and operations parts of big projects with dev teams.\nResources and capacity management. Clouds Amazon Web Services. Design and fine-tuning of resilient and HA infrastructures; costs optimization and security hardening. 4 years of hands-on experience. Techs Jenkins, GitlabCI, Github Actions, Bitrise, CircleCI.\nTerraform, Docker, Ansible, Nginx, Apache, Databases. English C1 (CEFR) / Advanced.  Activities and interests  Travel I like to explore new cities and countries, and I like trekking. I\u0026rsquo;ve been to the Annapurna base camp, and now I want to visit Everest\u0026rsquo;s base camp. Blogging I love to write tutorials and articles about the technologies I use and learn. Science Fond of History, Astronomy, and Physics. I wish to see the Betelgeuse supernova explosion someday, even though the expected explosion date is somewhere between today and 100k years.  ","permalink":"https://serhii.vasylenko.info/cv/","summary":"Serhii Vasylenko ‚Äî professional experience","title":"Serhii Vasylenko"}]