[{"content":"As senior engineers, we often hit the ceiling at some point: our individual output is high, but writing more code or spinning up one more cluster does not feel like the best way to make more impact.\nA more significant impact comes from multiplying our efforts. But how do you do that without a team reporting to you? How do you effectively involve peers in your projects, influencing their priorities when you don\u0026rsquo;t control their backlog or performance review?\nI had been looking for the answers to these questions for quite a long time, and then I realized there was no definitive answer or a straightforward rule to follow. Yet my brain always needs something systemized, and for those like me, I decided to compile this write-up: synthesis of my personal experience that worked well and learnings from books and blogs on that topic.\nThe foundation of influencing peers lies in understanding their perspective. Before asking someone to take on work, especially the work you\u0026rsquo;re proposing, you need to answer the fundamental question: What\u0026rsquo;s in it for them?\nThis is my very first non-technical blog. And I hope it is not the last either 😄\nUnderstanding the \u0026ldquo;Why\u0026rdquo;: The Engine of Peer Collaboration Why would a fellow engineer prioritize something offered by a peer like you rather than a directive from their manager?\nIt rarely comes down to just one reason. Usually, it\u0026rsquo;s a blend of motivations deeply rooted in personal drive and effective team dynamics:\nThe Hunger for Growth \u0026amp; Mastery: Does this task offer a chance to learn a coveted new skill, dive deeper into an interesting technology, or tackle a challenge that stretches their capabilities? Tapping into this intrinsic drive is incredibly powerful.\nThe Desire for Impact \u0026amp; Purpose: Can you clearly articulate how this work contributes to something meaningful? Whether it\u0026rsquo;s a key product goal, squashing critical tech debt, or improving life for the whole team, connecting the task to the bigger picture gives it weight.\nThe Value of Visibility \u0026amp; Recognition: Will this work get noticed? Could it lead to a cool demo, wider acknowledgment, or even open doors for future opportunities? Sometimes, positive exposure is a significant motivator.\nThe Power of Connection \u0026amp; Collaboration: Is working closely with you part of the appeal? Offering a chance to learn directly from a respected senior peer, build rapport, and strengthen their network can be a draw in itself.\nThe Relief of Reducing Future Pain: Does this task promise to automate some hard work, fix a persistent thorn in the side, or establish a pattern that saves everyone time down the line? Framing it as an investment with future payoffs can be compelling.\nThe Spark of Intrinsic Interest: Sometimes, the problem itself is just plain fascinating, or it involves technology the peer genuinely enjoys working with. Don\u0026rsquo;t underestimate the power of intellectual curiosity.\nThe Clarity of Strategic Alignment: This is often crucial. If you can demonstrate a direct link between the task and an agreed-upon team OKR, a strategic technical initiative, or a critical product outcome, it elevates the request beyond personal preference. It becomes a shared priority, providing justification for them (and potentially their manager) to consider adjusting focus.\nThe Pull of a Shared Technical Vision: If you\u0026rsquo;re championing a specific technical direction or architecture that resonates with your peers, they\u0026rsquo;ll be more inclined to pick up tasks that help realize that vision.\nThe Currency of Reciprocity \u0026amp; Trust: Have you built \u0026ldquo;relationship capital\u0026rdquo;? A history of helping others, providing timely code reviews, and tackling unglamorous tasks yourself makes peers more likely to reciprocate when you offer an opportunity or ask for help.\nThe Strength of a Collaborative Culture: In healthy teams, there\u0026rsquo;s an inherent understanding that helping each other and sharing interesting work benefits everyone. Your offer reinforces this positive norm.\nUnderstanding these potential \u0026ldquo;Whys\u0026rdquo; is your key. Now, how do you translate that understanding into effectively approaching your peers?\nCrafting the Opportunity: Making the Ask Compelling Knowing the potential motivators allows you to frame your request not as an ask for help but as presenting a valuable opportunity.\nFrame Based on Mutual Benefit: Tailor your pitch. Does this task primarily offer growth? Highlight the learning potential. Is it about impact? Emphasize its strategic importance. Does it align with their known interests? Point that out. Frame it as a chance for partnership or mentorship if that fits.\nLower the Barrier to Entry: Make it easy to say \u0026ldquo;yes\u0026rdquo; (or at least \u0026ldquo;tell me more\u0026rdquo;). Do the heavy lifting: clearly define the problem, provide essential context, perhaps outline potential approaches, and have a task ready in your tracker. Reduce the initial friction and cognitive load. A well-defined, manageable scope is far less daunting than a vague, massive undertaking.\nEmpower, Don\u0026rsquo;t Micromanage: Offer challenging work that genuinely stretches capabilities. Provide clear context and constraints (the \u0026ldquo;guardrails\u0026rdquo;). Then – critically – give them the space to figure things out. Use check-ins and discussions about success criteria as support mechanisms, not control tactics. Explicitly state why you think this is a good opportunity for their development.\nBuild and Leverage Trust: This is paramount and underpins everything else when evolved. Your technical credibility and relationship capital are essential. Peers are more likely to engage when they:\nTrust the Problem \u0026amp; Direction: They believe the problem is worth solving, and your proposed path (or the space you give them to find one) is sound.\nTrust Your Motives: They believe you\u0026rsquo;re genuinely offering this for their growth and the project\u0026rsquo;s benefit, not just offloading grunt work. Your history of collaboration, giving credit, and supporting others builds this trust.\nTrust Your Support: They believe you\u0026rsquo;ll provide necessary context, answer questions, help unblock them, and shield them from related distractions.\nOf course, even the best-framed opportunity might meet a scheduling conflict. What happens when the answer is \u0026ldquo;I\u0026rsquo;m swamped, I can\u0026rsquo;t\u0026rdquo;?\nNavigating the \u0026ldquo;No\u0026rdquo; (or \u0026ldquo;Not Now\u0026rdquo;) Hearing \u0026ldquo;I don\u0026rsquo;t have bandwidth\u0026rdquo; is common and perfectly okay. How you respond matters for the long-term relationship and potential future collaboration.\nValidate \u0026amp; Understand First: Always start by acknowledging their situation.\n\u0026ldquo;Okay, totally understand you\u0026rsquo;re busy.\u0026rdquo;\nThen, gently probe to distinguish bandwidth issues from lack of interest or alignment: \u0026ldquo;Just so I understand better, is the main challenge fitting it in time-wise alongside existing priorities, or does this opportunity maybe not feel like the right fit/priority compared to what\u0026rsquo;s on your plate?\u0026rdquo;\nAdapt the Ask: If bandwidth is the only issue, can the scope be reduced? Is there a smaller, valuable piece they could contribute (e.g., brainstorming, reviewing a design, tackling one specific sub-task)?\nAlign Priorities Collaboratively (Handle with Care): If the task is strategically important, the peer is interested, but they\u0026rsquo;re blocked by conflicting priorities, this is where careful alignment is needed.\nCrucially: Get Peer Buy-in First: Never go directly to their manager without discussing it with your peer first. Doing so instantly undermines trust. Instead, frame the idea of talking to the manager together as a way to seek support for their growth and achieve important shared team/organization goals. It is also OK to talk to their manager one-on-one; just confirm that with your peer first. The same goes if your peer says they will do that. The main point is that you talked to your peer first, and you agreed to continue.\nFrame the Manager Conversation: If you do talk to the manager with your peer, focus on shared objectives. \u0026ldquo;We identified this task as crucial for [Goal X]. [Peer Name] is interested, and it seems like a great growth opportunity. Given their current commitments, how can we best align priorities to make this feasible?\u0026rdquo; This turns it into collaborative problem-solving, not top-down pressure.\nRespect Boundaries: If the answer remains \u0026ldquo;no,\u0026rdquo; accept it gracefully. Thank them for considering it. Preserving the relationship is key for future influence.\nSeek Alternatives: If it\u0026rsquo;s not feasible, consider other peers. Can you tackle a smaller version of this task yourself? Is it worth deferring (sometimes rethinking reveals it wasn\u0026rsquo;t as critical as initially thought)?\nCreating specific task-based opportunities is powerful, but scaling your impact as a senior IC involves broader strategies too.\nBroader Strategies for Scaling Your Impact Beyond direct \u0026ldquo;delegation-as-opportunity,\u0026rdquo; think about these leverage points:\nMentorship \u0026amp; Sponsorship: Formalize the growth angle. Move beyond ad-hoc tasks to explicitly mentor someone through a larger piece of work. Actively sponsor them by highlighting their contributions and advocating for their visibility.\nCreating Leverage via Platforms, Tools, \u0026amp; Frameworks: Build things that make many engineers more effective. A robust library, a streamlined CI/CD pipeline, and a well-designed service template – that scale your impact exponentially without direct task delegation. You\u0026rsquo;re essentially \u0026ldquo;delegating\u0026rdquo; efficiency and good practices.\nEstablishing Patterns and Best Practices: Defining and evangelizing clear architectural patterns, coding standards, or review processes guide the work of many, scaling your influence on quality, consistency, and maintainability. You\u0026rsquo;re shaping how work gets done.\nArchitectural Guidance and Technical Strategy: Setting a clear, well-communicated technical direction influences the choices and priorities of the entire team or organization, ensuring efforts align towards a cohesive vision.\nEffective Documentation and Knowledge Sharing: Writing clear design documents, Architecture Decision Records (ADRs), runbooks, and tutorials enable others to understand systems, contribute effectively, and operate autonomously within the technical landscape you\u0026rsquo;ve helped shape.\nTime to Try: Practical Exercises Two practical exercises to go and try.\nFind a Growth Opportunity Scan your current or upcoming workload. Identify one item that isn\u0026rsquo;t just work to be done but represents a genuine development opportunity for a specific colleague. Think about why it\u0026rsquo;s a good fit for them.\nThen, practice framing it as an opportunity, highlighting their potential gains (learning, impact, interest). Prepare the context they\u0026rsquo;d need, and be ready to offer mentorship, not micromanagement. Anticipate how you\u0026rsquo;d handle a \u0026ldquo;too busy\u0026rdquo; response using the adaptive strategies.\nPitch Strategic Alignment Identify a task (on your plate or the team\u0026rsquo;s radar) that directly supports a key team or organizational goal (e.g., an OKR, a critical roadmap item, fixing a major pain point).\nClearly articulate why this task is strategically important – quantify its impact if possible. Mentally select a relevant peer and craft a concise pitch focusing first on the task\u0026rsquo;s alignment with shared objectives and its broader impact, then on the technical details or learning aspects.\nCrucially, plan how you would suggest a collaborative discussion about priority alignment (potentially involving their manager, with their consent) if they express interest but cite conflicting priorities due to the task\u0026rsquo;s strategic value.\nAfterword: Influence is the Goal Scaling your impact as an individual contributor isn\u0026rsquo;t about wielding authority you don\u0026rsquo;t have. It\u0026rsquo;s about building influence, trust, and shared understanding. You leverage your technical credibility, strategic thinking, and relationship capital to create opportunities that are genuinely valuable for your peers while advancing collective goals.\nPeers engage not because they have to but because they want to – driven by growth, impact, interest, or trust in you and the mission.\nPrioritization becomes a conversation about value and strategic alignment, sometimes requiring collaborative discussions with managers, rather than a zero-sum game.\nYes, this takes conscious effort. It requires empathy, clear communication, and sometimes navigating tricky conversations.\nBut \u0026ldquo;this is the path\u0026rdquo; because focusing on building trust, clearly articulating the \u0026ldquo;why,\u0026rdquo; connecting tasks to strategic goals, and genuinely investing in the growth of those around you is the essence of being a force multiplier as a senior IC.\nNaturally, every team and individual is different, so consider these strategies a starting point or a toolkit to adapt to your unique context.\nBooks on this topic:\nWill Larson\u0026rsquo;s \u0026ldquo;Staff Engineer\u0026rdquo; and \u0026ldquo;An Elegant Puzzle\u0026rdquo;, Camille Fournier\u0026rsquo;s \u0026ldquo;The Manager\u0026rsquo;s Path\u0026rdquo;, \u0026ldquo;The Software Engineer\u0026rsquo;s Guidebook\u0026rdquo; by Gergely Orosz, \u0026ldquo;Drive\u0026rdquo; by Daniel Pink for concepts around influence and motivation Blogs on this and many other great things:\n\u0026ldquo;A Life Engineered\u0026rdquo; by Steve Huynh https://alifeengineered.substack.com/ \u0026ldquo;Pragmatic Engineer\u0026rdquo; by Gergely Orosz https://newsletter.pragmaticengineer.com/about ","permalink":"https://devdosvid.blog/2025/04/04/delegate-for-growth-scaling-your-impact-through-others/","summary":"\u003cp\u003eAs senior engineers, we often hit the ceiling at some point: our individual output is high, but writing more code or spinning up one more cluster does not feel like the best way to make more impact.\u003c/p\u003e\n\u003cp\u003eA more significant impact comes from multiplying our efforts. But how do you do that without a team reporting to you? How do you effectively involve peers in your projects, influencing their priorities when you don\u0026rsquo;t control their backlog or performance review?\u003c/p\u003e","title":"Delegate for Growth: Scaling Your Impact Through Others"},{"content":"Legacy architectural decisions often carry hidden costs. Here\u0026rsquo;s how questioning a storage configuration saved us a hundred thousand and taught some lessons about S3.\nThis is a technical walkthrough of identifying, analyzing, and solving two specific S3 cost optimization problems I faced recently: eliminating unnecessary data and implementing intelligent storage tiering.\nNote: All mentioned AWS prices are from January 2025 in the us-east-1 region. AWS pricing and features may have changed since the publication.\nWhat was the cause Picture this: In December 2024, two S3 buckets quietly consumed about $19,500. The price is just for one month. That\u0026rsquo;s nearly $235,000 annually, but the size of buckets slowly grows, so it\u0026rsquo;s going to be more. These buckets are for JFrog Artifactory, which supports AWS S3 as a file storage. The first bucket contains CI/CD builds and cached third-party artifacts — essential data we need daily. The second bucket is the replica in another region. This build data is important, yet paying $120,000 a year to duplicate that data seems overkill.\nNow the question is: WHY do we have such a setup with replication? As always happens, that was configured long ago, so no one knows by now.\nLet\u0026rsquo;s give this a fresh look: S3 provides 11-nines durability by design; if something gets deleted by accident, we can always re-run CI/CD and build that again. Does replication provide any value?\nThe problem statement is simple: the buckets are enormous, we pay half the price for nothing, and the amount of data might grow more.\nTwo S3 buckets: main in us-east-1 and replica in us-west-2 About 340TB each 21+ million objects per bucket Average object size: 17MB 7% monthly growth rate based on the last 12 months\u0026rsquo; trend The two-fold solution to that was not that trivial, though.\nAddressing the unpredicted S3 bucket growth and access patterns By the design of S3 service, a client (you or your application) should specify the storage class when uploading the object to an S3 bucket; there is no thing like \u0026ldquo;default storage class for new objects\u0026rdquo;.\nSurprisingly, despite a versatile configuration options for S3, JFrog Artifactory does not allow setting storage classes for the objects it stores. So, everything you store is sent to S3 with the Standard storage class.\nOn a large scale, with many teams and projects, it is pretty hard to predict the lifetime and the frequency of one or other artifact for the particular team. Auto-cleanup options are built into Artifactory, but they do not answer the retention questions anyway: How long? How frequent?\nLuckily, AWS has two powerful things to address these issues:\nIntelligent-Tiering storage class Lifecycle Configuration Benefits of Intelligent-Tiering Storage class As its name implies, Intelligent-Tiering automatically defines the best access tier when access patterns change. There are three basic tiers: one is optimized for frequent access, another for infrequent access, and a very low-cost tier, which is optimized for rarely accessed data.\nFor a relatively small price, which is negligible on scale, S3 does the monitoring and tier transition of the objects:\nMonitoring: $0.0025 per 1,000 objects/month Storage for GB/month: $0.021 — frequent tier: object stored here by default $0.0125 — infrequent tier: object moved here after 30 days without access $0.004 — archive tier: object moved here after 90 days without access The vast advantage of Intelligent-Tiering comes with the infrequent and archive tier prices on scale.\nBut note that objects smaller than 128 KB are not eligible for auto tiering.\nS3 Intelligent-Tiering Lifecycle Configuration to move S3 objects between storage classes So now the question is: how to move all the objects to Intelligent-Tiering, old and any new ones?\nHere comes Lifecycle Configuration. S3 Lifecycle also manipulates objects, but the key difference between Intelligent-Tiering and Lifecycle Configuration is that Configuration does not have the access pattern analysis as a trigger — the only trigger for Configuration is the time (e.g., trigger in N days).\nHowever, a trigger by time mark is precisely what\u0026rsquo;s needed when your software does not support the custom storage classes — you want to move objects as soon as possible to Intelligent-Tiering using Lifecycle Configuration.\nHere\u0026rsquo;s how you need to configure the Lifecycle to move all the objects to Intelligent-Tiering:\nApply to all objects in the bucket Actions: The 1st option — Transition current versions of objects between storage classes The 2nd option — Transition noncurrent versions of objects between storage classes Specify Intelligent-Tiering for \u0026ldquo;Transition current version\u0026rdquo; and \u0026ldquo;Transition noncurrent versions\u0026rdquo; Set 0 as the number of \u0026ldquo;Days after object creation\u0026rdquo; and \u0026ldquo;Days after objects become noncurrent\u0026rdquo; S3 Lifecycle Rule: Move to Intelligent-Tiering A zero value here does not mean an immediate change of the storage class: by design, the transition will be executed at midnight UTC from the object upload date. For existing objects, the transition will begin at midnight UTC after the lifecycle rule is applied.\nI confirmed with AWS technical support that there are no indications of throttling or limitations on object access during transitions. All objects remain accessible during the transition process. Lifecycle Configuration has a price of $0.01 per 1,000 Transition requests, which technically adds $0.00001 as the one-time cost of each object affected by the policy.\nFor example, in my case, moving 21 million objects to the Intelligent-Tiering class had a one-time cost of $210, but ROI for this is much more significant the next month.\nThe effect of applying Intelligent-Tiering comes in time. At least 60 days should pass to see the real difference: 30 days to trigger the first tiering relocation and then another 30 for the next month\u0026rsquo;s usage so you can compare.\nBased on Storage Lense and Storage Class Analysis statistics, I expect about 60% of the objects to remain in the archive tier, reducing the total costs of the primary S3 bucket by 48%.\nUntrivial termination of a large S3 bucket How hard can it be to empty and delete the bucket with 21 million objects inside? (To clarify: S3 does not allow deletion of a non-empty bucket)\nThe most obvious way, at first glance, would be to use the AWS S3 web Console option called \u0026ldquo;Empty\u0026rdquo;, right?\nS3 Empty Bucket Alas, it is not that simple.\nWhen you go that way, it is your browser who removes the objects by sending API calls to AWS. There is no background job for you. It does that efficiently, sending DELETE requests in 1000-item batches, but it does that as long as your IAM session remains active (or the browser window remains open, whatever ends first).\nIf your bucket has Bucket Versioning enabled, \u0026ldquo;Empty\u0026rdquo; action will not remove all the object versions.\nSo then we have two other options:\nEither run some script that removes all object versions (including current, older, and null versions) and delete markers. Or set up a couple of Lifecycle Configuration rules to purge a bucket in an unattended way. If the total number of objects is relatively small, e.g., up to few hundred thousand, it is feasible to run a script that removes all object versions and delete markers. Click here to see the code snippet #!/usr/bin/env python3 import sys import boto3 def purge_bucket_interactive(bucket_name: str) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Display the bucket ARN and region, warn the user, and prompt them to type \u0026#39;YES\u0026#39; in uppercase. If confirmed, permanently remove all object versions (including current, older, and null versions) and delete markers. \u0026#34;\u0026#34;\u0026#34; # We use the S3 client to query bucket location info s3_client = boto3.client(\u0026#34;s3\u0026#34;) response = s3_client.get_bucket_location(Bucket=bucket_name) region = response.get(\u0026#34;LocationConstraint\u0026#34;) or \u0026#34;us-east-1\u0026#34; # Construct the bucket\u0026#39;s ARN (for standard S3 buckets, the region is not typically embedded) bucket_arn = f\u0026#34;arn:aws:s3:::{bucket_name}\u0026#34; print(f\u0026#34;Bucket ARN : {bucket_arn}\u0026#34;) print(f\u0026#34;Bucket region: {region}\u0026#34;) print(\u0026#34;WARNING: This will permanently remove ALL VERSIONS of every object.\u0026#34;) print(\u0026#34;It is NOT REVERSIBLE!!!\\n\u0026#34;) confirm = input(\u0026#34;Type \u0026#39;YES\u0026#39; in uppercase to proceed with the permanent removal: \u0026#34;).strip() if confirm != \u0026#34;YES\u0026#34;: print(\u0026#34;Aborted. No changes made.\u0026#34;) return # Proceed with deletion print(\u0026#34;Starting purge... This might take a while.\u0026#34;) # Use the S3 resource to delete all versions s3 = boto3.resource(\u0026#34;s3\u0026#34;) bucket = s3.Bucket(bucket_name) bucket.object_versions.all().delete() print(\u0026#34;All object versions and delete markers have been removed.\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: if len(sys.argv) \u0026lt; 2: print(f\u0026#34;Usage: {sys.argv[0]} \u0026lt;bucket_name\u0026gt;\u0026#34;) sys.exit(1) bucket_to_purge = sys.argv[1] purge_bucket_interactive(bucket_to_purge) But if you have millions of objects, you\u0026rsquo;d better use Lifecycle Configuration rules to empty a version-enabled bucket. First, you need to pause the versioning. Then, create two Lifecycle Configuration rules.\nThe first rule will delete all versions of the objects:\nApply to all objects in the bucket Actions: The 3rd option — \u0026ldquo;Expire current versions of objects\u0026rdquo; The 4th option — \u0026ldquo;Permanently delete previous versions of objects\u0026rdquo; The number of days you would like the current version to expire, to do as soon as possible, enter 1 in the text box. That makes it \u0026ldquo;1 day\u0026rdquo;. For the days after which the noncurrent versions will be permanently deleted, enter 1 in the text box. That makes it \u0026ldquo;1 day\u0026rdquo; of being noncurrent. S3 Lifecycle Rule: Delete Versions Due to how S3 versioning works, a special DELETE marker will be created for each object processed by the first rule. To handle the delete markers, you must create a new lifecycle rule, as you won’t be able to select the option to delete the \u0026ldquo;delete markers\u0026rdquo; in the first rule.\nFor the second lifecycle rule, the steps are similar, and the only difference is that you must select the 5th and last option: \u0026ldquo;Delete expired object delete markers or incomplete multipart uploads.\u0026rdquo; Select both the options — \u0026ldquo;Delete expired object delete markers\u0026rdquo; and \u0026ldquo;Delete incomplete multipart uploads\u0026rdquo; — and set it to 1 day.\nS3 Lifecycle Rule: Delete Markers S3 Lifecycle operations are asynchronous, and it may take some time for the Lifecycle to delete the objects in your S3 bucket. However, at midnight UTC, once the S3 objects are marked for expiration, you are no longer charged for storing that objects, and S3 will do the rest. Implementation Results Immediate cost reduction: $120,000 baseline or $270,000 if the 7% growth rate remains Projected additional cost reduction from Intelligent-Tiering: 48% — means the drop of the baseline price of the main bucket from $120,000 to roughly $63,000 a year One-time transition cost: $210 Implementation time: 2 days Storage class transition completed in 1 day The final $63,000 versus $235,000 is a nice result. Combining the data from Intelligent-Tiering with built-in Artifactory retention policies can reduce that even more. However, it would require some custom logic wrapped around.\nKey Takeaways Challenge your architecture decisions overtime. Challenge redundancy — is it providing real value? Regular cost reviews can be a source of quick wins. Let automated solutions do the job, and do not overengineer things. AWS Simple Storage Service, despite its name, might be tricky, but AWS has a bunch of tools to help you. S3 Lifecycle rules proved their worth twice: first by automating our transition to Intelligent-Tiering despite Artifactory\u0026rsquo;s limitations and then by cleaning up millions of versioned objects without operational overhead.\n","permalink":"https://devdosvid.blog/2025/01/29/aws-s3-cost-optimization-removing-redundancy-and-implementing-intelligent-tiering/","summary":"Legacy setups can hide costs, and sometimes big. By challenging this, I learned some cool stuff about S3 Intelligent-Tiering and Lifecycle Configuration.","title":"Aws S3 Cost Optimization: Removing Redundancy and Implementing Intelligent Tiering"},{"content":"In this blog post, I’m excited to share some valuable system design insights drawn from AWS’s post-event summaries on major outages.\nSystem design is a topic I’m especially passionate about—it’s even my favorite interview question at Grammarly, and I love to conduct those interviews. This fascination led me to thoroughly analyze AWS’s PES in search of the most interesting cases.\nLearning from mistakes is essential. Yet it\u0026rsquo;s precious to learn from others\u0026rsquo; mistakes because they come for free (for you, but not for their owners). The AWS team is doing a great job sharing their Post-Event Summaries because they not only demonstrate the open engineering culture but also help others.\nFrom the sixteen reports available at the time of writing, I’ve selected the four most captivating ones. Each presents unexpected challenges and turns of events, along with valuable outcomes we can learn from. Let\u0026rsquo;s explore these intriguing reports and uncover the key strategies for building more resilient systems.\nRemirroring Storm The April 21, 2011, Amazon EC2/EBS event1 in the US East Region provides valuable insights into dependency management and the dangers of cascading failures.\nA network configuration change, promoted as a part of normal scaling activity, set off a cascade of failures in the Amazon Elastic Block Store system. The intention was simple — to upgrade the capacity of the primary network. This operation involved a traffic shift between underlying networks, but it was executed incorrectly so that change caused many EBS nodes to disconnect from their replicas. When these nodes reconnected, they tried to replicate their data on other nodes, quickly overwhelming the EBS cluster’s capacity. This surge, or as AWS called it “remirroring storm,” left many EBS volumes “stuck,” unable to process read and write operations.\nBlast Radius: Initially, the issue affected only a single Availability Zone in the US East Region, and about 13% of the volumes were in this “stuck” state. However, the outage soon spread to the entire region. The EBS control plane, responsible for handling API requests, was dependent on the degraded EBS cluster. The increased traffic from the remirroring storm overwhelmed the control plane, making it intermittently unavailable and affecting users across the region.\nAffected Services and Processes:\nEC2: Users faced difficulties launching new EBS-backed instances and managing existing ones. EBS: Many volumes became “stuck,” rendering them inaccessible and impacting EC2 instances dependent on them. RDS: As a service dependent on EBS, some RDS databases, particularly single-AZ deployments, became inaccessible due to the EBS volume issues. This incident underscores the importance of building resilient systems. The EBS control plane’s dependence on a single Availability Zone and the absence of back-off mechanisms in the remirroring process were critical factors in the cascading failures.\nElectrical Storm The June 29, 2012, AWS services event2 in the US East Region exemplifies how a localized power outage can trigger a region-wide service disruption due to complex dependencies.\nA severe electrical storm caused a power outage in a single data center, impacting a small portion of AWS resources in the US East Region — a single-digit percentage of the total resources in the region (as of the date of the incident).\nBlast Radius: The power outage was initially confined to the affected Availability Zone. However, it soon led to degrading service control planes that manage resources across the entire region. While these control planes aren\u0026rsquo;t required for ongoing resource usage, their degradation hindered users\u0026rsquo; ability to respond to the outage, such using the AWS console to try moving resources to other Availability Zones.\nAffected Services and Processes:\nEC2 and EBS: Approximately 7% of EC2 instances and a similar proportion of EBS volumes in the region were offline until power was restored and systems restarted. The control planes for both services were significantly impacted, making it difficult for users to launch new instances, create EBS volumes, or attach volumes in any Availability Zone within the region. ELB: Although the direct impact was limited to ELBs within the affected data center, the service\u0026rsquo;s inability to process new requests quickly hampered recovery for users trying to replace lost EC2 capacity in other Availability Zones. RDS: Many Single-AZ databases in the affected zone became inaccessible due to their dependent EBS volumes being affected. A few Multi-AZ RDS instances, designed for redundancy, also failed to failover automatically due to a software bug triggered by the specific server shutdown sequences during the power outage. Even though we host our applications in the cloud and power outages may not be a primary concern when starting new projects, this event underscores the critical importance of designing fault-tolerant systems. It also highlights the potential for cascading failures when a small percentage of infrastructure is impacted. Dependencies on control planes and the interconnected nature of services can significantly amplify the impact of localized outages.\nSimple Point of Failure Another excellent example of how small can quickly become big — is the Amazon SimpleDB service disruption on June 13, 2014, in the US East Region3. This incident demonstrates how a seemingly minor issue can escalate into a significant disruption due to dependencies on a centralized service.\nA power outage in a single data center caused multiple storage nodes to become unavailable. This sudden failure led to a spike in load on the internal lock service, which manages node responsibility for data and metadata. And while this lock services is replicated accross multiple data centers, the load spike wat too sudden and too high.\nBlast Radius: Initially, the impact was confined to the storage nodes in the affected data center. However, the increased load on the centralized lock service, crucial for all SimpleDB operations, caused cascading failures that affected the entire service.\nAffected Services and Processes:\nSimpleDB: The service became unavailable for all API calls, except for a small fraction of eventually consistent read calls, because the storage and metadata nodes couldn’t renew their membership with the overloaded lock service. This unavailability prevented users from accessing and managing their data. This outage highlights the critical importance of addressing the single point of failure when designing systems. The centralized nature of the lock service, intended for coordination, became a single point of failure. A more distributed or load-balanced approach for the lock service could have mitigated the impact of the simultaneous node failures.\nScaling for the better In search of absolute The Amazon Kinesis event in the US East Region on November 25, 20204, is a perfect example of how adding capacity can unexpectedly trigger a cascade of failures due to unforeseen dependencies and resource limitations. What could possibly go wrong by adding more nodes to the cluster? Scaling horizontally is best practice, right? Well, it depends.\nA small capacity addition to the front-end fleet of the Kinesis service led to unexpected behavior in the front-end servers responsible for routing requests.\nThese newly added servers exceeded the maximum allowed threads due to operating system configuration: each front-end server creates operating system threads for each of the other servers in the front-end fleet — this is needed for services to learn about new servers added to the cluster. Eventually, all this caused cache construction failures and prevented servers from routing requests to the back-end clusters.\nBlast Radius: Although the initial trigger was a capacity addition intended to enhance performance, the resulting issue affected the entire Kinesis service in the US East Region. The dependency of many AWS services on Kinesis amplified the impact significantly.\nAffected Services and Processes:\nKinesis: Customers experienced failures and increased latencies when putting and getting Kinesis records, rendering the service unusable for real-time data processing. Cognito: As a dependent service on Kinesis, Cognito faced elevated API failures and increased latencies for user pools and identity pools. This disruption prevented external users from authenticating or obtaining temporary AWS credentials. CloudWatch: Kinesis Data Streams are used by CloudWatch to process metrics and log data. The event caused increased error rates and latencies for CloudWatch APIs (PutMetricData and PutLogEvents), with alarms transitioning to an INSUFFICIENT_DATA state, hindering monitoring and alerting capabilities. Auto Scaling and Lambda: These services rely on CloudWatch metrics, so they were indirectly affected, too. Reactive Auto Scaling policies experienced delays, and Lambda function invocations encountered increased error rates due to memory contention caused by backlogged CloudWatch metric data. This incident highlights the importance of thoroughly understanding dependencies, resource limitations, and potential failure points when making changes to a system, even those intended to improve capacity or performance. Robust testing and monitoring are crucial to identify and mitigate such unexpected behaviors.\nFinal Thoughts The AWS outage reports reveal a fundamental truth about system design: complexity and interdependency can be both our greatest strengths and our most significant vulnerabilities.\nAs we build and scale our systems, let\u0026rsquo;s strive for simplicity, resilience, and a thorough understanding of our dependencies.\nTo ensure our systems are prepared for the unexpected, it\u0026rsquo;s crucial to internalize and act upon the lessons these incidents teach us. Here are some key takeaways to guide us in this:\nEmbrace Resilience: Design systems with robust fault-tolerance mechanisms to handle unexpected failures without cascading effects. Understand Dependencies: Map out and regularly review your system\u0026rsquo;s dependencies to identify and mitigate potential single points of failure. Continuous Learning: Analyze past incidents, both your own and industry-wide, to gain insights and improve your system design. Proactive Monitoring: Implement comprehensive monitoring and alerting to detect and address issues before they escalate. Thorough Testing: Regularly test your systems under various failure scenarios to ensure they can withstand real-world conditions. The path to reliable systems is paved with continuous learning and adaptation. Let\u0026rsquo;s embrace these lessons and push the boundaries of what our systems can achieve, ensuring that we are always prepared for the unexpected. 🙂\nSummary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region, April 21, 2011\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSummary of the AWS Service Event in the US East Region, June 29, 2012\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSummary of the Amazon SimpleDB Service Disruption, June 13, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSummary of the Amazon Kinesis Event in the Northern Virginia (US-EAST-1) Region, November 25, 2020\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://devdosvid.blog/2024/06/03/unpacking-aws-outages-system-design-lessons-from-post-event-summaries/","summary":"Explore AWS outage case studies, uncovering essential strategies for building resilient systems by understanding dependencies and preventing cascading failures","title":"Unpacking AWS Outages: System Design Lessons from Post-Event Summaries"},{"content":"Many teams employ Terraform by HashiCorp to efficiently manage their infrastructure, leveraging its ability to automate the lifecycle of complex environments. Yet, integrating security scanning into Terraform pipelines often remains overlooked, exposing these environments to potential security risks and compliance issues.\nThis article explores several prominent static code analyzers that support Terraform code and focus on its security scanning. This comparison will guide teams in choosing the right tool to enhance their security measures within Terraform workflows, ensuring safer and more compliant infrastructure management.\nHere are the tools we\u0026rsquo;ll be reviewing: KICS, tfsec, Trivy, Terrascan, Checkov, and Semgrep OSS.\nWhile many of these tools also support other platforms and technologies, this review will concentrate exclusively on their functionality with Terraform.\nWhy use Static Code Analysis for Terraform Static code analysis tools are necessary to enhance the security of Terraform-managed infrastructures. Unlike linters, these tools focus not on syntax errors or coding style but delve deeply into the code to identify security vulnerabilities and potential compliance issues without running the actual code. This proactive approach to security helps safeguard the infrastructure from potential threats before deployment.\nKey Benefits Early Detection: Identifies security vulnerabilities and misconfigurations early in development, preventing them from reaching production. Compliance Assurance: Ensures Terraform code complies with industry standards and internal security policies. Automated Security Integration: Seamlessly integrates with CI/CD pipelines, automating security checks to maintain a continuous focus on security. Actionable Insights: Delivers detailed vulnerability reports, facilitating swift and effective resolution. Scalability: Effectively handles increasing project complexity and size, maintaining rigorous security standards without additional manual effort. Expected Features Policy Coverage: The tool should offer comprehensive scanning capabilities to detect security vulnerabilities specific to Infrastructure as Code. Customizable Security Policies: It must allow users to define and adjust security policies and severity levels to align with specific project needs or compliance requirements. Seamless Integration: The analyzer should integrate effortlessly with existing CI/CD tools and version control systems, facilitating a smooth workflow. Detailed Reporting: Clear and actionable reports are crucial. The tool should prioritize issues based on severity and provide practical steps for remediation. Scanning Customization: Users should be able to tailor the scanning process to focus on particular aspects of the codebase, enabling targeted and efficient security assessments. With a clear understanding of the necessary features in a static code analyzer, which tools on the market best fulfill these criteria?\nLet\u0026rsquo;s take a closer look at some leading options!\nMeet the Static Code Analyzers for Terraform Following on what makes a static code analyzer robust, let\u0026rsquo;s dive into some open-source tools that exemplify these essential features.\nI picked six tools for my review. I know there are more on the market, but I focused on open-source, free-to-use tools and those that provide at least \u0026gt;100 out-of-the-box scanning policies for Terraform.\nKICS (stands for \u0026ldquo;Keeping Infrastructure as Code Secure\u0026rdquo;): Owner/Maintainer: Checkmarx\nAge: First released on GitHub on November 30th, 2020\nLicense: Apache License 2.0\ntfsec\nOwner/Maintainer: Aqua Security (acquired in 2021)\nAge: First released on GitHub on March 5th, 2019\nLicense: MIT License tfsec project is no longer actively maintained in favor of the Trivy tool. But because many people still use it and it\u0026rsquo;s quite famous, I added tfsec to this comparison.\nHowever, I recommend against using it for new projects. Trivy\nOwner/Maintainer: Aqua Security\nAge: First released on GitHub on May 7th, 2019\nLicense: Apache License 2.0\nbackward-compatible with tfsec\nTerrascan\nOwner/Maintainer: Tenable (acquired in 2022)\nAge: First release on GitHub on November 28th, 2017\nLicense: Apache License 2.0\nCheckov\nOwner/Maintainer: Prisma Cloud by Palo Alto Networks (acquired in 2021)\nAge: First released on GitHub on March 31st, 2021\nLicense: Apache License 2.0\nSemgrep OSS\nOwner/Maintainer: Semgrep\nAge: First release on GitHub on February 6th, 2020\nLicense: GNU Lesser General Public License v2.1\nThese tools are essential in enhancing Terraform\u0026rsquo;s security posture and reflect a strong collaboration between open-source communities and enterprise backing. This blend ensures that the tools are not only accessible but also robustly maintained and up-to-date.\nLet’s explore how these tools stack up regarding features and usability.\nComparing Out-of-the-Box Policies and Terraform Providers Understanding the number and variety of default policies each tool offers is crucial for those just beginning to explore security automation for Terraform.\nThe extent of out-of-the-box policies can significantly ease the integration process of static analysis by providing immediate and comprehensive insights into potential security and compliance issues. Similarly, the number of supported Terraform Providers also plays a critical role.\nIn this chapter, we delve into these foundational features across observed tools, helping you pinpoint which one could best satisfy your requirements for robust, ready-to-use security scanning.\nTool Policies Supported Terraform Providers KICS 663 aws, azure, gcp, kubernetes, alicloud, databricks, github, nifcloud tfsec 154 aws, azure, gcp, digitalocean, kubernetes, cloudstack, github, openstack, oracle Trivy 322 aws, azure, gcp, digitalocean, cloudstack, github, oracle, openstack Terrascan 790 aws, azure, gcp, digitalocean, kubernetes, docker, github Checkov 2110 aws, azure, gcp, digitalocean, kubernetes, github, gitlab, ibm, linode, openstack, alicloud Semgrep OSS 362 aws, azure, gcp As you can see, all tools support the \u0026ldquo;Big Three\u0026rdquo; cloud service Terraform providers—AWS, Azure, and GCP—for managing resources on these popular platforms.\nWith over 2000 out-of-the-box policies, Checkov significantly stands out from the competition. This tool also leads in the total number of supported Terraform providers.\nWhile the default policies provide a strong foundation for security scanning, the ability to tailor these policies is just as crucial. Next, we\u0026rsquo;ll explore how each tool accommodates custom policy capabilities, allowing you to fine-tune the policies to fit your project\u0026rsquo;s specific requirements.\nCustom Policy Capabilities Default policies serve as the foundation, but the nuances of each project demand the extension of this base.\nHere, we delve into how each tool enables you to add custom policies, thus enhancing and refining the provided defaults.\nWhile all six tools support adding custom policies to their default set, they differ in terminology: \u0026lsquo;policy\u0026rsquo; is the common term, whereas KICS refers to them as \u0026lsquo;queries,\u0026rsquo; and Semgrep calls them \u0026lsquo;rules.\u0026rsquo;\nRegarding policy syntax:\nOPA Rego syntax is used by KICS, Trivy, tfsec, and Terrascan. It\u0026rsquo;s a powerful language widely adopted in the industry, though there\u0026rsquo;s a learning curve that could pay dividends for future projects.\nYAML syntax is used by Checkov and Semgrep. This offers a familiar and straightforward start, with Checkov also allowing policies to be written in Python, albeit with some constraints. With YAML, the ease of use is balanced against the limitations set by the tool\u0026rsquo;s capabilities.\nUnderstanding these differences will guide you to a tool that matches your security requirements, your team\u0026rsquo;s expertise, and the scope of your infrastructure projects.\nTo illustrate, here is an example of a KICS Rego policy checking for default RDS instance ports:\npackage Cx import data.generic.common as common_lib import data.generic.terraform as tf_lib CxPolicy[result] { db := input.document[i].resource.aws_db_instance[name] enginePort := common_lib.engines[e] db.engine == e db.port == enginePort result := { \u0026#34;documentId\u0026#34;: input.document[i].id, \u0026#34;resourceType\u0026#34;: \u0026#34;aws_db_instance\u0026#34;, \u0026#34;resourceName\u0026#34;: tf_lib.get_resource_name(db, name), \u0026#34;searchKey\u0026#34;: sprintf(\u0026#34;aws_db_instance[%s].port\u0026#34;, [name]), \u0026#34;issueType\u0026#34;: \u0026#34;IncorrectValue\u0026#34;, \u0026#34;keyExpectedValue\u0026#34;: sprintf(\u0026#34;aws_db_instance[%s].port should not be set to %d\u0026#34;, [name, enginePort]), \u0026#34;keyActualValue\u0026#34;: sprintf(\u0026#34;aws_db_instance[%s].port is set to %d\u0026#34;, [name, enginePort]), \u0026#34;searchLine\u0026#34;: common_lib.build_search_line([\u0026#34;resource\u0026#34;, \u0026#34;aws_db_instance\u0026#34;, name, \u0026#34;port\u0026#34;], []), } } And an example of a Checkov YAML policy forbidding specific EC2 instance types:\n--- metadata: name: \u0026#34;Org\u0026#39;s compute instances should not be p5.48xlarge or p4d.24xlarge\u0026#34; id: \u0026#34;ACME_AWS_FORBIDDEN_EC2_TYPES\u0026#34; category: \u0026#34;NETWORKING\u0026#34; definition: or: - cond_type: \u0026#34;attribute\u0026#34; resource_types: - \u0026#34;aws_instance\u0026#34; attribute: \u0026#34;instance_type\u0026#34; operator: \u0026#34;not_equals\u0026#34; value: \u0026#34;p5.48xlarge\u0026#34; - cond_type: \u0026#34;attribute\u0026#34; resource_types: - \u0026#34;aws_instance\u0026#34; attribute: \u0026#34;instance_type\u0026#34; operator: \u0026#34;not_equals\u0026#34; value: \u0026#34;p4d.24xlarge\u0026#34; With the ability to tailor policies to our specific needs, we\u0026rsquo;ll next explore each tool\u0026rsquo;s capacity to integrate broadly, determining how well they play with the rest of our tech stack.\nIntegration Capabilities Integration capabilities are the cornerstone of efficient DevOps practices.\nThis section will evaluate how each static code analyzer enhances your tech stack through seamless integration with other systems and technologies.\nWe will assess each tool against four key integration points that are vital for development workflows:\nDocker Image: Ensures easy deployment across any container-supported environment. IDE Plugins: Facilitates real-time feedback and improves code quality directly within the developer\u0026rsquo;s workspace. CI/CD Systems: Supports direct integration through plugins or extensions, eliminating the need for manual downloads or CLI setups. Pre-commit Hook: Provides an early security checkpoint by scanning code before it is committed, catching errors at the initial stages. Tool Docker Image IDE Plugins CI/CD Systems Hook KICS ✅ VSCode Github Actions, GitLab, Terraform Cloud, Codefresh ✅ tfsec ✅ VSCode, JetBrains, Vim Github Actions ❌ Trivy ✅ VSCode, JetBrains, Vim Azure DevOps, GitHub Actions, Buildkite, Dagger, Semaphore, CircleCI, Concourse CI ❌ Terrascan ✅ VSCode GitHub Actions, Atlantis ✅ Checkov ✅ VSCode, JetBrains GitHub Actions, GitLab ✅ Semgrep OSS ✅ VSCode, JetBrains, Emacs, Vim GitLab ✅ In addition to the table above, here are a few noteworthy features of some tools:\nCheckov supports OpenAI integration to suggest remediations. But be careful because AI tends to hallucinate. KICS supports applying auto-remediation for some of its out-of-the-box policies. This also applies to custom policies, where you can define remediations and apply them automatically. Terrascan is the only one that provides the VSCode extension to create and test custom policies written in Rego. GitLab uses KICS as its default built-in IaC scanner — available out of the box with \u0026ldquo;Infrastructure as Code scanning\u0026rdquo;. However, there\u0026rsquo;s also GitLab CI Component available for Checkov. Having covered the integration capabilities, let’s now focus on the output formats each tool provides.\nOutput Formats Provided Output formats extend the utility of static code analysis, facilitating integration with the CI/CD feedback loop and enabling its use as an artifact in subsequent CI jobs.\nThis chapter examines the variety of formats each tool supports for this purpose.\nEach tool offers a range of output formats tailored to different needs.\nFor GitLab users: For teams leveraging GitLab\u0026rsquo;s security scanning, KICS, Checkov, and Semgrep OSS are equipped with compatible output formats, facilitating smooth GitLab integration.\nFor GitHub users: SARIF\u0026rsquo;s adoption as an industry standard, particularly by GitHub for code scanning, makes it a must-have. All tools assessed offer SARIF support, ensuring interoperability and broad utility.\nJUnit Reports: The availability of JUnit output is crucial for capturing test results in a format recognizable by various CI systems. Trivy, Terrascan, Checkov, and Semgrep OSS support this, enabling clear visualization of test outcomes and enhancing the feedback loop within CI pipelines.\nBeyond these, each tool supports additional formats, enriching their application and versatility. Here\u0026rsquo;s the full breakdown of the output formats, complementing the standard CLI output:\nTool Supported Output Formats KICS ASFF, CSV, Code Climate, CycloneDX, GItLab SAST, HTML, JSON, JUnit, PDF, SARIF, SonarQube tfsec Checkstyle, CSV, HTML, JSON, JUnit, Markdown, SARIF Trivy ASFF, Cosign, CycloneDX, JSON, SARIF, SPDX Terrascan JSON, JUnit, SARIF, XML, YAML Checkov CSV, CycloneDX, GItLab SAST, JSON, JUnit, SARIF, SPDX Semgrep OSS Emacs, GitLab SAST, JSON, JUnit, SARIF, Vim Moving from output formats to operational adaptability, let\u0026rsquo;s investigate the customization options for scanner settings. This important feature allows each tool to align with varied project demands.\nCustomizing Scanner Settings This chapter moves beyond the default scanner settings and delves into scanner settings\u0026rsquo; customizability, ensuring that tools can be calibrated for any development environment or security requirement.\nI will evaluate each tool against criteria that define a tool\u0026rsquo;s adaptability and user-friendliness:\nTargeted Scans: Select specific directories for scanning or exclusion to focus on pertinent areas and skip irrelevant ones. In-Code Ignore Policies: Enable ignore directives within code to skip checks when exceptions apply selectively. Severity Thresholds: Set reporting to include only findings above a chosen severity level, concentrating on the most impactful issues. Configuration File: Employ configuration files for consistency and collaboration, enabling a \u0026lsquo;configuration as code\u0026rsquo; approach. TF Variables Interpolation: Interpret and evaluate Terraform variables for an accurate security assessment of IaC. Module Scanning: For complete coverage, scans should include both local and remote (public/private) Terraform modules. Based on these criteria, the following table offers a comparative view of how each tool performs, giving you a clear snapshot of their customization capabilities:\nTool Targeted Scans Ignore Policies Min Severity Config File Variables Interpolation Module Scanning KICS ✅ ✅ ✅ ✅ ✅ ⁉️️ tfsec ✅ ✅ ✅ ✅ ✅ ✅ Trivy ✅ ✅ ✅ ✅ ✅ ✅ Terrascan ✅ ✅ ✅ ✅ ✅ ✅ Checkov ✅ ✅ ✅ ✅ ✅ ✅ Semgrep OSS ✅ ✅ ✅ ❌ ✅ ❌ Most reviewed tools meet nearly all the criteria set for scanner setting customization, demonstrating their flexibility and advanced capabilities. However, there are notable features worth considering:\nKICS: Provides limited module scanning capabilities, restricted to some public modules from the Terraform registry, and does not cover local or private custom modules. Terrascan \u0026amp; Trivy: Both feature server modes that centralize vulnerability databases. This centralization facilitates a unified approach to applying policies and configurations, enhancing consistency and efficiency for teams and reducing the management overhead of diverse policies across multiple projects. Semgrep: It doesn\u0026rsquo;t support scanner configuration files; instead, it uses the \u0026ldquo;config\u0026rdquo; word to call the rule sets and accepts such configs. Notably, it also does not support the scanning of Terraform modules at all. Terraform Security Scanning: The Big Picture and Top Pick Here\u0026rsquo;s a comprehensive comparison summary to guide your selection of the most suitable Terraform static code analyzer:\nTool Default Policies Custom Policies Integration Output Formats Customization KICS 663 OPA Rego ✅Docker, ✅IDE, ✅CI/CD, ✅Git Hook ASFF, CSV, Code Climate, CycloneDX, GItLab SAST, HTML, JSON, JUnit, PDF, SARIF, SonarQube ✅Targeted Scans, ✅Ignore Policies, ✅Min Severity, ✅Config File, ✅Variables Interpolation, ❌Module Scanning tfsec 154 OPA Rego ✅Docker, ✅IDE, ✅CI/CD, ❌Git Hook Checkstyle, CSV, HTML, JSON, JUnit, Markdown, SARIF ✅Targeted Scans, ✅Ignore Policies, ✅Min Severity, ✅Config File, ✅Variables Interpolation, ✅Module Scanning Trivy 322 OPA Rego ✅Docker, ✅IDE, ✅CI/CD, ❌Git Hook ASFF, Cosign, CycloneDX, JSON, SARIF, SPDX ✅Targeted Scans, ✅Ignore Policies, ✅Min Severity, ✅Config File, ✅Variables Interpolation, ✅Module Scanning Terrascan 790 OPA Rego ✅Docker, ✅IDE, ✅CI/CD, ✅Git Hook JSON, JUnit, SARIF, XML, YAML ✅Targeted Scans, ✅Ignore Policies, ✅Min Severity, ✅Config File, ✅Variables Interpolation, ✅Module Scanning Checkov 2110 YAML, Python ✅Docker, ✅IDE, ✅CI/CD, ✅Git Hook CSV, CycloneDX, GItLab SAST, JSON, JUnit, SARIF, SPDX ✅Targeted Scans, ✅Ignore Policies, ✅Min Severity, ✅Config File, ✅Variables Interpolation, ✅Module Scanning Semgrep OSS 362 YAML ✅Docker, ✅IDE, ✅CI/CD, ✅Git Hook Emacs, GitLab SAST, JSON, JUnit, SARIF, Vim ✅Targeted Scans, ✅Ignore Policies, ✅Min Severity, ❌Config File, ✅Variables Interpolation, ❌Module Scanning Integrating a Terraform security scanning into your development pipeline is a proven strategy to boost your security posture. These tools detect potential vulnerabilities early and enforce best practices and compliance standards, representing a proactive approach to infrastructure security.\nFor teams not yet utilizing these tools, Checkov is my top recommendation:\nBiggest number of default policies and supported Terraform providers for a quick start. Custom policy support in YAML and Python for flexible policy creation. Wide integration options with Docker, IDEs, CI/CD systems, and Git Hooks for a smooth workflow. Please share your favorite tool in the comments below! Also, let me know if I missed a cool product that should have been included in the review.\n","permalink":"https://devdosvid.blog/2024/04/16/a-deep-dive-into-terraform-static-code-analysis-tools-features-and-comparisons/","summary":"Explore key features and comparisons of top Terraform static code analysis tools to enhance security and compliance in your infrastructure management.","title":"A Deep Dive Into Terraform Static Code Analysis Tools: Features and Comparisons"},{"content":"With a solid foundation in AWS API Gateway and Lambda for serverless architecture, my recent deep dive into these cloud computing services felt like uncovering new layers in familiar territory. This article aims to be a comprehensive guide for developers and DevOps professionals looking to master serverless solutions using AWS and Terraform.\nThe article provides an in-depth guide to combining AWS API Gateway V2 HTTP API (yes, this is the official name of that service 😄) and AWS Lambda services to implement a simple, robust, and cost-effective serverless back-end using Terraform.\nThe journey was enlightening and engaging, especially as I were transforming these services into Infrastructure as Code. Through this article, I aim to share those moments of insight and the practical, hands-on tips that emerged from weaving these AWS services into a seamless, serverless architecture.\nNavigating the System Design: HTTP API Gateway and Lambda in Action Beginning our journey, we examine the complexities of serverless architecture, focusing on HTTP API and Lambda. A comprehensive system diagram will guide us as we analyze each component\u0026rsquo;s function and their collaborative roles in the larger infrastructure.\nHTTP API Gateway and AWS Lambda flowchart In our architecture, the HTTP API delegates access control to the Lambda function called \u0026ldquo;Authorizer\u0026rdquo;. This function stands as the gatekeeper, ensuring that only legitimate requests pass through to the underlying business logic.\nThe HTTP API can have multiple routes (e.g., \u0026ldquo;/calendar,\u0026rdquo; \u0026ldquo;/meters,\u0026rdquo; and so on) and use different Authorizers per route or a single one for all of them. Clients that send their requests to the API must include specific identification information in their request header or query string. In this project, I go with a single authorizer to keep it simple.\nUpon receiving a request, the API service forwards a payload to the Authorizer containing metadata about the request, such as headers and query string components. The Authorizer processes this metadata (headers, in my case) to determine the request\u0026rsquo;s legitimacy.\nThe decision, Allow or Deny, is passed back to the API, and if allowed, the API service then forwards the original request to the back-end, which, in this case, is implemented by additional Lambda functions. Otherwise, the client gets a response with a 403 status code, and the original request is not passed to the back-end.\nSubscribe to blog updates! Behind The Decision: Why Such a Setup? Choosing the right architectural setup is critical in balancing simplicity, cost-efficiency, and security. In this section, we uncover why integrating AWS HTTP API Gateway with Lambda Authorizer is a compelling choice, offering a streamlined approach without compromising security.\nCost-Effectiveness: Balancing Performance and Price The AWS HTTP API is noteworthy for its streamlined and simple design compared to other API Gateway options. That translates directly into cost savings for businesses. Its efficiency makes it an ideal choice for cost-effective serverless computing, especially for those looking to optimize their cloud infrastructure with Terraform automation. Here is a more detailed comparison of different API Gateway options — Cost optimization.\nSecurity with Lambda Authorizer. This option means a Lambda function used for authorization, which is lean and efficient. It generally requires a bare minimum of resources. It executes quickly, particularly when configured with the ARM-based environment and 128M RAM allocation, costing $0,0000017 per second of running time, with $0.20 per 1M requests per month.\n💰 This pricing and performance combination are well-suited for rapid, lightweight authorizations. Together with AWS Lambda as a back-end, it makes a cost-effective solution. For example, if we add a few more Lambdas to back-end and assume that our setup receives 10000 requests per month, it would cost around $0.6 per month. Here is the link to detailed calculations — AWS Pricing Calculator.\nSimplicity in Configuration: The Power of Header-Based Authorization A header-based authentication method facilitates straightforward client-server communication, often requiring less coding and resources to implement compared to more complex schemes.\nAlthough HTTP API offers stronger JWT-based authorization and mutual TLS authentication, header-based authorization remains a suitable choice for simpler applications that prioritize ease and quickness. By the way, there is also an option for IAM-based authorization whose core idea is the \u0026ldquo;private API\u0026rdquo; or internal usage of the API (e.g., solely inside the VPC, no internet), but with \u0026ldquo;IAM Anywhere,\u0026rdquo; this can be expanded to practically anywhere. 😁\nThis architecture suits applications requiring rapid development and deployment without complex authorization mechanisms. It\u0026rsquo;s ideal for small to medium-sized serverless applications or specific use cases in larger systems where quick, cost-effective, and secure access to APIs is a priority.\n💡 Imagine a retail company wanting to manage its inventory efficiently. By leveraging AWS API Gateway and Lambda, they can develop a system where each item\u0026rsquo;s RFID tags are scanned and processed through an API endpoint. When a product is moved or sold, its status is updated in real-time in the database, facilitated by Lambda functions. This serverless architecture ensures high availability and scalability and significantly reduces operational costs, a crucial factor for the highly competitive retail industry. This example showcases how our serverless setup can be effectively utilized in retail for streamlined inventory tracking and management.\nExploring AWS Lambda: Features and Integration Diving into AWS Lambda, this section explores its features and indispensable role within the serverless infrastructure. We will unravel the complexities of Lambda functions and examine the practicalities of deploying and managing these functions within the project.\nAWS Lambda Runtime and Deployment Model 🚀 Choosing the AWS Lambda runtime arm64, combined with the OS-only runtime based on Amazon Linux 2023, strategically boosts cost efficiency and performance. This choice aligns with the best practices for serverless computing in AWS, offering an optimal solution for those seeking to leverage AWS services for scalable cloud solutions.\nParticularly effective for Go-based functions, this runtime configuration is lean yet powerful. For applications in other languages, delving into language-specific runtimes based on AL 2023 can also leverage the latest efficiencies of AWS-managed operating systems.\nI also welcome you to read this benchmarking analysis to get more insights about the ARM-based environment for AWS Lambda — Comparing AWS Lambda Arm vs. x86 Performance, Cost, and Analysis. The .zip deployment model is chosen for its simplicity, avoiding additional management of the image registry (ECR) and Docker images. Also, AWS automatically patches .zip functions for the latest runtime security and bug fixes.\nEfficient Terraform Coding for AWS Lambda In our architecture, AWS Lambda functions serve dual purposes — as an authentication gatekeeper and a robust back-end for business logic. Despite varying code across functions, their configurations share much of similarities.\nBy adhering to the DRY (Don\u0026rsquo;t Repeat Yourself) principle, I have crafted a Terraform module to streamline the management of Lambda functions and their dependencies. This approach ensures maintainable and scalable infrastructure. The module\u0026rsquo;s structure is as follows:\naws_lambda_function — to describe the core configuration of the function aws_iam_role + aws_iam_role_policy + aws_iam_policy_document — to manage the access from Lambda to other resources (e.g., SSM Parameter Store) aws_cloudwatch_log_group — to keep the execution logs aws_ssm_parameter — to store sensitive information (e.g., secrets) and other configurations that we should keep separate from the source code. This Terraform module implements a project-specific use case for Lambda functions. However, if you\u0026rsquo;re seeking for a generic all-in-one module for AWS Lambda, I recommend checking out this one — Terraform AWS Lambda Module by Anton Babenko.\nTo efficiently develop Terraform code for Lambda functions, use the following techniques:\nUse local values, expressions, and variables to implement consistent naming across different resources logically grouped by a module or project; Use function environment variables to connect the code with SSM Parameter Store parameters or Secrets Manager secrets to protect sensitive data like tokens or credentials; Use for_each meta-argument and for expression to reduce the amount of code and automate the configuration for resources of the same type (e.g., ssm_parameter) or code blocks within a resource. Below is a practical example illustrating these Terraform strategies in action:\nlocals { full_function_name = \u0026#34;${var.project_name}-${var.function_name}\u0026#34; } resource \u0026#34;aws_lambda_function\u0026#34; \u0026#34;this\u0026#34; { function_name = local.full_function_name role = aws_iam_role.this.arn architectures = [\u0026#34;arm64\u0026#34;] filename = var.deployment_file package_type = \u0026#34;Zip\u0026#34; runtime = \u0026#34;provided.al2023\u0026#34; handler = \u0026#34;bootstrap.handler\u0026#34; timeout = var.function_timeout environment { variables = { for item in var.function_ssm_parameter_names : upper(replace(item, \u0026#34;-\u0026#34;, \u0026#34;_\u0026#34;)) =\u0026gt; aws_ssm_parameter.function_ssm_parameters[item].name } } } resource \u0026#34;aws_ssm_parameter\u0026#34; \u0026#34;function_ssm_parameters\u0026#34; { for_each = var.function_ssm_parameter_names name = \u0026#34;/projects/${var.project_name}/lambda/${var.function_name}/${each.value}\u0026#34; type = \u0026#34;SecureString\u0026#34; key_id = data.aws_kms_alias.ssm.arn value = \u0026#34;1\u0026#34; lifecycle { ignore_changes = [ value, ] } } resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;this\u0026#34; { name = \u0026#34;/aws/lambda/${local.full_function_name}\u0026#34; log_group_class = \u0026#34;STANDARD\u0026#34; retention_in_days = 7 } The complete terraform module code is available in the project repository.\nIn this Terraform code, I deliberately hardcoded specific arguments for an optimal Lambda runtime configuration, ensuring efficiency and performance.\nThen variables and local values, set only once, implement a naming convention for all resource arguments, making it easy to understand the infrastructure and change the naming and attributes later.\nLambda\u0026rsquo;s environment variables and corresponding SSM parameters coexist effectively with the help of for_each and for. I used the for_each meta-argument to dynamically create SSM Parameter resources and the for expression to configure environment variables in AWS Lambda. This also means that if the function_ssm_parameter_names variable value is not provided, then Terraform does not create either SSM parameter resources or the environment code block inside the Lambda resource because the default value of that variable is an empty set. By the way, I have another blog post that explains several techniques to enhance your Terraform proficiency — check it out!\nInvoking Lambda: Permissions and Resource-Based Policies Configured with just a few input variables, the Terraform module efficiently outputs the aws_lambda_function resource. This streamlined output is then adeptly used to facilitate subsequent configurations within the HTTP API.\nmodule \u0026#34;lambda_api_gw_authorizer\u0026#34; { source = \u0026#34;./modules/lambda\u0026#34; deployment_file = \u0026#34;../backend/lambda-apigw-authorizer/deployment.zip\u0026#34; function_name = \u0026#34;api-gateway-authorizer\u0026#34; project_name = local.project_name function_ssm_parameters = [ \u0026#34;authorization-token\u0026#34; ] } module \u0026#34;lambda_calendar_backend\u0026#34; { source = \u0026#34;./modules/lambda\u0026#34; deployment_file = \u0026#34;../backend/lambda-calendar-backend/deployment.zip\u0026#34; function_name = \u0026#34;calendar-backend\u0026#34; project_name = local.project_name function_ssm_parameters = [ \u0026#34;google-api-oauth-token\u0026#34;, \u0026#34;google-api-credentials\u0026#34; ] } As an example of module output usage, here is the configuration of aws_lambda_permissions resource that I use outside the AWS Lambda module to allow the HTTP API service to invoke the function used as Authorizer:\nresource \u0026#34;aws_lambda_permission\u0026#34; \u0026#34;allow_api_gw_invoke_authorizer\u0026#34; { statement_id = \u0026#34;allowInvokeFromAPIGatewayAuthorizer\u0026#34; action = \u0026#34;lambda:InvokeFunction\u0026#34; function_name = module.lambda_api_gw_authorizer.lambda.function_name principal = \u0026#34;apigateway.amazonaws.com\u0026#34; source_arn = \u0026#34;${aws_apigatewayv2_api.this.execution_arn}/authorizers/${aws_apigatewayv2_authorizer.header_based_authorizer.id}\u0026#34; } The Lambda resource-based policy combines the trust and permission policies, and provides a simple yet efficient way to grant other AWS services or principals the ability to invoke Lambda functions. It is important to note that for an API to invoke a function, Lambda requires its execution ARN, not the resource ARN. As a side note, check out this AWS Lambda Operator Guide, which offers specialized advice on developing, securing, and monitoring applications based on AWS Lambda.\nLet\u0026rsquo;s switch to the HTTP API part to see how it looks and learn how it integrates Lambda functions.\nDeep Dive into HTTP API Gateway Now, we focus on the HTTP API Gateway, delving into its essential concepts, seamless integration with AWS Lambda, and using Terraform efficiently for streamlined configuration.\nBut before we do that, and since we have partially covered the Terraform code already, I\u0026rsquo;d like to illustrate the logical connection between three main components of the project\u0026rsquo;s Terraform codebase: AWS Lambda, HTTP API, and API Routes.\nAWS Lambda and HTTP API Terraform integration diagram I will explain the API Route module in detail a bit later, but for now, for the broader context, here is what happens inside Terraform: AWS HTTP API code logically represents the \u0026ldquo;global\u0026rdquo; (within a project) set of resources and uses the function created by the Lambda Terraform module for the Authorizer configuration. Meanwhile, the API Route Terraform module configures specific routes for the HTTP API (hence, requires some info from it) with integration to back-ends implemented by Lambdas (hence, requires some info from them, too).\nBack to HTTP API overview. The following components of the HTTP API constitute its backbone:\nRoute — a combination of the HTTP method (e.g., GET or POST) with the API route (e.g., /meters). For example: \u0026ldquo;POST /meters\u0026rdquo;. Routes can optionally use Authorizers — a mechanism to control access to the HTTP API. Integration — the technical and logical connection between the Route and one of the supported back-end resources. For example, with AWS Lambda integration, API Gateway sends the entire request as input to a back-end Lambda function and then transforms the Lambda function output to a front-end HTTP response. Stage and Deployment — A stage serves as a designated reference to a deployment, essentially capturing a snapshot of the API at a certain point. It\u0026rsquo;s employed to control and optimize a specific deployment version. For instance, stage configurations can be adjusted to tailor request throttling, set up logging, or establish stage variables to be used by API (if needed). Implementing AWS API Gateway V2 HTTP API with Terraform Below, I detail the Terraform resources essential for implementing the HTTP API, ensuring a transparent and effective setup:\naws_apigatewayv2_api — the HTTP API itself; aws_apigatewayv2_route — the Route for the API that must specify the integration target (e.g., Lambda) and, optionally, the Authorizer; aws_apigatewayv2_authorizer — the Authorizer to use for Routes; aws_apigatewayv2_integration — the resource that specifies the back-end where the API sends the requests (e.g., AWS Lambda); aws_lambda_permission — the resource-based policy for AWS Lambda to allow the invocations from the API; aws_apigatewayv2_stage — the name of the Stage that references the Deployment. Applying Terraform for HTTP API Gateway and Lambda Authorizer The HTTP API is the simplest in the API Gateway family (so far), so its Terraform resource has relatively few configuration options, most of which can be left at their default values.\nAs for the Authorizer, it can have two options for letting API know its decision: simple response and IAM policy.\nThe simple response just returns a Boolean value to indicate whether the API should allow the request (True) or forbid it (False).\nThe IAM policy option is customizable and allows crafting custom policy statements that allow granular access to explicitly provided resources.\nIn this project, I follow the way of simplicity and use the \u0026ldquo;simple response\u0026rdquo;, so the response from Lambda Authorizer to HTTP API looks as follows:\n{ \u0026#34;isAuthorized\u0026#34;: true/false } Let\u0026rsquo;s review the HTTP API resource along with the API Authorizer that I used for all routes:\nresource \u0026#34;aws_apigatewayv2_api\u0026#34; \u0026#34;this\u0026#34; { name = local.project_name protocol_type = \u0026#34;HTTP\u0026#34; } resource \u0026#34;aws_apigatewayv2_authorizer\u0026#34; \u0026#34;header_based_authorizer\u0026#34; { api_id = aws_apigatewayv2_api.this.id authorizer_type = \u0026#34;REQUEST\u0026#34; name = \u0026#34;header-based-authorizer\u0026#34; authorizer_payload_format_version = \u0026#34;2.0\u0026#34; authorizer_uri = module.lambda_api_gw_authorizer.lambda.invoke_arn enable_simple_responses = true identity_sources = [\u0026#34;$request.header.authorization\u0026#34;] authorizer_result_ttl_in_seconds = 3600 } resource \u0026#34;aws_lambda_permission\u0026#34; \u0026#34;allow_api_gw_invoke_authorizer\u0026#34; { statement_id = \u0026#34;allowInvokeFromAPIGatewayAuthorizer\u0026#34; action = \u0026#34;lambda:InvokeFunction\u0026#34; function_name = module.lambda_api_gw_authorizer.lambda.function_name principal = \u0026#34;apigateway.amazonaws.com\u0026#34; source_arn = \u0026#34;${aws_apigatewayv2_api.this.execution_arn}/authorizers/${aws_apigatewayv2_authorizer.header_based_authorizer.id}\u0026#34; } The complete code is available in the project repository.\nConsider the following key points when Terraforming this part.\nidentity_sources argument of the aws_apigatewayv2_authorizer resource: This is where I defined what exactly the Authorizer should validate. I used the header named authorization so the Authorizer Lambda function would check its value to decide whether to authorize the request.\n💡 Check out other options available to use as the identity source — Identity sources.\nauthorizer_uri argument of the aws_apigatewayv2_authorizer resource: It is the invocation ARN of the Lambda function used as Authorizer (not the Lambda\u0026rsquo;s resource ARN).\nauthorizer_result_ttl_in_seconds argument of the aws_apigatewayv2_authorizer resource: This allows to skip the Authorizer invocation for the given time if a client provided the same identity source values (e.g., authorization header).\nAWS API Gateway HTTP can employ the identity sources as the cache key to preserve the authorization results for a while. Should a client provide identical parameters in identity sources within the preset TTL duration, API Gateway will retrieve the result from the cached authorizer instead of calling upon it again. This helps save a lot on AWS Lambda Authorizer invocations and works great with simple scenarios. However, it might be cumbersome if you need severral custom authorization responses per function or if you use custom IAM policies instead of the \u0026ldquo;simple response\u0026rdquo; option. source_arn argument of aws_lambda_permission: Similar to the authorizer_uri argument, this one expects the execution ARN of the HTTP API followed by the Authorizer identifier.\nNow, let\u0026rsquo;s see how Routes are codified with Terraform.\nApplying Terraform for HTTP API Routes 💡 Because an API typically has multiple routes, creating another Terraform module that implements the configurable HTTP API Gateway route is beneficial. Hence, the aws_apigatewayv2_route, aws_apigatewayv2_integration, and aws_lambda_permission resources would constitute such a module.\nThis Terraform module implements a specific use case for HTTP API Gateway. However, if you\u0026rsquo;re seeking for a generic all-in-one module for API Gateway, I recommend checking out this one — Terraform AWS API Gateway Module by Anton Babenko.\nresource \u0026#34;aws_apigatewayv2_route\u0026#34; \u0026#34;this\u0026#34; { api_id = var.api_id route_key = var.route_key authorization_type = \u0026#34;CUSTOM\u0026#34; authorizer_id = var.authorizer_id target = \u0026#34;integrations/${aws_apigatewayv2_integration.this.id}\u0026#34; } resource \u0026#34;aws_apigatewayv2_integration\u0026#34; \u0026#34;this\u0026#34; { api_id = var.api_id integration_type = \u0026#34;AWS_PROXY\u0026#34; connection_type = \u0026#34;INTERNET\u0026#34; integration_uri = var.lambda_invocation_arn payload_format_version = \u0026#34;2.0\u0026#34; } resource \u0026#34;aws_lambda_permission\u0026#34; \u0026#34;this\u0026#34; { statement_id = \u0026#34;allowInvokeFromAPIGatewayRoute\u0026#34; action = \u0026#34;lambda:InvokeFunction\u0026#34; function_name = var.lambda_function_name principal = \u0026#34;apigateway.amazonaws.com\u0026#34; source_arn = \u0026#34;${var.api_gw_execution_arn}/*/*/*/*\u0026#34; } First, I want to highlight several key aspects for understanding the resources\u0026rsquo; arguments within that module.\nThe target argument of the aws_apigatewayv2_route resource implies that the integration ID should be prefixed with the \u0026ldquo;integrations/\u0026rdquo; keyword.\nWhile the connection_type argument of the aws_apigatewayv2_integration resource specifies \u0026ldquo;INTERNET\u0026rdquo;, it does not mean that the Lambda function must have the publicly available URL. This value must be used unless you work with a VPC endpoint for API Gateway for internal usage.\nFor the source_arn argument in the aws_lambda_permission resource, similar to earlier, it requires the execution ARN of the API. However, this time, it is the integration of the HTTP API Route with Lambda. And the ARN format of this one is different and a bit tricky:\narn:partition:execute-api:region:account-id:api-id/stage/http-method/resource-path\nThe arn:partition:execute-api:region:account-id:api-id part constitutes the execution ARN of the HTTP API itself, so for the sake of simplicity, I decided to go with wildcards after it.\nFor your convenience, here is the detailed specification of API Gateway ARNs.\nThe HTTP API Route module expects several input variables:\nauthorizer_id — the identifier of the Authorizer to use on this route; route_key — the route key for the route, e.g., GET /foo/bar; api_id — the identifier of HTTP API created earlier; lambda_invocation_arn — the Invocation ARN of the Lambda function; lambda_function_name — the name of the Lambda function to integrate with the route; api_gw_execution_arn — the Execution ARN of the HTTP API that invokes a Lambda function. Let\u0026rsquo;s take a closer look on API Gateway V2 HTTP API route.\nA route consists of an HTTP method and a resource path with an optional variable. Based on the pre-defined convention, it uses a simplified routing configuration and methods request model (comparable to other APIs).\nWhile I was working with the HTTP API, I found this simplified approach to be great because it allows easy access to the request context from AWS Lambda functions, for example:\nA path variable in a route, e.g., GET /calendars/{calendar-name}, would be available for the integrated AWS Lambda by its name inside the pathParameter JSON field, e.g., pathParamters.calendar-name, of the event object sent by API to Lambda. In other words, you do not need to explicitly set the mapping between the path variable and its representation to the back-end. A request query string is parsed into separate parameter-value pairs and available in the queryStringParameters field of the event object sent by API to Lambda. Again, without the explicit mapping configuration. Here, you can read more about the Route specification of HTTP API and how to transform requests and responses from the API side if you need to adjust something:\nWorking with routes for HTTP APIs Transforming API requests and responses Now back to Terraform. Below is the code snippet that illustrates the call of the API Route Terraform module:\nmodule \u0026#34;route_calendars\u0026#34; { source = \u0026#34;./modules/api-gateway-route\u0026#34; api_id = aws_apigatewayv2_api.this.id route_key = \u0026#34;GET /calendars/{calendar-name}\u0026#34; api_gw_execution_arn = aws_apigatewayv2_api.this.execution_arn lambda_invocation_arn = module.lambda_calendar_backend.lambda.invoke_arn lambda_function_name = module.lambda_calendar_backend.lambda.function_name authorizer_id = aws_apigatewayv2_authorizer.header_based_authorizer.id } This module logically relies on both the HTTP API and Lambda resources to configure their integration by implementing the Route.\nEnhancing Security and Monitoring of AWS API Gateway V2 HTTP API Several additional options are available to monitor and protect the HTTP API: logs, metrics, and throttling.\nOverview of HTTP API monitoring and protection options Logging, metrics, and throttling are configured on the Stage level but allow configuration granularity for the Routes.\nFor logs, you can configure the CloudWatch log group, the log format (JSON, CLF, XML, CSV), and content filters. The logging variables allow you to customize the information that appears in logs. I will provide an example of such a configuration later in the article.\nBy default, API Gateway sends only API and stage-level metrics to CloudWatch in one-minute periods. However, you can enable detailed metrics and additionally collect the per-route metrics.\nTo safeguard your HTTP API from excessive requests, you can employ throttling settings, which allow you to set limits per individual route as well as for all routes collectively.\nConfiguring monitoring and protection for HTTP API with Terraform Now, let\u0026rsquo;s see how Terraform helps configure the protection and monitoring for HTTP API.\nAs mentioned earlier, API Gateway applies these configurations at the Stage level, which is why the aws_apigatewayv2_stage resource encapsulates them all.\nresource \u0026#34;aws_apigatewayv2_stage\u0026#34; \u0026#34;default\u0026#34; { api_id = aws_apigatewayv2_api.this.id name = \u0026#34;$default\u0026#34; auto_deploy = true description = \u0026#34;Default stage (i.e., Production mode)\u0026#34; default_route_settings { throttling_burst_limit = 1 throttling_rate_limit = 1 } access_log_settings { destination_arn = aws_cloudwatch_log_group.api_gateway_logs_inkyframe.arn format = jsonencode({ authorizerError = \u0026#34;$context.authorizer.error\u0026#34;, identitySourceIP = \u0026#34;$context.identity.sourceIp\u0026#34;, integrationError = \u0026#34;$context.integration.error\u0026#34;, integrationErrorMessage = \u0026#34;$context.integration.errorMessage\u0026#34; integrationLatency = \u0026#34;$context.integration.latency\u0026#34;, integrationRequestId = \u0026#34;$context.integration.requestId\u0026#34;, integrationStatus = \u0026#34;$context.integration.integrationStatus\u0026#34;, integrationStatusCode = \u0026#34;$context.integration.status\u0026#34;, requestErrorMessage = \u0026#34;$context.error.message\u0026#34;, requestErrorMessageString = \u0026#34;$context.error.messageString\u0026#34;, requestId = \u0026#34;$context.requestId\u0026#34;, routeKey = \u0026#34;$context.routeKey\u0026#34;, }) } } Here, I applied the default throttling settings: for my project, 1 request per second was enough at that point.\n🤔 There is a nuance, though, that makes Terraforming API Gateway a little inconvenient — the IAM role that allows API to write logs must be defined on a region level. Therefore, if you maintain several Terraform projects for the same AWS account, you might need to have the following configuration stand separately to avoid conflicts or misunderstandings:\nresource \u0026#34;aws_api_gateway_account\u0026#34; \u0026#34;this\u0026#34; { cloudwatch_role_arn = aws_iam_role.api_gateway_cloudwatch_logs.arn } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;api_gateway_cloudwatch_logs\u0026#34; { name = \u0026#34;api-gateway-cloudwatch-logs\u0026#34; assume_role_policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [ { Effect = \u0026#34;Allow\u0026#34; Principal = { Service = \u0026#34;apigateway.amazonaws.com\u0026#34; } Action = \u0026#34;sts:AssumeRole\u0026#34; } ] }) managed_policy_arns = [\u0026#34;arn:aws:iam::aws:policy/service-role/AmazonAPIGatewayPushToCloudWatchLogs\u0026#34;] } resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;api_gateway_logs_inkyframe\u0026#34; { name = \u0026#34;/aws/apigateway/inkyframe\u0026#34; log_group_class = \u0026#34;STANDARD\u0026#34; retention_in_days = 7 } And one more thing about HTTP API deployments and stages. I use the special $default keyword to have a single stage (hence, the default one), and I also used automatic deployments: with any change made to API configuration, AWS will automatically generate a new Deployment and bound it with the Stage. If you prefer controlling deployments manually, there is a special resource exists that implements this — aws_apigatewayv2_deployment\nresource \u0026#34;aws_apigatewayv2_deployment\u0026#34; \u0026#34;example\u0026#34; { api_id = aws_apigatewayv2_api.example.id description = \u0026#34;Example deployment\u0026#34; triggers = { redeployment = sha1(join(\u0026#34;,\u0026#34;, tolist([ jsonencode(aws_apigatewayv2_integration.example), jsonencode(aws_apigatewayv2_route.example), ]))) } lifecycle { create_before_destroy = true } } In that case, the aws_apigatewayv2_stage resource requires the deployment_id argument to link itself with a particular Deployment and, therefore, represent the state of the API configuration.\nAlso, API Gateway requires at least one configured API Route before the deployment is initiated/created. However, these resources do not explicitly depend on each other via attribute references. To avoid the race condition in Terraform, you need to reference the Route resource in the aws_apigatewayv2_deployment resource via the triggers argument (as shown above) or via the depends_on meta-argument. Otherwise, Terraform will try to apply changes to both resources simultaneously.\nAfterword: Simplifying Serverless Architectures In wrapping up our exploration of AWS HTTP API Gateway, AWS Lambda, and Terraform, we\u0026rsquo;ve delved into how these powerful tools work in tandem to streamline and enhance serverless architectures. This article aimed to combine my experience with new knowledge and demystify the complexities of used services, showcasing their capabilities in creating efficient, cost-effective solutions for modern cloud-based applications.\nWe focused on practical implementation and the tangible benefits of combining these technologies. By leveraging Terraform, we\u0026rsquo;ve seen how infrastructure management can be simplified, allowing for clearer, more maintainable code. The combination of AWS Lambda and HTTP API Gateway has demonstrated the efficiency of serverless computing, offering scalability and performance without the burden of extensive configuration and management.\nThis exploration underlines the importance of choosing the right tools and strategies in cloud computing. It reminds developers and architects that creating robust and efficient serverless systems is within reach with a thoughtful approach and the right set of tools. As the cloud landscape continues to evolve, staying informed and adaptable is key to harnessing the full potential of these technologies. 💚\n","permalink":"https://devdosvid.blog/2024/01/09/mastering-aws-api-gateway-v2-http-and-aws-lambda-with-terraform/","summary":"The article provides insights into using AWS API Gateway and AWS Lambda with Terraform for efficient, cost-effective serverless solutions.","title":"Mastering AWS API Gateway V2 HTTP and AWS Lambda With Terraform"},{"content":"Terraform version 1.4 was recently released and brought a range of new features, including improved run output in Terraform Cloud, the ability to use OPA policy results in the CLI, and a built-in alternative to the null resource — terraform_data.\nIn this blog post, I want to demonstrate and explain the terraform_data resource that serves two purposes:\nfirstly, it allows arbitrary values to be stored and used afterward to implement lifecycle triggers of other resources secondly, it can be used to trigger provisioners when there isn\u0026rsquo;t a more appropriate managed resource available. Subscribe to blog updates! For those of you, who are familiar with the null provider, the terraform_data resource might look very similar. And you\u0026rsquo;re right!\nRather than being a separate provider, the terraform_data managed resource now offers the same capabilities as an integrated feature. Pretty cool! While the null provider is still available and there are no statements about its deprecation thus far (as of April 2023, v3.2.1), the terraform_data is the native replacement of the null_resource, and the latter might soon become deprecated.\nThe terraform_data resource has two optional arguments, input and triggers_replace, and its configuration looks as follows:\nterraform data resource arguments The input (optional) stores the value that is passed to the resource The triggers_replace (optional) is a value that triggers resource replacement when changes. There are two attributes, in addition to the arguments, which are stored in the state: id and output after the resource is created. Let\u0026rsquo;s take a look:\nterraform data resource attributes The output attribute is computed based on the value of the input The id is just a unique value of the resource instance in the state (as for any other resource). Use case for terraform_data with replace_triggered_by Let\u0026rsquo;s take a look at the first use case for the terraform_data resource. It is the ability to trigger resource replacement based on the value of the input argument.\nA bit of context here: the replace_triggered_by argument of the resource lifecycle meta-argument allows you to trigger resource replacement based on another referenced resource or its attribute.\nIf you are not yet familiar with the replace_triggered_by, you can check another blog post that explains it. The replace_triggered_by is a powerful feature, but here is the thing about it: only a resource or its attribute must be specified, and you cannot use a variable or a local value for replace_triggered_by.\nBut with terraform_data, you can indirectly initiate another resource replacement by using either a variable or an expression within a local value for the input argument.\nLet me give you an example here. Consider the following code:\ntrigger replacement based on input variable By modifying the revision variable, the next Terraform plan will suggest a replacement action against aws_instance.webserver:\nterraform_data with replace_triggered_by Use case for terraform_data with provisioner Before we start: HashiCorp suggests (and I also support that) avoiding provisioner usage unless you have no other options left. One of the reasons — additional, implicit, and unobvious dependency that appears in the codebase — the binary, which is called inside the provisioner block, must be present on the machine. But let\u0026rsquo;s be real, the provisioner feature is still kicking, and there\u0026rsquo;s always that one unique project that needs it.\nHere is the code snippet that demonstrates the usage of the provisioner within the terraform_data resource:\nterraform_data with provisioner In this example, the following happens:\nWhen resources are created the first time, the provisioner inside terraform_data runs Sequential plan/apply will trigger another execution of the provisioner only when the private IP of the instance (aws_instance.webserver.private_ip) changes because that will trigger terraform_data recreation. At the same time, no changes to the internal IP mean no provisioner execution. With its ability to store and use values for lifecycle triggers and provisioners, terraform_data is a powerful tool that can enhance your Terraform configuration.\nAlthough the null provider still has its place in the Terraform ecosystem, terraform_data is its evolution, and its integration as a feature is certainly something to be excited about.\nWhy not give it a try in your next project and see how it can simplify your infrastructure as code workflows? Keep on coding! 🙌\n","permalink":"https://devdosvid.blog/2023/04/16/hello-terraform-data-goodbye-null-resource/","summary":"Native built-in replacement for null_resource with Terraform 1.4","title":"Hello Terraform Data; Goodbye Null Resource"},{"content":"EC2 AMI catalog consists of more than 160k public AMIs — a mix of shared AMIs created by users, published by vendors, and provided by AWS.\nSo how to ensure that an AMI comes from the verified vendor or that is an official AMI published by AWS?\nHow to find the trusted AMI among them all when you\u0026rsquo;re about to launch an EC2 Instance?\nFind the Waldo verified AMI owner On AWS, it\u0026rsquo;s typical that something can be made or done in several ways — that\u0026rsquo;s awesome. Some of them work better than others, some methods are official, and some you can use just for fun (check that).\nIn this article, I will describe five ways of getting the official and verified AMI for your next EC2 Instance launch.\nUse EC2 Launch Wizard and AMI Catalog to get the official AMI When launching an EC2 Instance from a Management Console, you can apply the \u0026ldquo;Verified Provider\u0026rdquo; filter for the Community AMIs tab to ensure you get an AMI from a verified provider. The \u0026ldquo;Verified provider\u0026rdquo; label means an AMI is owned by an Amazon verified account.\nIn the following example, I want to make sure that the Ubuntu 20.04 AMI comes from the verified source:\nVerified AMI in the AMI Catalog In the past, you had to compare the AMI Owner ID with the publicly shared list of verified Owner IDs for every region. Not rocket science, but it takes time. So now it\u0026rsquo;s much more straightforward, thanks to the \u0026ldquo;Verified Provider\u0026rdquo; label.\nThis feature also works great when you are creating a Launch Template. The Launch Template creation wizard seamlessly guides you from itself to the AMI Catalog (where you can search and pick the AMI) and back again.\nLook for verified AMIs on the AMI page Another interface in the Management Console acts as the AMI browser. It does not have any fancy name except for the \u0026ldquo;AMIs page\u0026rdquo;, but you probably already know about it: it looks like a list of AMIs, and you can see it when you click on the \u0026ldquo;AMIs\u0026rdquo; menu item on the left side of the EC2 page menu.\nThe AMI page allows you to leverage the API filters to narrow down the search, and the \u0026ldquo;Owner alias\u0026rdquo; filter is the one you need to ensure that an AMI comes from a trusted owner.\nHere is how it looks for my search of the official Amazon Linux 2 AMI:\nOfficial Amazon Linux 2 AMI AMIs shared by verified sources have amazon (for AWS) or aws-marketplace (for AWS partners) as the value for the Owner alias filter.\nFind the EC2 AMI with Terraform Finding the official AMI with Terraform is also simple — the aws_ami data source does the job.\nFor example, here is how you can find the same Amazon Linux 2 AMI by specifying the amazon as the value for the owner argument of the data source:\nFinding the official Amazon Linux 2 AMI with Terraform Compare that with the filters on the AMI page — it looks similar, right? This is because of how Terraform works: it translates your code into API calls and sends them to AWS API endpoints.\nIf you\u0026rsquo;re very new to Terraform, I suggest reading this article to understand the basic concepts of Terraform and Infrastructure as Code: Terraform explained in English\nFind the official AWS AMI using Describe Images CLI Sometimes you might need to get the AMI from CLI to pass it along as an argument downstream of the pipeline.\nThis can be done with the ec2 describe-images command of the AWS CLI Find the verified AMI with AWS CLI The API filters I mentioned before also work here — use them to narrow your search.\nFind the trusted AWS AMI with SSM Another way that involves AWS CLI is the ssm get-parameter command: Get the latest EKS optimized AMI from SSM It reveals one helpful feature of the Systems Manager — the Public parameters.\nSystems Manager Public parameters are how AWS distributes some widely used artifacts related to their services.\nFor example, you can find official AMIs for many distributives there: Amazon Linux, Windows, macOS, Bottlerocket, Ubuntu, Debian, and FreeBSD.\nRead more at the Finding public parameters documentation page if you want to know more.\nAre all verified AMIs good? The \u0026ldquo;Verified provider\u0026rdquo; badge can be earned by a third party only when an AMI developer is registered as a Seller on the AWS Marketplace.\nBecoming a Seller there is not trivial and requires some conditions to be met, and the registration process itself also implies submitting the tax and banking information.\nAdditionally, there are specific policies and review processes apply to all AMIs submitted to the Marketplace.\nSo it is okay to trust the third-party vendors with the \u0026ldquo;Verified\u0026rdquo; badge on a certain level. However, it is also always good to have additional scans and validation of the software you use. 🪲 😉\n","permalink":"https://devdosvid.blog/2022/07/24/five-practical-ways-to-get-the-verified-ec2-ami/","summary":"How to find the AMI you cat trust among thousands available in AWS","title":"Five Practical Ways To Get The Verified EC2 AMI"},{"content":"Many of us know and use Packer to build golden images for cloud providers. But did you know that Packer is not just a CLI tool?\nThere is an HCP (stands for HashiCorp Cloud Platform) Packer that acts as the image registry that stores the image metadata, allows you to organize images in distribution channels, and perform other management actions.\nIn this blog, I would like to showcase some features of the HCP Packer and explain how you can use it to set up an image factory for the organization (or for your own fun 🙃) to maintain the Golden Images.\nI will be using AWS AMI as the OS image appliance for examples in this blog, but Packer supports many other formats and clouds through its plugins.\nHCP Packer Registry HCP Packer is the image metadata registry that stores the information (not an image file) about OS images you create using the Packer CLI tool.\nIt solves the challenge of the Golden Image pipeline maintenance by acting as a hub that organizes and streamlines the processes of OS image creation, usage, and continuity.\nOS Image lifecycle with HCP Packer HCP Packer introduces several new concepts that compose the registry: Image Buckets, Iterations, and Channels. Further in this blog, I will explain them, but let\u0026rsquo;s start with security first.\nSecurity First — Creating Service Principals Before launching the builds, you need to create a Service Principal to allow your local Packer CLI to communicate with the HCP.\nI recommend creating at least two principals: the one with the \u0026ldquo;contributor\u0026rdquo; role — used by Packer CLI to store the image metadata in HCP; and another one with the \u0026ldquo;viewer\u0026rdquo; role — used by Terraform (as it requires only read-level access for Packer HCP).\nService Principals for HCP Once you have created a principal, you can generate a key for authentication. The key consists of an ID and a secret.\nBoth the Packer CLI and the Packer Terraform provider support environment variables for the principal client ID and client secret for authentication:\nHCP_CLIENT_ID and HCP_CLIENT_SECRET\nImage Buckets to Store Image Metadata The central entity in HCP Packer is the Image Buckets.\nImage Bucket is a repository where the metadata from a Packer template is stored once image(s) creation is completed.\nImage Bucket can contain a single image or several images if you define several sources for the build block in the Packet template.\nFor example, a bucket can span several custom AMIs based on Ubuntu AMI provided by Amazon and built and distributed within several regions.\nYou cannot create buckets manually from the web interface (at least as of June 2022), but I will show you how they are defined as code inside a Packer template file just a bit later.\nHCP Packer Image Buckets Iterations of Image Creation Every execution of the build action made by Packer CLI (if used in conjunction with HCP) is recorded specially and called Iteration.\nEach Iteration has a unique fingerprint — an SHA value of the head reference in the Git repository that contains your Packer template.\nTip: you can override that with the HCP_PACKER_BUILD_FINGERPRINT env variable if you want to set the Iteration ID manually. HCP Packer Iterations Every Iteration consists of at least one Build — another special record that contains image metadata produced by Packer CLI.\nThe Builds inside Iteration are represented by the number of sources specified in your Packer template\u0026rsquo;s build section.\nHCP Packer Iteration Builds Packer Template Configuration for HCP Let\u0026rsquo;s now review a code example to understand how all this combines.\nHere is a build block from Packer template file.\nHCP Packer registry usage in a Packer template Look at the hcp_packer_registry block: it defines the Bucket where Packer will store image information and custom labels for the Bucket and the image.\nThe bucket_name defines my Image Bucket: Packer will either use the existing Bucket with that name or create a new one if it does not exist.\nThe bucket_labels map defines custom labels you specify for an Image Bucket. In my example, I set the Bucket owner and the OS name.\nThe build_labels map defines custom labels for the Builds within the Iteration inside a bucket.\nAnd because I define two sources here, my Iteration will have two Builds inside it.\nUsing Channels Although all Iterations have unique identifiers, giving a familiar name to some of them would be more convenient.\nChannel is a way to assign a specific Iteration to a friendly name that you can use later:\nin other Packer templates, if you want to use your custom image as the base for other images in Terraform code (we will review this further) to reference the image by the channel name, avoiding the hard code of the image ID. Channels are created through the web interface or using the API. And I hope HashiCorp will add HCP Packer resources to the HCP Terraform provider in the future so channel creation can be described as code.\nHCP Packer Image Channel You can manually promote an Iteration to a channel with a web interface.\nBut before promoting an Iteration to a channel, you might want to perform the following:\ntest and validate the newly created image before its promotion to a channel: create a temporary virtual machine using Terraform and ensure it successfully boots from the image.\nassess that VM with some vulnerability scanning service. For example, if you\u0026rsquo;re an AWS customer, then Amazon Inspector might work for you in such a case.\nOnce an image from the Iteration is validated and passed the security assessment, it\u0026rsquo;s safe to promote that Iteration to a channel.\nHCP Packer provides a rich API that you can leverage to automate that process.\nWhen a packer build successfully finishes its execution, it returns the Iteration ID (ULID) that you can use later for an API call with a request to promote the new Iteration to a channel.\nPacker build output with Iteration ULID The \u0026ldquo;Update Channel\u0026rdquo; PATCH API method is needed to assign the Iteration to a channel.\nFirst, you need to obtain the access token as described in this guide.\nThen, the following cURL request can be used to update the channel with a new Iteration ULID (please expand the code snippet below):\nClick here to see the code snippet HCP_ACCESS_TOKEN=\u0026#34;your token here\u0026#34; HCP_ORG_ID=\u0026#34;your org id here\u0026#34; HCP_PROJECT_ID=\u0026#34;your project id here\u0026#34; HCP_BUCKET_NAME=\u0026#34;amazon-linux2\u0026#34; HCP_CHANNEL_NAME=\u0026#34;stable\u0026#34; HCP_BASE_URL=\u0026#34;https://api.cloud.hashicorp.com/packer/2021-04-30\u0026#34; curl -X PATCH \\ --header \u0026#34;Authorization: Bearer $HCP_ACCESS_TOKEN\u0026#34; \\ --data \u0026#39;{ \u0026#34;incremental_version\u0026#34;:\u0026#34;3\u0026#34;, \u0026#34;iteration_id\u0026#34;:\u0026#34;01H8V7WBDWRBCMZDZ2HG3MKSDL\u0026#34; }\u0026#39; \\ \u0026#34;${HCP_BASE_URL}/organizations/${HCP_ORG_ID}/projects/${HCP_PROJECT_ID}/images/${HCP_BUCKET_NAME}/channels/${HCP_CHANNEL_NAME}\u0026#34; Using HCP Packer with Terraform Having a streamlined golden image creation process is good. Still, it would be even better to have an easy way to always use the latest validated image without hardcoding or some other duck taping.\nWith the help of the HCP Terraform provider, you can reference the image channel in your Terraform code and have a completed end-to-end workflow.\nHere is an example of the Terraform configuration that uses the HCP Packer registry as the source of AMI ID for an AWS instance:\nUsing HCP Packer registry with Terraform Two data sources do all the magic here.\nThe hcp_packer_iteration data source gets the most recent Iteration assigned to the specified Channel (i.e., latest). We need that because the Iteration (not the Channel) holds the image information.\nThen the hcp_packer_image gets the cloud image ID (AWS AMI ID in my example) from that Iteration so you can use it later in your code.\nThe configuration of the hcp provider in this example is empty on purpose: this provider supports HCP_CLIENT_ID and HCP_CLIENT_SECRET env variables to use their values for the authentication and avoid hard coding. Alternatively, you can use the client_id and client_secret options to configure the provider.\nImage revocation It is possible to revoke a specific Iteration, and therefore all Images in it, to alert the users about the Image decommission. For example, your SecOps team can revoke it due to the new CVE announced.\nRevoked Images are treated differently by Packer CLI and Terraform CLI.\nPacker CLI and Revoked Image When you reference the Image in a Packer template to use it as a source for another image, its revocation makes further Packer builds to fail.\nIn other words, Packer won\u0026rsquo;t let you build a new Image on top of the revoked Image.\nTerraform CLI and Revoked Image On the contrary, Terraform CLI does not prevent the usage of the revoked Image by default, although its Cloud version does it if used with the \u0026ldquo;Run tasks\u0026rdquo; feature.\nAlthough you can get the Image ID, when the Iteration is revoked, the hcp_packer_image data source returns a non-empty revoke_at attribute with the value set to the revocation timestamp.\nTherefore, you can use the precondition (available in Terraform CLI v1.2.0 and higher) to validate the Image with Terraform CLI and make sure it was not revoked\nHere is the code example that illustrates that:\nWork with revoked HCP Packer image in Terraform Why HCP Packer? So what makes the HCP Packer a good fit and worth a try?\n1️⃣ A centralized place to view and manage the OS images throughout an organization. And as for me, it is good to have a neat web panel to look at things.\n2️⃣ Image Channels that help with logical organization and control.\n3️⃣ Ability to revoke an image to prevent its usage.\n4️⃣ API and Terraform provider as additional tools that enrich the user experience.\nWhen dealing with multiple golden images or with various cloud providers, the HCP Packer can be a good fit for your image pipeline.\nAs a registry, it enables the end-to-end workflow for golden image usage: create, validate, use and decommission the images in a centralized way.\nAnd no more hard-coded IDs, manual variable settings, or other duck tape and glue in your Terraform.\nIf you want to learn more about HCP Packer and have some practice, I suggest starting from the tutorial at HashiCorp Learn portal.\n","permalink":"https://devdosvid.blog/2022/06/26/golden-image-pipelines-with-hcp-packer/","summary":"How to create an end-to-end golden image workflow with the HCP Packer image registry","title":"Golden Image Pipelines With HCP Packer"},{"content":"In this blog, I would like to tell you about new cool features that Terraform 1.1 and 1.2 bring. It feels like Terraform has doubled its speed of delivering the new features after they released the 1.0. 🤩\nIt\u0026rsquo;s been only a few months since Terraform 1.1 was released with the moved block that empowers the code refactoring.\nNow Terraform 1.2 is almost ready (as I am writing this blog in early May 2022) to bring three new efficient controls to the resource lifecycle.\nThese are three new expressions: precondition, postcondition, and replace_triggered_by.\nSubscribe to blog updates! Terraform Code Refactoring With the Moved Block Starting from the 1.1 version, Terraform users can use the moved block to describe the changes in resource or module addresses (or resources inside a module) in the form of code. Once that is described, Terraform performs the movement of the resource within the state during the first apply.\nIn other words, what this feature gives you, is the ability to document your terraform state mv actions, so you and other project or module users don\u0026rsquo;t need to perform them manually.\nAs your code evolves, a resource or module can have several moved blocks associated with it, and Terraform will thoroughly reproduce the whole history of its movement within a state (i.e., renaming).\nLet\u0026rsquo;s review some examples that illustrate how it works.\nMove or rename a resource In a module, I have a bucket policy that has a generic, meaningless name. It is used in a module that creates a CloudFront distribution with an S3 bucket.\nAn example resource It\u0026rsquo;s pretty OK to name a resource like that if you have only a single instance of that kind in your code.\nLater, when I need to add another policy to the module, I don\u0026rsquo;t want to name it \u0026ldquo;that\u0026rdquo;. Instead, I want my policies to have meaningful names now.\nFor example, I could rename the old policy with the terraform state mv command, but other users of my module would not know about that.\nThat is where the moved block turns out to be helpful!\nThe moved block allows you to document how you rename or move an object in Terraform so that other code users can have the same changes afterward. Resource address update with the Moved block Terraform follows the instructions inside the module block to plan and apply changes. Although the resource address update is not counted as a change in the Plan output, Terraform will perform that update during apply.\nTerraform plan output Move or rename a module The same approach can be applied to a module — you can move or rename it as a code too.\nHere, I use two modules to create static hosting for a website with a custom TLS certificate.\nTwo modules with generic names Again, if I need to add another couple of the CDN+Certificate modules, I would like to have meaningful names in my code so clearly distinguish one from another.\nTherefore, I would add two moved blocks — one per module call.\nAnd by the way, since I renamed the module (from cert to example_com_cert), I need to update all references to that module\u0026rsquo;s outputs in the code too.\nTwo modules renamed However, there is one nuance: when you rename a module and declare that in the moved block, you need to run the terraform init before applying the change because Terraform must initialize the module with the new name first.\nTerraform error: module not installed There are some more advanced actions you can make with the moved block:\nImplement count and for_each meta-arguments to resources and modules Break one module into multiple Check the following detailed guide from HashiCorp that explains how to do that — Refactoring Introducing the moved blocks into your codebase defacto starts the refactoring process for your module users. But the finale of that refactoring happens when you ultimately remove these blocks.\nTherefore, here is some advice on how to manage that:\nKeep the moved blocks in your code for long. For example, when removing a moved block from the code, Terraform does not treat the new object name as a renaming anymore. Instead, Terraform will plan to delete the resource or module with the old name instead of renaming it.\nKeep the complete chains of object renaming (sequence of moves). The whole history of object movement ensures that users with different module versions will get a consistent and predictable behavior of the refactoring.\nLifecycle expressions: conditions and replacement trigger Terraform 1.2 fundamentally improves the lifecycle meta-argument by adding three new configuration options with rich capabilities.\nNew configuration options for the lifecycle meta-argument Precondition and Postcondition When you need to make sure that specific condition is met before or after you create a resource, you can use postcondition and precondition blocks.\nThe condition here — is some data or information about a resource you need to confirm to apply the code.\nHere are a few examples of such conditions:\nValidate some attributes of the Data Source that you cannot check using filters or other available arguments; Confirm the Resource argument that can compound several variables (e.g., list); Precondition works as an expectation or a guess about some external (but within a module) value that a resource depends on.\nPostcondition works as the assurance that a resource fulfills a specific condition so other resources may rely on that. If postcondition fails for a resource, this prevents changes to all other resources that depend on it.\nLet\u0026rsquo;s review this new feature with an example of postcondition usage.\nConsider the following case: our module receives AMI ID as the input variable, and that AMI should be used in the Launch Template then; we also have the requirement for the EC2 instance created from that Launch Template — its root EBS size must be equal or bigger than 600 GB.\nWe cannot validate the EBS size using the variable that accepts the AMI ID. But we can write a postcondition for the Data Source that gets the information about the AMI and reference that Data Source in the Launch Template resource afterward.\nData Source Postcondition The condition argument within the block accepts any of Terraform\u0026rsquo;s built-in functions or language operators.\nThe special self object is available only for the postcondition block because it assumes that validation can be performed after the object is created and its attributes are known.\nLater, if a module user specifies the AMI with an EBS size lesser than 600 GB, Terraform will fail to create the Launch Template because it depends on the Data Source that did not pass the postcondition check.\nResource postcondition error Terraform tries to evaluate the condition expressions as soonest: sometimes Terraform can check the value during the planning phase, but sometimes that can be done only after the resource is created if the value is unknown.\nValidate module output with precondition The precondition block is also available for the module outputs.\nJust like the variable validation block assures that module input meets certain expectations, the precondition is intended to ensure that a module produces the valid output.\nHere is an example: a module that creates an ACM certificate must prevent the usage of a specific domain name in the certificate\u0026rsquo;s Common Name or its SANs.\nModule output precondition In this case, instead of validating several input variables, we can write the validation only once for the output.\nValidation of the module output helps with standardization and control of the data passed between Terraform modules. Trigger resource replacement with replace_triggered_by Sometimes it\u0026rsquo;s needed to specify the dependency in the way that recreates a resource when another resource or its attribute changes.\nThis is useful when two (or more) resources do not have any explicit dependency.\nConsider the following case: you have two EC2 instances, A and B, and need to recreate the B instance if the private IP of instance A is changed.\nreplace_triggered_by example This is extremely useful when you\u0026rsquo;re dealing with logical abstractions over the set of resources.\nResource replacement is triggered when:\nany of the resources referenced in replace_triggered_by are updated any value is set to the resource attribute that is referenced in replace_triggered_by Getting started with Terraform 1.1 and 1.2 If you\u0026rsquo;re still using older Terraform versions, these new features might be a good motivation for you to upgrade!\nBefore upgrading, be sure to read the upgrade notes for the specific version at the releases page.\nAlso, an excellent tool can help with fast switching between different Terraform versions while you\u0026rsquo;re experimenting — tfswitch.\n","permalink":"https://devdosvid.blog/2022/05/04/new-lifecycle-options-and-refactoring-capabilities-in-terraform-1-1-and-1-2/","summary":"Terraform code refactoring and resource lifecycle conditions, and triggers — now natively available","title":"New Lifecycle Options and Refactoring Capabilities in Terraform 1.1 and 1.2"},{"content":"Here I want to share two Apple Shortcuts that I created for myself and use to process images for this blog:\nImage Optimization\nand\nImage Resize\nAbout a year ago, I posted the blog about the Automator quick action \u0026ldquo;Using TinyPNG Image Compression From MacOS Finder Contextual Menu\u0026rdquo;) to optimize PNG and JPEG images with TinyPNG service and save the processed images next to the original ones.\nWhile that Automator-based solution still works, macOS Monterey now supports Shortcuts — a new automating tool that seems to substitute the old fellow Automator.\nSo I decided to create a couple of automation with Shortcuts: one for image optimization (reduce file size but not the size in pixels), and one for image scaling (change its size in pixels).\nI have used them for several months to prepare images for this blog, and I really like how they work!\nImage Optimization with Monterey Shortcuts This Shortcut replicates the functionality of the Automator quick action and also uses TinyPNG service as a back-end. There are tons of other similar services, but I like TinyPNG for its simplicity: it just does one thing, and it does it well.\nSo first, you need to get yourself an API key for TinyPNG.\nThe simplest way to reuse my Shortcut is to import it from iCloud using the following URL:\n➡️ Click here to import the Image Optimization Shortcut ⬅️\nThe import will work only when the link is opened in Safari.\nShortcut Import Dialog To make the Shortcut work from the context menu of the Finder, set the options on the Details panel of the Shortcut setting as displayed on the screenshot:\nShortcut Settings Here is what this Image Optimization Shortcut does:\nThe Shortcut receives image files. Then, for every image file received, the Shotcut does the following:\nGets the file\u0026rsquo;s name and parent directory Sends the original file to TinyPNG Process the response with URL to download the optimized image Downloads the optimized image using the URL from the response and replaces the original image with the optimized And here is how this Shortcuts looks if you want to create it from scratch:\nImage Optimization Shortcut Unfortunately, this Shortcut won\u0026rsquo;t work on iOS or watchOS because they do not support the \u0026ldquo;File Storage\u0026rdquo; actions used in the Shortcut.\n🌟 Demo 🌟 Shortcut Demo Image Resize with Monterey Shortcuts Another Shortcut I actively use is the image resizer. Most of the images on my blog are 1600px width fitted into an 800px frame to look sharp on the high-res displays (e.g., Retina).\nAnd when I have many images in my folder, I want to make them all be 1600px width at once or don\u0026rsquo;t change their own size if they were created smaller intentionally (no upscale, in other words).\nHere is the link to Shortcut import (again, the import will work only when the link is opened in Safari):\n➡️ Click here to import the Image Resize Shortcut ⬅️\nHere is how the Image Resize Shortcut looks if you want to create it from scratch:\nImage Resizing Shortcut Fun with Shortcuts I love the way Apple works on routine automation. This Shortcuts app, ported from iOS, brings a lot of cool and fun possibilities to Mac.\nDo you use Shortcuts? What is your favorite? I would love to know!\n","permalink":"https://devdosvid.blog/2022/01/31/monterey-shortcuts-for-easy-and-fast-image-processing/","summary":"Some handy automation for image processing with Apple Shortcuts on your Mac","title":"Monterey Shortcuts for Easy and Fast Image Processing"},{"content":"Terraform built-in functionality is very feature-rich: functions, expressions, and meta-arguments provide many ways to shape the code and fit it to a particular use case. I want to share a few valuable practices to boost your Terraform expertise in this blog.\nSome code examples in this article will work with Terraform version 0.15 and onwards. But if you\u0026rsquo;re still using 0.14 or lower, here\u0026rsquo;s another motivation for you to upgrade. Conditional resource creation or how to implement the \u0026ldquo;if else\u0026rdquo; statement in Terraform With Terraform, you can have a conditional module or a resource creation by implementing the ternary operator — so-called Conditional Expressions.\nLet\u0026rsquo;s start from the most popular one: whether to create a resource depending on some fact, e.g., the value of a variable.\nTerraform meta-argument count helps to describe that kind of resource creation logic.\nHere is how it may look like:\ndata \u0026#34;aws_ssm_parameter\u0026#34; \u0026#34;ami_id\u0026#34; { count = var.ami_channel == \u0026#34;\u0026#34; ? 0 : 1 name = local.ami_channels[var.ami_channel] } The notation var.ami_channel == \u0026quot;\u0026quot; ? 0 : 1 is called conditional expression and means the following: if my variable is empty (var.ami_channel == \u0026quot;\u0026quot; — hence, true) then set the count to 0, otherwise set to 1.\ncondition ? true_val : false_val In this illustration, I want to get the AMI ID from the SSM Parameter only if the AMI channel (e.g., beta or alpha) is specified. Otherwise, providing that the ami_channel variable is an empty string by default (\u0026quot;\u0026quot;), the data source should not be created.\nWhen following this method, keep in mind that the resource address will contain the index identifier. So when I need to use the value of the SSM parameter from our example, I need to reference it the following way:\nami_id = data.aws_ssm_parameter.ami_id[0].value The count meta-argument can also be used when you need to conditionally create a Terraform module.\nmodule \u0026#34;bucket\u0026#34; { count = var.create_bucket == true ? 1 : 0 source = \u0026#34;./modules/s3_bucket\u0026#34; name = \u0026#34;my-unique-bucket\u0026#34; ... } The var.create_bucket == true ? 1 : 0 expression can be written even shorter: var.create_bucket ? 1 : 0 because the create_bucket variable has boolean type, apparently.\nBut what if you need to produce more than one instance of a resource or module? And still be able to avoid their creation.\nAnother meta-argument — for_each — will do the trick.\nFor example, this is how the for_each argument works for the conditional module creation:\nmodule \u0026#34;bucket\u0026#34; { for_each = var.bucket_names == [] ? [] : var.bucket_names source = \u0026#34;./modules/s3_bucket\u0026#34; name = \u0026#34;${each.key}\u0026#34; enable_encryption = true ... } In this illustration, I also used a conditional expression that makes Terraform iterate through the set of values of var.bucket_names if it\u0026rsquo;s not empty and create several modules. Otherwise, do not iterate at all and do not create anything.\nThe same can be done for the resources. For example, when you need to create an arbitrary number of security group rules, e.g., to allowlist some IPs for your bastion host:\nresource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;allowlist\u0026#34; { for_each = var.cidr_blocks == [] ? [] : var.cidr_blocks type = \u0026#34;ingress\u0026#34; from_port = 22 to_port = 22 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [each.value] security_group_id = aws_security_group.bastion.id } And just like with the count meta-argument, with the for_each, resource addresses will have the identifier named by the values provided to for_each.\nFor example, here is how I would reference a resource created in the module with for_each described earlier:\nbucket_name = module.bucket[\u0026#34;photos\u0026#34;].name Subscribe to blog updates! Conditional resource arguments (attributes) setting Now let\u0026rsquo;s go deeper and see how resource arguments can be conditionally set (or not).\nFirst, let\u0026rsquo;s review the conditional argument value setting with the null data type:\nresource \u0026#34;aws_launch_template\u0026#34; \u0026#34;this\u0026#34; { name = \u0026#34;my-launch-template\u0026#34; ... key_name = var.use_default_keypair ? var.keypair_name : null ... Here I want to skip the usage of the EC2 Key Pair for the Launch Template in some instances and Terraform allows me to write the conditional expression that will set the null value for the argument. It means the absence or omission and Terraform would behave the same as if you did not specify the argument at all.\nDynamic blocks are another case where conditional creation suits best. Take a look at the following piece of CloudFront resource code where I want to either describe the configuration for the custom error response or omit that completely:\nresource \u0026#34;aws_cloudfront_distribution\u0026#34; \u0026#34;cdn\u0026#34; { enabled = true ... dynamic \u0026#34;custom_error_response\u0026#34; { for_each = var.custom_error_response == null ? [] : [var.custom_error_response] iterator = cer content { error_code = lookup(cer.value, \u0026#34;error_code\u0026#34;, null) error_caching_min_ttl = lookup(cer.value, \u0026#34;error_caching_min_ttl\u0026#34;, null) response_code = lookup(cer.value, \u0026#34;response_code\u0026#34;, null) response_page_path = lookup(cer.value, \u0026#34;response_page_path\u0026#34;, null) } } ... } The custom_error_response variable is null by default, but it has the object type, and users can assign the variable with the required nested specifications if needed. And when they do it, Terraform will add the custom_error_response block to the resource configuration. Otherwise, it will be omitted entirely.\nConvert types in Terraform with ease Ok, let\u0026rsquo;s move to the less conditional things now 😅\nTerraform has several type conversion functions: tobool(), tolist(),tomap(), tonumber(), toset(), and tostring(). Their purpose is to convert the input values to the compatible types.\nFor example, suppose I need to pass the set to the for_each (it accepts only sets and maps types of value), but I got the list as an input; let\u0026rsquo;s say I got it as an output from another module. In such a case, I would do something like this:\nfor_each = toset(var.remote_access_ports) However, I can make my code cleaner and avoid the explicit conversion — I just need to define the value type in the configuration block of the my_list variable. Terraform will do the conversion automatically when the value is assigned.\nvariable \u0026#34;remote_access_ports\u0026#34; { description = \u0026#34;Ports for remote access\u0026#34; type = set(string) } While Terraform can do a lot of implicit conversions for you, explicit type conversions are practical during values normalization or when you need to calculate some complex value for a variable. For example, the Local Values, known as locals, are the most suitable place for doing that.\nBy the way, although there is a tolist() function, there is no such thing as the tostring() function. But what if you need to convert the list to string in Terraform?\nThe one() function can help here: it takes a list, set, or tuple value with either zero or one element and returns either null or that one element in the form of string.\nIt\u0026rsquo;s useful in cases when a resource created using conditional expression is represented as either a zero- or one-element list, and you need to get a single value which may be either null or string, for example:\nresource \u0026#34;aws_kms_key\u0026#34; \u0026#34;main\u0026#34; { count = var.ebs_encrypted ? 1 : 0 enable_key_rotation = true tags = var.tags } resource \u0026#34;aws_kms_alias\u0026#34; \u0026#34;main\u0026#34; { count = var.ebs_encrypted ? 1 : 0 name = \u0026#34;alias/encrypt-ebs\u0026#34; target_key_id = one(aws_kms_key.main[*]key_id) } Write YAML or JSON as Terraform code (HCL) Sometimes you need to supply JSON or YAML files to the services you manage with Terraform. For example, if you want to create something with CloudFormation using Terraform (and I am not kidding). Sometimes the AWS Terraform provider does not support the needed resource, and you want to maintain the whole infrastructure code using only one tool.\nInstead of maintaining another file in JSON or YAML format, you can embed JSON or YAML code management into HCL by taking benefit of the jsonencode() or yamlencode() functions.\nThe attractiveness of this approach is that you can reference other Terraform resources or their attributes right in the code of your object, and you have more freedom in terms of the code syntax and its formatting comparable to native JSON or YAML.\nHere is how it looks like:\nlocals { some_string = \u0026#34;ult\u0026#34; myjson_object = jsonencode({ \u0026#34;Hashicorp Products\u0026#34;: { Terra: \u0026#34;form\u0026#34; Con: \u0026#34;sul\u0026#34; Vag: \u0026#34;rant\u0026#34; Va: local.some_string } }) } The value of the myjson_object local variable would look like this:\n{ \u0026#34;Hashicorp Products\u0026#34;: { \u0026#34;Con\u0026#34;: \u0026#34;sul\u0026#34;, \u0026#34;Terra\u0026#34;: \u0026#34;form\u0026#34;, \u0026#34;Va\u0026#34;: \u0026#34;ult\u0026#34;, \u0026#34;Vag\u0026#34;: \u0026#34;rant\u0026#34; } } And here is a piece of real-world example:\nlocals { cf_template_body = jsonencode({ Resources : { DedicatedHostGroup : { Type : \u0026#34;AWS::ResourceGroups::Group\u0026#34; Properties : { Name : var.service_name Configuration : [ { Type : \u0026#34;AWS::EC2::HostManagement\u0026#34; Parameters : [ { Name : \u0026#34;auto-allocate-host\u0026#34; Values : [var.auto_allocate_host] }, ... ... Create custom file templates in Terraform The last case in this blog but not the least by its efficacy — render source file content as a template in Terraform.\nLet\u0026rsquo;s review the following scenario: you launch an EC2 instance and want to supply it with a bash script (via the user-data parameter) for some additional configuration at launch.\nSuppose we have the following bash script instance-init.sh that sets the hostname and registers our instance in a monitoring system:\n#!/bin/bash hostname example.com bash /opt/system-init/register-monitoring.sh But what if you want to set a different hostname per instance, and some instances should not be registered in the monitoring system?\nIn such a case, here is how the script file content will look:\n#!/bin/bash hostname ${system_hostname} %{ if register_monitoring } bash /opt/system-init/register-monitoring.sh %{endif} And when you supply this file as an argument for the EC2 instance resource in Terraform, you will use the templatefile() function to make the magic happen:\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;web\u0026#34; { ami = var.my_ami_id instance_type = var.instance_type ... user_data = templatefile(\u0026#34;${path.module}/instance-init.tftpl\u0026#34;, { system_hostname = var.system_hostname register_monitoring = var.add_to_monitoring }) ... } And of course, you can create a template from any file type. The only requirement here is that the template file must exist on the disk at the beginning of the Terraform execution.\nKey takeaways Terraform is far beyond the standard resource management operations. With the power of built-in functions, you can write more versatile code and reusable Terraform modules.\n✅ Use conditional expressions with count and for_each meta-arguments, when the creation of a resource depends on some context or user input.\n✅ Take advantage of implicit type conversion when working with input variables and their values to keep your code cleaner.\n✅ Embed YAML and JSON-based objects right into your Terraform code using built-in encoding functions.\n✅ And when you need to pass some files to the managed service, you can treat them as templates and make them multipurpose.\nThank you for reading down to this point! 🤗\nIf you have some favorite Terraform tricks — I would love to know!\n","permalink":"https://devdosvid.blog/2022/01/16/some-techniques-to-enhance-your-terraform-proficiency/","summary":"Learn what cool things Terraform can do with its built-in functionality","title":"Some Techniques to Enhance Your Terraform Proficiency"},{"content":"Terraform by itself automates a lot of things: it creates, changes, and versions your cloud resources. Although many teams run Terraform locally (sometimes with wrapper scripts), running Terraform in CI/CD can boost the organization\u0026rsquo;s performance and ensure consistent deployments.\nIn this article, I would like to review different approaches to integrating Terraform into generic deployment pipelines.\nWhere to store the Terraform code Storing Terraform code in the same repository as the application code or maintaining a separate repository for the infrastructure?\nThis question has no strict and clear answer, but here are some insights that may help you decide:\nThe Terraform and application code coupled together represent one unit, so it\u0026rsquo;s simple to maintain by one team; Conversely, if you have a dedicated team that manages infrastructure (e.g., platform team), a separate repository for infrastructure is more convenient because it\u0026rsquo;s a standalone project in that case. When infrastructure code is stored with the application, sometimes you have to deal with additional rules for the pipeline to separate triggers for these code parts. But sometimes (e.g., serverless apps) changes to either part (app/infra) should trigger the deployment. There is no right or wrong approach, but whichever you choose, remember to follow the Don’t Repeat Yourself (DRY) principle: make the infrastructure code modular by logically grouping resources into higher abstractions and reusing these modules. Preparing Terraform execution environment Running Terraform locally generally means that all dependencies are already in-place: you have the binary installed and present in the user\u0026rsquo;s PATH and perhaps even some providers already stored in the .terraform directory.\nBut when you shift Terraform runs from your local machine to stateless pipelines, this is not the case. However, you can still have a pre-built environment — this will speed up the pipeline execution and provide control over the process.\nDocker image with a Terraform binary is one of the popular solutions that address this. Once created, you can execute Terraform within a container context with configuration files mounted as a Docker volume.\nYou can use the official image from Hashicorp, but sometimes it makes sense to maintain your own Docker images with additional tools you may need. For instance, you can bake the tfsec tool into the image to use it for security inspection and have it ready inside the Docker container without the need to install it every time.\nHere is an example of a Dockerfile that builds an image with a custom Terraform version (you can override it as a build argument) and a tfsec tool. This example also shows how to verify the installed Terraform binary to make sure it\u0026rsquo;s signed by HashiCorp before we run it.\nFROM alpine:3.14 ARG TERRAFORM_VERSION=1.0.11 ARG TFSEC_VERSION=0.59.0 RUN apk add --no-cache --virtual .sig-check gnupg RUN wget -O /usr/bin/tfsec https://github.com/aquasecurity/tfsec/releases/download/v${TFSEC_VERSION}/tfsec-linux-amd64 \\ \u0026amp;\u0026amp; chmod +x /usr/bin/tfsec RUN cd /tmp \\ \u0026amp;\u0026amp; wget \u0026#34;https://releases.hashicorp.com/terraform/${TERRAFORM_VERSION}/terraform_${TERRAFORM_VERSION}_linux_amd64.zip\u0026#34; \\ \u0026amp;\u0026amp; wget https://keybase.io/hashicorp/pgp_keys.asc \\ \u0026amp;\u0026amp; gpg --import pgp_keys.asc \\ \u0026amp;\u0026amp; gpg --fingerprint --list-signatures \u0026#34;HashiCorp Security\u0026#34; | grep -q \u0026#34;C874 011F 0AB4 0511 0D02 1055 3436 5D94 72D7 468F\u0026#34; || exit 1 \\ \u0026amp;\u0026amp; gpg --fingerprint --list-signatures \u0026#34;HashiCorp Security\u0026#34; | grep -q \u0026#34;34365D9472D7468F\u0026#34; || exit 1 \\ \u0026amp;\u0026amp; wget https://releases.hashicorp.com/terraform/${TERRAFORM_VERSION}/terraform_${TERRAFORM_VERSION}_SHA256SUMS \\ \u0026amp;\u0026amp; wget https://releases.hashicorp.com/terraform/${TERRAFORM_VERSION}/terraform_${TERRAFORM_VERSION}_SHA256SUMS.sig \\ \u0026amp;\u0026amp; gpg --verify terraform_${TERRAFORM_VERSION}_SHA256SUMS.sig terraform_${TERRAFORM_VERSION}_SHA256SUMS || exit 1 \\ \u0026amp;\u0026amp; sha256sum -c terraform_${TERRAFORM_VERSION}_SHA256SUMS 2\u0026gt;\u0026amp;1 | grep -q \u0026#34;terraform_${TERRAFORM_VERSION}_linux_amd64.zip: OK\u0026#34; || exit 1 \\ \u0026amp;\u0026amp; unzip terraform_${TERRAFORM_VERSION}_linux_amd64.zip -d /bin \\ \u0026amp;\u0026amp; rm -rf /tmp/* \u0026amp;\u0026amp; apk del .sig-check But the main functionality of Terraform is delivered by provider plugins. It takes time to download the provider: for example, the AWS provider is about 250MB, and in a large scale, with hundreds of Terraform runs per day, this makes a difference.\nThere are two common ways to deal with it: either use a shared cache available to your pipeline workloads or bake provider binaries into the runtime environment (i.e., Docker image).\nThe critical element for both approaches is the configuration of the plugin cache directory path. By default, Terraform looks for plugins and downloads them in the .terraform directory, which is local to the main project directory. But you can override this, and you can leverage the TF_PLUGIN_CACHE_DIR environment variable to do that.\nIf supported by your CI/CD tool, the shared cache can significantly reduce the operational burden because all your pipeline runtime environments can use it to get the needed provider versions.\nSo all you have to do is to maintain the provider versions in the shared cache and instruct Terraform to use it:\nMount the cache directory to the pipeline runtime (i.e., docker container) and specify its internal path Set the value of the TF_PLUGIN_CACHE_DIR environment variable accordingly On the other hand, you can bake the provider binaries into the Docker image and inject the value for the TF_PLUGIN_CACHE_DIR environment variable right into the Dockerfile.\nThis approach takes more operational effort but makes the Terraform environment self-sufficient and stateless. It also allows you to set strict boundaries around permitted provider versions as a security measure. Planning and Applying changes Now let\u0026rsquo;s review the ways to automate planning and applying of changes. Although terraform apply can do both, it\u0026rsquo;s sometimes useful to separate these actions.\nInitialization CI/CD pipelines generally run in stateless environments. Thus, every subsequent run of Terraform looks like a fresh start, so the project needs to be initialized before other actions can be performed.\nThe usage of the init command in CI/CD slightly differs from its common local usage:\n\u0026gt; terraform init -input=false The -input=false option prevents Terraform CLI from asking for user actions (it will throw an error if the input was required).\nAlso, there is -no-color option that prevents the usage of color codes in a shell, so the output will look much cleaner if your CI/CD logging system cannot render the terminal formatting.\nAnother option of the init command that is useful in CI — is the -backend-config. That option allows you to override the backend configuration in your code or define it if you prefer to use partial configuration, thus creating more uniform pipelines.\nFor example, here is how you can use the same code with different roles in different environments on AWS:\n\u0026gt; terraform init -input=false \\ -backend-config=\u0026#34;role_arn=arn:aws:iam::012345678901:role/QADeploymentAutomation\u0026#34; Terraform init produces two artifacts:\n.terraform directory, which Terraform uses to manage cached provider plugins and modules, and record backend information .terraform.lock.hcl file, which Terraform uses to track provider dependencies They both must be present in the project directory to successfully run the subsequent plan and apply commands.\nHowever, I suggest checking in .terraform.lock.hcl to your repository as suggested by HashiCorp (Dependency Lock File): this way you will be able to control dependencies more thoroughly, and you will not worry about transferring this file between build stages.\nPlan The terraform plan command helps you validate the changes manually. However, there are ways to use it in automation as well.\nBy default, Terraform prints the plan output in a human-friendly format but also supports machine-readable JSON. With additional command-line options, you can extend your CI experience.\nFor example, you can use your validation conditions to decide whether to apply the changes automatically; or you can parse the plan details and integrate the summary into a Pull Request description. Let’s review a simple example that illustrates it.\nFirst, you need to save the plan output to the file:\n\u0026gt; terraform plan -input=false -compact-warnings -out=plan.file The main point here is the -out option — it tells Terraform to save its output into a binary plan file, and we will talk about it in the next paragraph.\nThe -compact-warnings option suppresses the warning-level messages produced by Terraform.\nAlso, the plan command has the -detailed-exitcode option that returns detailed exit codes when the command exits. For example, you can leverage this in a script that wraps Terraform and adds more conditional logic to its execution, because CIs will generally fail the pipeline on a command’s non-zero exit code. However, that may add complexity to the pipeline logic.\nSo if you need to get detailed info about the plan, I suggest parsing the plan output.\nWhen you have a plan file, you can read it in JSON format and parse it. Here is a code snippet that illustrates that:\n\u0026gt; terraform show -json plan.file| jq -r \u0026#39;([.resource_changes[]?.change.actions?]|flatten)|{\u0026#34;create\u0026#34;:(map(select(.==\u0026#34;create\u0026#34;))|length),\u0026#34;update\u0026#34;:(map(select(.==\u0026#34;update\u0026#34;))|length),\u0026#34;delete\u0026#34;:(map(select(.==\u0026#34;delete\u0026#34;))|length)}\u0026#39; { \u0026#34;create\u0026#34;: 1, \u0026#34;update\u0026#34;: 0, \u0026#34;delete\u0026#34;: 0 } Another way to see the information about changes, is to run the plan command with -json option and parse its output to stdout (available starting from Terraform 1.0.5):\n\u0026gt; terraform plan -json|jq \u0026#39;select( .type == \u0026#34;change_summary\u0026#34;)|.\u0026#34;@message\u0026#34;\u0026#39; \u0026#34;Plan: 1 to add, 0 to change, 0 to destroy.\u0026#34; This technique can make your Pull Request messages more informative and improve your collaboration with teammates. You can write a custom script/function that sends a Pull Request comment to VCS using its API. Or you can try the existing features of your VCS: with GitHub Actions, you can use the Terraform PR Commenter or similar action to achieve that; for GitLab, there is a built-in functionality that integrates plan results into the Merge Request — Terraform integration in Merge Requests.\nYou can find more information about the specification of the JSON output here — Terraform JSON Output Format.\nApply When the plan file is ready, and the proposed changes are expected and approved, it\u0026rsquo;s time to apply them.\nHere is how the apply command may look like in automation:\nterraform apply -input=false -compact-warnings plan.file Here, the plan.file is the file we got from the previous plan step.\nAlternatively, you might want to omit the planning phase at all. In that case, the following command will apply the configuration immediately, without the need for a plan:\nterraform apply -input=false -compact-warnings -auto-approve Here, the -auto-approve option tells Terraform to create the plan implicitly and skip the interactive approval of that plan before applying.\nWhichever way you choose, keep in mind the destructive nature of the apply command. Hence, the fully automated apply of configuration generally works well with environments that tolerate unexpected downtimes, such as development or testing. Whereas plan review is recommended for production-grade environments, and in that case, the apply job is configured for a manual trigger.\nDealing with stateless environments If you run init, plan, and apply commands in different environments, you need to care for some artifacts produced by Terraform:\nThe .terraform directory with information about modules, providers, and the state file (even in the case of remote state). The .terraform.lock.hcl file — the dependency lock file which Terraform uses to check the integrity of provider versions used for the project. If your VCS does not track it, you\u0026rsquo;ll need to pass that file to the plan and apply commands to make them work after init. The output file of the plan command is essential for the apply command, so treat it as a vital artifact. This file includes a full copy of the project configuration, the state, and variables passed to the plan command (if any). Therefore, mind the security precautions because sensitive information may be present there. There is one shortcut, though. You can execute the init and plan commands within the same step/stage and transfer the artifacts only once — to the apply execution.\nUsing the command-line and environments variables Last but not least, a few words about ways to maximize the advantage of variables when running Terraform in CI.\nThere are two common ways how you can pass values for the variables used in the configuration:\nUsing a -var-file option with the variable definitions file — a filename ending in .tfvars or .tfvars.json. For example: terraform apply -var-file=development.tfvars -input=false -no-color -compact-warnings -auto-approve Also, Terraform can automatically load the variables from files named exactly terraform.tfvars or terraform.tfvars.json: with that approach, you don’t need to specify the tfvar file as a command option explicitly. Using environment variables with the prefix TF_VAR_. Implicitly, Terraform always looks for the environment variables (within its process context) with that prefix, so the same \u0026ldquo;instance_type\u0026rdquo; variables from the example above can be passed as follows: export TF_VAR_instance_type=t3.nano terraform -input=false -no-color -compact-warnings -auto-approve The latter method is widely used in CI because modern CI/CD tools support the management of the environment variables for automation jobs.\nPlease refer to the following official documentation if you want to know more about variables — Terraform Input Variables.\nAlong with that, Terraform supports several configuration parameters in the form of environment variables. These parameters are optional; however, they can simplify the automation management and streamline its code.\nTF_INPUT — when set to \u0026ldquo;false\u0026rdquo; or \u0026ldquo;0\u0026rdquo;, this tells Terraform to behave the same way as with the -input=false flag; TF_CLI_ARGS — can contain a set of command-line options that will be passed to one or another Terraform command. Therefore, the following notation can simplify the execution of apply and plan commands by unifying their options for CI: export TF_CLI_ARGS=\u0026#34;-input=false -no-color -compact-warnings\u0026#34; terraform plan ... terraform apply ... You can advantage this even more when using this variable as the environment configuration of stages or jobs in a CI/CD tool. TF_IN_AUTOMATION — when set to any non-empty value (e.g., \u0026ldquo;true\u0026rdquo;), Terraform stops suggesting commands run after the one you execute, hence producing less output. Key takeaways There are two primary outcomes from automating Terraform executions: consistent results and integrating with the code or project management solutions. Although the exact implementation of Terraform in CI may vary per project or team, try to aim the following goals when working on it:\nEase of code management A secure and controlled execution environment Coherent runs of init, plan, apply phases Leveraging of built-in Terraform capabilities I originally wrote this article for the Spacelift.io technical blog. But I decided to keep it here as well, for the history. The canonical link to their blog has been set accordingly. ","permalink":"https://devdosvid.blog/2021/11/24/guide-to-using-terraform-in-ci/cd/","summary":"How to configure, how to run, and what to mind for when using Terraform in CI/CD","title":"Guide to Using Terraform in CI/CD"},{"content":"In November 2021, AWS announced Response Headers Policies — native support of response headers in CloudFront. You can read the full announcement here: Amazon CloudFront introduces Response Headers Policies\nI said \u0026ldquo;native\u0026rdquo; because previously you could set response headers either using CloudFront Functions or Lambda@Edge.\nAnd one of the common use cases for that was to set security headers. Now you don\u0026rsquo;t need to add intermediate requests processing to modify the headers: CloudFront does that for you with no additional fee.\nManage Security Headers as Code Starting from the 3.64.0 version of Terraform AWS provider, you can create the security headers policies and apply them for your distribution.\nLet\u0026rsquo;s see how that looks!\nFirst, you need to describe the aws_cloudfront_response_headers_policy resource:\nresource \u0026#34;aws_cloudfront_response_headers_policy\u0026#34; \u0026#34;security_headers_policy\u0026#34; { name = \u0026#34;my-security-headers-policy\u0026#34; security_headers_config { content_type_options { override = true } frame_options { frame_option = \u0026#34;DENY\u0026#34; override = true } referrer_policy { referrer_policy = \u0026#34;same-origin\u0026#34; override = true } xss_protection { mode_block = true protection = true override = true } strict_transport_security { access_control_max_age_sec = \u0026#34;63072000\u0026#34; include_subdomains = true preload = true override = true } content_security_policy { content_security_policy = \u0026#34;frame-ancestors \u0026#39;none\u0026#39;; default-src \u0026#39;none\u0026#39;; img-src \u0026#39;self\u0026#39;; script-src \u0026#39;self\u0026#39;; style-src \u0026#39;self\u0026#39;; object-src \u0026#39;none\u0026#39;\u0026#34; override = true } } } List of security headers used:\nX-Content-Type-Options X-Frame-Options Referrer Policy X-XSS-Protection Strict Transport Security Content Security Policy The values for the security headers can be different, of course. However, the provided ones cover the majority of cases. And you can always get the up to date info about these headers and possible values here: Mozilla web Security Guidelines\nAlso, you could notice that provided example uses the override argument a lot. The override argument tells CloudFront to set these values for specified headers despite the values received from the origin. This way, you can enforce your security headers configuration.\nOnce you have the aws_cloudfront_response_headers_policy resource, you can refer to it in the code of aws_cloudfront_distribution resource inside cache behavior block (default or ordered). For example, in your default_cache_behavior:\nresource \u0026#34;aws_cloudfront_distribution\u0026#34; \u0026#34;test\u0026#34; { default_cache_behavior { target_origin_id = aws_s3_bucket.my_origin.id allowed_methods = [\u0026#34;GET\u0026#34;, \u0026#34;HEAD\u0026#34;, \u0026#34;OPTIONS\u0026#34;] cached_methods = [\u0026#34;GET\u0026#34;, \u0026#34;HEAD\u0026#34;] viewer_protocol_policy = \u0026#34;redirect-to-https\u0026#34; # some arguments skipped from listing for the sake of simplicity response_headers_policy_id = aws_cloudfront_response_headers_policy.security_headers_policy.id } # some arguments skipped from listing for the sake of simplicity } Security Scan Results Here is what Mozilla Observatory reports about my test CF distribution where I enabled the policy described above:\nScan summary for CloudFront distribution with security headers policy So with just minimum effort, you can greatly boost your web application security posture.\nMore to read: Terraform Resource: aws_cloudfront_response_headers_policy Creating response headers policies - Amazon CloudFront Using the managed response headers policies - Amazon CloudFront Understanding response headers policies - Amazon CloudFront ","permalink":"https://devdosvid.blog/2021/11/05/apply-cloudfront-security-headers-with-terraform/","summary":"How to use Response Headers Policy and Terraform to configure security headers for CloudFront Distribution","title":"Apply Cloudfront Security Headers With Terraform"},{"content":"It’s been almost a year since I started using macOS EC2 instances on AWS: there were ups and downs in service offerings and a lot of discoveries with macOS AMI build automation.\nAnd I like this small but so helpful update to the offerings list of the EC2 service: with mac1.metal instances, seamless integration of Apple-oriented CI/CD with other AWS infrastructure could finally happen.\nBut while management of a single mac1.metal node (or a tiny number of ones) is not a big deal (especially when Dedicated Host support was added to Terraform provider), governing the fleet of instances is still complicated.\nOr it has been complicated until recent days.\nWith a growing number of instances, the following challenges arise:\nScale mac1.metal instances horizontally Automatically allocate and release Dedicated Hosts needed for instances Automatically replace unhealthy instances If you have worked with AWS before, you know that Auto Scaling Group service can solve such things.\nAuto Scaling for macOS EC2 Instances So how does all that work?\nLet’s review the diagram that illustrates the interconnection between involved services:\nServices logical interconnection With the help of Licence Manager service and Launch Templates, you can set up EC2 Auto Scaling Group for mac1.metal and leave the automated instance provisioning to the service.\nLicense Configuration First, you need to create a License Configuration so that the Host resource group can allocate the hots.\nGo to AWS License Manager -\u0026gt; Customer managed licenses -\u0026gt; Create customer-managed license.\nSpecify Sockets as the Licence type. You may skip setting the Number of Sockets. However, the actual limit of mac1.metal instances per account is regulated by Service Quota. The default number of mac instances allowed per account is 3. Therefore, consider increasing this to a more significant number.\nLicence configuration values Host resource group Second, create the Host resource group: AWS License Manager -\u0026gt; Host resource groups -\u0026gt; Create host resource group.\nWhen creating the Host resource group, check “Allocate hosts automatically” and “Release hosts automatically” but leave “Recover hosts automatically” unchecked. Dedicated Host does not support host recovery for mac1.metal. However, Auto Scaling Group will maintain the desired number of instances if one fails the health check (which assumes the case of host failure as well).\nAlso, I recommend specifying “mac1” as an allowed Instance family for the sake of transparent resource management: only this instance type is permitted to allocate hosts in the group.\nHost resource group configuration values Optionally, you may specify the license association here (the Host group will pick any compatible license) or select the license you created on step one.\nLaunch Template Create Launch Template: EC2 -\u0026gt; Launch templates -\u0026gt; Create launch template.\nI will skip the description of all Launch Template parameters (but here is a nice tutorial), if you don’t mind, and keep focus only on the items relevant to the current case.\nSpecify mac1.metal as the Instance type. Later, in Advanced details: find the Tenancy parameter and set it to “Dedicated host”; for Target host by select “Host resource group”, and once selected the new parameter Tenancy host resource group will appear where you should choose your host group; select your license in License configurations parameter.\nLaunch Template configuration values Auto Scaling Group Finally, create the Auto Scaling Group: EC2 -\u0026gt; Auto Scaling groups -\u0026gt; Create Auto Scaling group.\nThe vital thing to note here — is the availability of the mac1.metal instance in particular AZ.\nMac instances are available in us-east-1 and 7 more regions, but not every Availability Zone in the region supports it. So you must figure out which AZ supports the needed instance type.\nThere is no documentation for that, but there is an AWS CLI command that can answer this question: describe-instance-type-offerings — AWS CLI 2.3.0 Command Reference\nHere is an example for the us-east-1 region: Click here to see the code snippet \u0026gt; aws ec2 describe-instance-type-offerings --location-type availability-zone-id --filters Name=instance-type,Values=mac1.metal --region us-east-1 --output text INSTANCETYPEOFFERINGS\tmac1.metal\tuse1-az6\tavailability-zone-id INSTANCETYPEOFFERINGS\tmac1.metal\tuse1-az4\tavailability-zone-id Keep that nuance in mind when selecting a subnet for the mac1.metal instances.\nWhen you know the AZ, specify the respective Subnet in the Auto Scaling Group settings, and you\u0026rsquo;re ready to go!\nBring Infrastructure as Code here I suggest describing all that as a code. I prefer Terraform, and its AWS provider supports the needed resources. Except one.\nAs of October 2021, resources supported :\naws_servicequotas_service_quota aws_licensemanager_license_configuration aws_launch_template aws_autoscaling_group The Host resource group is not yet supported by the provider, unfortunately. However, we can use CloudFormation in Terraform to overcome that: describe the Host resource group as aws_cloudformation_stack Terraform resource using CloudFormation template from a file.\nHere is how it looks like: Click here to see the code snippet resource \u0026#34;aws_licensemanager_license_configuration\u0026#34; \u0026#34;this\u0026#34; { name = local.full_name license_counting_type = \u0026#34;Socket\u0026#34; } resource \u0026#34;aws_cloudformation_stack\u0026#34; \u0026#34;this\u0026#34; { name = local.full_name # the name of CloudFormation stack template_body = file(\u0026#34;${path.module}/resource-group-cf-stack-template.json\u0026#34;) parameters = { GroupName = local.full_name # the name for the Host group, passed to CloudFormation template } on_failure = \u0026#34;DELETE\u0026#34; } And the next code snippet explains the CloudFromation template (which is the resource-group-cf-stack-template.json file in the code snippet above) Click here to see the code snippet { \u0026#34;Parameters\u0026#34; : { \u0026#34;GroupName\u0026#34; : { \u0026#34;Type\u0026#34; : \u0026#34;String\u0026#34;, \u0026#34;Description\u0026#34; : \u0026#34;The name of Host Group\u0026#34; } }, \u0026#34;Resources\u0026#34; : { \u0026#34;DedicatedHostGroup\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;AWS::ResourceGroups::Group\u0026#34;, \u0026#34;Properties\u0026#34;: { \u0026#34;Name\u0026#34;: { \u0026#34;Ref\u0026#34;: \u0026#34;GroupName\u0026#34; }, \u0026#34;Configuration\u0026#34;: [ { \u0026#34;Type\u0026#34;: \u0026#34;AWS::ResourceGroups::Generic\u0026#34;, \u0026#34;Parameters\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;allowed-resource-types\u0026#34;, \u0026#34;Values\u0026#34;: [\u0026#34;AWS::EC2::Host\u0026#34;] }, { \u0026#34;Name\u0026#34;: \u0026#34;deletion-protection\u0026#34;, \u0026#34;Values\u0026#34;: [\u0026#34;UNLESS_EMPTY\u0026#34;] } ] }, { \u0026#34;Type\u0026#34;: \u0026#34;AWS::EC2::HostManagement\u0026#34;, \u0026#34;Parameters\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;allowed-host-families\u0026#34;, \u0026#34;Values\u0026#34;: [\u0026#34;mac1\u0026#34;] }, { \u0026#34;Name\u0026#34;: \u0026#34;auto-allocate-host\u0026#34;, \u0026#34;Values\u0026#34;: [\u0026#34;true\u0026#34;] }, { \u0026#34;Name\u0026#34;: \u0026#34;auto-release-host\u0026#34;, \u0026#34;Values\u0026#34;: [\u0026#34;true\u0026#34;] }, { \u0026#34;Name\u0026#34;: \u0026#34;any-host-based-license-configuration\u0026#34;, \u0026#34;Values\u0026#34;: [\u0026#34;true\u0026#34;] } ] } ] } } }, \u0026#34;Outputs\u0026#34; : { \u0026#34;ResourceGroupARN\u0026#34; : { \u0026#34;Description\u0026#34;: \u0026#34;ResourceGroupARN\u0026#34;, \u0026#34;Value\u0026#34; : { \u0026#34;Fn::GetAtt\u0026#34; : [\u0026#34;DedicatedHostGroup\u0026#34;, \u0026#34;Arn\u0026#34;] } } } } The aws_cloudformation_stack resource will export the DedicatedHostGroup attribute (see the code of CloudFromation template), which you will use later in the Launch Template resource.\nIf you manage an AWS Organization, I have good news: Host groups and Licenses are supported by Resource Access Manager service.\nHence, you can host all mac instances in one account and share them with other accounts — it might be helpful for costs allocation, for example. Also, check out my blog about AWS RAM if you are very new to this service.\nAnd you can leverage the aws_ec2_instance_type_offerings and aws_subnet_ids data sources to solve the “which AZ supports mac metal” puzzle.\nCosts considerations License Manager is a free of charge service, as well as Auto Scaling, and Launch Template.\nSo it’s all about the price for mac1.metal Dedicated Host which is $1.083 per hour as of October 2021. However, Saving Plans can be applied.\nPlease note that the minimum allocation time for that type of host is 24 hours. Maybe someday AWS will change that to 1-hour minimum someday (fingers crossed).\nOh. So. ASG. The Auto Scaling for mac1.metal opens new possibilities for CI/CD: you can integrate that to your favorite tool (GitLab, Jenkins, whatsoever) using AWS Lambda and provision new instances when your development/testing environments need that.\nOr you can use other cool ASG stuff, such as Lifecycle hooks, to create even more custom scenarios.\nAlso, I want to say thanks (thanks, pal!) to OliverKoo, who started digging into that back in April'21.\n","permalink":"https://devdosvid.blog/2021/10/24/auto-scaling-group-for-your-macos-ec2-instances-fleet/","summary":"Scale macOS EC2 Instances fleet with the Licence Manager and ASG services","title":"Auto Scaling Group for your macOS EC2 Instances fleet"},{"content":"With a multi-account approach of building the infrastructure, there is always a challenge of provision and governance of the resources to subordinate accounts within the Organization. Provision resources, keep them up to date, and decommission them properly — that\u0026rsquo;s only a part of them.\nAWS has numerous solutions that help make this process reliable and secure, and the Resource Access Manager (RAM) is one of them. In a nutshell, the RAM service allows you to share the AWS resources you create in one AWS account with other AWS accounts. They can be your organizations\u0026rsquo; accounts, organizational units (OU), or even third-party accounts.\nSo let\u0026rsquo;s see what the RAM is and review some of its usage examples.\nWhy using RAM There are several benefits of using the RAM service:\nReduced operational overhead: eliminate the need of provisioning the same kind of resource multiple times — RAM does that for you\nSimplified security management: AWS RAM-managed permissions (at least one per resource type) define the actions that principals with access to the resources (i.e., resource users) can perform on those resources.\nConsistent experience: you share the resource in its state and with its security configuration with an arbitrary number of accounts.\nThat plays incredibly well in the case of organization-wide sharing: new accounts get the resources automatically. And the shared resource itself looks like a native resource in the account that accepts your sharing.\nAudit and visibility: RAM integrates with the CloudWatch and CloudTrail.\nHow to share a resource When you share a resource, the AWS account that owns that resource retains full ownership of the resource.\nSharing of the resource doesn\u0026rsquo;t change any permissions or quotas that apply to that resource. Also, you can share the resource only if you own it.\nAvailability of the shared resources scopes to the Region: the users of your shared resources can access these resources only in the same Region where resources belong.\nCreation of resource share consists of three steps: Specify the share name and the resource(s) you want to share. It can be either one resource type or several. You can also skip the resources selection and do that later.\nIt\u0026rsquo;s possible to modify the resource share later (e.g., you want to add some resources to the share).\nAssociate permissions with resource types you share. Some resources can have only one managed permission (will be attached automatically), and some can have multiple.\nYou can check the Permissions Library in the AWS RAM Console to see what managed permissions are available.\nSelect who can use the resources you share: either external or Organization account or IAM role/user. If you share the resource with third parties, they will have to accept the sharing explicitly.\nOrganization-wide resource share is accepted implicitly if resource sharing is enabled for the Organization.\nFinally, review the summary page of the resource share and create it.\nOnly specific actions are available to the users of shared resources. These actions mostly have the \u0026ldquo;read-only\u0026rdquo; nature and vary by resource type.\nAlso, the RAM service is supported by Terraform, so the resource sharing configuration may look like that, for example:\nresource \u0026#34;aws_ram_resource_share\u0026#34; \u0026#34;example\u0026#34; { name = \u0026#34;example\u0026#34; allow_external_principals = false tags = { Environment = \u0026#34;Production\u0026#34; } } resource \u0026#34;aws_ram_resource_association\u0026#34; \u0026#34;example\u0026#34; { resource_arn = aws_subnet.example.arn resource_share_arn = aws_ram_resource_share.example.arn } Example use cases One of the trivial but valuable examples of RAM service usage is sharing a Manged Prefix List. Suppose you have some service user across your Organization, a self-hosted VPN server, for example. And you have a static set of IPs for that VPN: you trust these IPs and would like them to be allow-listed in your other services. How to report these IPs to all organization accounts/users? And if the IP set changes, how to announce that change, and what should be done to reflect that change in services that depend on it, for example, Security Groups?\nThe answer is a shared Managed Prefix List. You create the list once in the account and share it across your Organization. Other accounts automatically get access to that list and can reference the list in their Security Groups. And when the list entry is changed, they do not need to perform any actions: their Security Groups will get the updated IPs implicitly.\nAnother everyday use case of RAM is the VPC sharing that can form the foundation of the multi-account AWS architectures.\nOf course, the RAM service is not the only way to organize and centralize resource management in AWS. There are Service Catalog, Control Tower, Systems Manager, Config, and others. However, the RAM is relatively simple to adopt but is capable of providing worthy outcomes.\n","permalink":"https://devdosvid.blog/2021/09/25/aws-resource-access-manager-multi-account-resource-governance/","summary":"Provision and manage resources within the AWS Organization with ease","title":"AWS Resource Access Manager — Multi Account Resource Governance"},{"content":"In days of containers and serverless applications, Ansible looks not such a trendy thing.\nBut still, there are cases when it helps, and there are cases when it combines very well with brand new product offerings, such as EC2 Mac instances.\nThe more I use mac1.metal in AWS, the more I see that Ansible becomes a bedrock of software customization in my case.\nAnd when you have a large instances fleet, the AWS Systems Manager becomes your best friend (the sooner you get along together, the better).\nSo is it possible to use Ansible playbooks for mac1.metal on a big scale, with the help of AWS Systems Manager?\n(Not) Available out of the box AWS Systems Manager (SSM hereafter) has a pre-defined, shared Document that allows running Ansible playbooks.\nIt’s called “AWS-RunAnsiblePlaybook,” and you can find it in AWS SSM → Documents → Owned by Amazon.\nHowever, this Document is not quite “friendly” to macOS. When the SSM agent calls Ansible on the Mac EC2 instance, it does not recognize the Ansible installed with Homebrew (de-facto most used macOS package manager).\nSo if you try to run a command on the mac1.metal instance using this Document, you will get the following error:\nAnsible is not installed. Please install Ansible and rerun the command. The root cause is trivial: the path to Ansible binary is not present on the list of paths available to the SSM agent by default.\nThere are several ways to solve that, but I believe that the most convenient one would be to create your custom Document — a slightly adjusted version of the default one provided by AWS.\nCreating own SSM Document for Ansible installed with Homebrew All you need to do is clone the Document provided by AWS and change its code a little — replace the callouts of ansible with the full path to the binary.\nNavigate to AWS SSM → Documents → Owned by Amazon and type AWS-RunAnsiblePlaybook in the search field.\nSelect the Document by pressing the circle on its top-right corner and then click Actions → Clone document.\nGive the new SSM Document a name, e.g., macos-arbitrary-ansible-playbook, and change the ansible callouts (at the end of the code) with the full path to the ansible symlink made by Homebrew which is /usr/local/bin/ansible\nHere is the complete source code of the Document with adjusted Ansible path:\nClick here to see the code snippet { \u0026#34;schemaVersion\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Use this document to run arbitrary Ansible playbooks on macOS EC2 instances. Specify either YAML text or URL. If you specify both, the URL parameter will be used. Use the extravar parameter to send runtime variables to the Ansible execution. Use the check parameter to perform a dry run of the Ansible execution. The output of the dry run shows the changes that will be made when the playbook is executed.\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;playbook\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;(Optional) If you don\u0026#39;t specify a URL, then you must specify playbook YAML in this field.\u0026#34;, \u0026#34;default\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;displayType\u0026#34;: \u0026#34;textarea\u0026#34; }, \u0026#34;playbookurl\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;(Optional) If you don\u0026#39;t specify playbook YAML, then you must specify a URL where the playbook is stored. You can specify the URL in the following formats: http://example.com/playbook.yml or s3://examplebucket/plabook.url. For security reasons, you can\u0026#39;t specify a URL with quotes.\u0026#34;, \u0026#34;default\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;allowedPattern\u0026#34;: \u0026#34;^\\\\s*$|^(http|https|s3)://[^\u0026#39;]*$\u0026#34; }, \u0026#34;extravars\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;(Optional) Additional variables to pass to Ansible at runtime. Enter a space separated list of key/value pairs. For example: color=red or fruits=[apples,pears]\u0026#34;, \u0026#34;default\u0026#34;: \u0026#34;foo=bar\u0026#34;, \u0026#34;displayType\u0026#34;: \u0026#34;textarea\u0026#34;, \u0026#34;allowedPattern\u0026#34;: \u0026#34;^((^|\\\\s)\\\\w+=(\\\\S+|\u0026#39;.*\u0026#39;))*$\u0026#34; }, \u0026#34;check\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34; (Optional) Use the check parameter to perform a dry run of the Ansible execution.\u0026#34;, \u0026#34;allowedValues\u0026#34;: [ \u0026#34;True\u0026#34;, \u0026#34;False\u0026#34; ], \u0026#34;default\u0026#34;: \u0026#34;False\u0026#34; }, \u0026#34;timeoutSeconds\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;(Optional) The time in seconds for a command to be completed before it is considered to have failed.\u0026#34;, \u0026#34;default\u0026#34;: \u0026#34;3600\u0026#34; } }, \u0026#34;mainSteps\u0026#34;: [ { \u0026#34;action\u0026#34;: \u0026#34;aws:runShellScript\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;runShellScript\u0026#34;, \u0026#34;inputs\u0026#34;: { \u0026#34;timeoutSeconds\u0026#34;: \u0026#34;{{ timeoutSeconds }}\u0026#34;, \u0026#34;runCommand\u0026#34;: [ \u0026#34;#!/bin/bash\u0026#34;, \u0026#34;/usr/local/bin/ansible --version\u0026#34;, \u0026#34;if [ $? -ne 0 ]; then\u0026#34;, \u0026#34; echo \\\u0026#34;Ansible is not installed. Please install Ansible and rerun the command\\\u0026#34; \u0026gt;\u0026amp;2\u0026#34;, \u0026#34; exit 1\u0026#34;, \u0026#34;fi\u0026#34;, \u0026#34;execdir=$(dirname $0)\u0026#34;, \u0026#34;cd $execdir\u0026#34;, \u0026#34;if [ -z \u0026#39;{{playbook}}\u0026#39; ] ; then\u0026#34;, \u0026#34; if [[ \\\u0026#34;{{playbookurl}}\\\u0026#34; == http* ]]; then\u0026#34;, \u0026#34; wget \u0026#39;{{playbookurl}}\u0026#39; -O playbook.yml\u0026#34;, \u0026#34; if [ $? -ne 0 ]; then\u0026#34;, \u0026#34; echo \\\u0026#34;There was a problem downloading the playbook. Make sure the URL is correct and that the playbook exists.\\\u0026#34; \u0026gt;\u0026amp;2\u0026#34;, \u0026#34; exit 1\u0026#34;, \u0026#34; fi\u0026#34;, \u0026#34; elif [[ \\\u0026#34;{{playbookurl}}\\\u0026#34; == s3* ]] ; then\u0026#34;, \u0026#34; aws --version\u0026#34;, \u0026#34; if [ $? -ne 0 ]; then\u0026#34;, \u0026#34; echo \\\u0026#34;The AWS CLI is not installed. The CLI is required to process Amazon S3 URLs. Install the AWS CLI and run the command again.\\\u0026#34; \u0026gt;\u0026amp;2\u0026#34;, \u0026#34; exit 1\u0026#34;, \u0026#34; fi\u0026#34;, \u0026#34; aws s3 cp \u0026#39;{{playbookurl}}\u0026#39; playbook.yml\u0026#34;, \u0026#34; if [ $? -ne 0 ]; then\u0026#34;, \u0026#34; echo \\\u0026#34;Error while downloading the document from S3\\\u0026#34; \u0026gt;\u0026amp;2\u0026#34;, \u0026#34; exit 1\u0026#34;, \u0026#34; fi\u0026#34;, \u0026#34; else\u0026#34;, \u0026#34; echo \\\u0026#34;The playbook URL is not valid. Verify the URL and try again.\\\u0026#34;\u0026#34;, \u0026#34; fi\u0026#34;, \u0026#34;else\u0026#34;, \u0026#34; echo \u0026#39;{{playbook}}\u0026#39; \u0026gt; playbook.yml\u0026#34;, \u0026#34;fi\u0026#34;, \u0026#34;if [[ \\\u0026#34;{{check}}\\\u0026#34; == True ]] ; then\u0026#34;, \u0026#34; /usr/local/bin/ansible-playbook -i \\\u0026#34;localhost,\\\u0026#34; --check -c local -e \\\u0026#34;{{extravars}}\\\u0026#34; playbook.yml\u0026#34;, \u0026#34;else\u0026#34;, \u0026#34; /usr/local/bin/ansible-playbook -i \\\u0026#34;localhost,\\\u0026#34; -c local -e \\\u0026#34;{{extravars}}\\\u0026#34; playbook.yml\u0026#34;, \u0026#34;fi\u0026#34; ] } } ] } Applying Ansible playbook to the fleet of mac1.metal Let’s give our new SSM Document a try! (I suppose you have at least one mac1 instance running, right?)\nIn AWS SSM, go to the Run Command feature, then click on the Run Command button.\nOn the new panel, type the name of your Document (macos-arbitrary-ansible-playbook in this example) in the search field and press enter.\nSelect the Document, and you’ll see its parameters and settings.\nThe rest is self-explanatory. Enter either a playbook code or a link to the source file, add extra variables if needed, and select the target host or a filtered bunch (I like that feature with tags filtering!). Finally, click on the “Run” orange button to apply your playbook.\nThat’s it! Now you can make all your ansible-playbook dreams come true! 😁\n","permalink":"https://devdosvid.blog/2021/05/27/run-ansible-playbook-on-mac-ec2-instances-fleet-with-aws-systems-manager/","summary":"Configuration management for mac1.metal and mac2.metal AWS Instances","title":"Run Ansible playbook on Mac EC2 Instances fleet with AWS Systems Manager"},{"content":" In November 2021, AWS has added this functionality as a native CloudFront feature.\nI suggest switching to the native implementation. I have described how to configure Security Response Headers for CloudFront in the following article:\nApply Cloudfront Security Headers With Terraform\nA couple of weeks ago, AWS released CloudFront Functions — a “true edge” compute capability for the CloudFront.\nIt is “true edge” because Functions work on 200+ edge locations (link to doc) while its predecessor, the Lambda@Edge, runs on a small number of regional edge caches.\nOne of the use cases for Lambda@Edge was adding security HTTP headers (it’s even listed on the product page), and now there is one more way to make it using CloudFront Functions.\nSubscribe to blog updates! What are security headers, and why it matters Security Headers are one of the web security pillars.\nThey specify security-related information of communication between a web application (i.e., website) and a client (i.e., browser) and protect the web app from different types of attacks. Also, HIPAA and PCI, and other security standard certifications generally include these headers in their rankings.\nWe will use CloudFront Functions to set the following headers:\nContent Security Policy Strict Transport Security X-Content-Type-Options X-XSS-Protection X-Frame-Options Referrer Policy You can find a short and detailed explanation for each security header on Web Security cheatsheet made by Mozilla\nCloudFront Functions overview In a nutshell, CloudFront Functions allow performing simple actions against HTTP(s) request (from the client) and response (from the CloudFront cache at the edge). Functions take less than one millisecond to execute, support JavaScript (ECMAScript 5.1 compliant), and cost $0.10 per 1 million invocations.\nEvery CloudFront distribution has one (default) or more Cache behaviors, and Functions can be associated with these behaviors to execute upon a specific event.\nThat is how the request flow looks like in general, and here is where CloudFront Functions execution happens:\nCloudFront Functions support Viewer Request (after CloudFront receives a request from a client) and Viewer Response (before CloudFront forwards the response to the client) events.\nYou can read more about the events types and their properties here — CloudFront Events That Can Trigger a Lambda Function - Amazon CloudFront.\nAlso, the CloudFront Functions allow you to manage and operate the code and lifecycle of the functions directly from the CloudFront web interface.\nSolution overview CloudFront distribution should exist before Function creation so you could associate the Function with the distribution.\nCreation and configuration of the CloudFront Function consist of the following steps:\nCreate Function In the AWS Console, open CloudFront service and lick on the Functions on the left navigation bar, then click Create function button. Enter the name of your Function (e.g., “security-headers”) and click Continue.\nBuild Function On the function settings page, you will see four tabs with the four lifecycle steps: Build, Test, Publish, Associate.\nPaste the function code into the editor and click “Save.”\nHere is the source code of the function:\nfunction handler(event) { var response = event.response; var headers = response.headers; headers[\u0026#39;strict-transport-security\u0026#39;] = { value: \u0026#39;max-age=63072000; includeSubdomains; preload\u0026#39;}; headers[\u0026#39;content-security-policy\u0026#39;] = { value: \u0026#34;default-src \u0026#39;none\u0026#39;; img-src \u0026#39;self\u0026#39;; script-src \u0026#39;self\u0026#39;; style-src \u0026#39;self\u0026#39;; object-src \u0026#39;none\u0026#39;; frame-ancestors \u0026#39;none\u0026#39;\u0026#34;}; headers[\u0026#39;x-content-type-options\u0026#39;] = { value: \u0026#39;nosniff\u0026#39;}; headers[\u0026#39;x-xss-protection\u0026#39;] = {value: \u0026#39;1; mode=block\u0026#39;}; headers[\u0026#39;referrer-policy\u0026#39;] = {value: \u0026#39;same-origin\u0026#39;}; headers[\u0026#39;x-frame-options\u0026#39;] = {value: \u0026#39;DENY\u0026#39;}; return response; } Test Function Open the “Test” tab — let’s try our function first before it becomes live!\nSelect Viewer Response event type and Development Stage, then select “Viewer response with headers” as a Sample test event (you will get a simple set of headers automatically).\nNow click the blue “Test” button and observe the output results:\nCompute utilization represents the relative amount of time (on a scale between 0 and 100) your function took to run Check the Response headers tab and take a look at how the function added custom headers. Publish Function Let’s publish our function. To do that, open the Publish tab and click on the blue button “Publish and update.” Associate your Function with CloudFront distribution Now, you can associate the function with the CloudFront distribution.\nTo do so, open the Associate tab, select the distribution and event type (Viewer Response), and select the Cache behavior of your distribution which you want to use for the association.\nOnce you associate the function with the CloudFront distribution, you can test it in live mode.\nI will use curl here to demonstrate it:\n\u0026gt; curl -i https://d30i87a4ss9ifz.cloudfront.net HTTP/2 200 content-type: text/html content-length: 140 date: Sat, 22 May 2021 00:22:18 GMT last-modified: Tue, 27 Apr 2021 23:07:14 GMT etag: \u0026#34;a855a3189f8223db53df8a0ca362dd62\u0026#34; accept-ranges: bytes server: AmazonS3 via: 1.1 50f21cb925e6471490e080147e252d7d.cloudfront.net (CloudFront) content-security-policy: default-src \u0026#39;none\u0026#39;; img-src \u0026#39;self\u0026#39;; script-src \u0026#39;self\u0026#39;; style-src \u0026#39;self\u0026#39;; object-src \u0026#39;none\u0026#39;; frame-ancestors \u0026#39;none\u0026#39; strict-transport-security: max-age=63072000; includeSubdomains; preload x-xss-protection: 1; mode=block x-frame-options: DENY referrer-policy: same-origin x-content-type-options: nosniff x-cache: Miss from cloudfront x-amz-cf-pop: WAW50-C1 x-amz-cf-id: ud3qH8rLs7QmbhUZ-DeupGwFhWLpKDSD59vr7uWC65Hui5m2U8o2mw== You can also test your results here — Mozilla Observatory\nRead more That was a simplified overview of the CloudFront Functions capabilities.\nBut if you want to get deeper, here is a couple of useful links to start:\nAnother overview from AWS — CloudFront Functions Launch Blog More about creating, testing, updating and publishing of CloudFront Functions — Managing functions in CloudFront Functions - Amazon CloudFront So what to choose? CloudFront Functions are simpler than Lambda@Edge and run faster with minimal latency and minimal time penalty for your web clients.\nLambda@Edge takes more time to invoke, but it can run upon Origin Response event so that CloudFront can cache the processed response (including headers) and return it faster afterward.\nBut again, the CloudFront Functions invocations are much cheaper (6x times) than Lambda@Edge, and you do not pay for the function execution duration.\nThe final decision would also depend on the dynamic/static nature of the content you have at your origin.\nTo make a wise and deliberate decision, try to analyze your use case using these two documentation articles:\nChoosing between CloudFront Functions and Lambda@Edge How to Decide Which CloudFront Event to Use to Trigger a Lambda Function ","permalink":"https://devdosvid.blog/2021/05/21/configure-http-security-headers-with-cloudfront-functions/","summary":"Modifying response headers to enforce the security of the web application","title":"Configure HTTP Security headers with CloudFront Functions"},{"content":"I just wanted to compress one image, but went to far\u0026hellip;\nor \u0026ldquo;How to add TinyPNG image compression to your macOS Finder contextual menu.\u0026rdquo;\nWhat is it and how it works You select needed files or folders, then right-click on them, click on the Services menu item and choose TinyPNG.\nAfter a moment, the new optimized versions of images will appear near to original files.\nIf you selected a folder along with the files, the script would process all png and jpeg files in it.\nPrerequisites You need to register at TinyPNG and get your API key here — Developer API.\nThey sometimes block some countries (for example, Ukraine) from registration; in that case, try to use a web-proxy or VPN.\nHow to create Quick Action Workflow Open Automator application. If you never used this app before, please read about it on the official user guide website.\nOn the New Action screen, chose Quick Action\nAfter you click the \u0026ldquo;Choose\u0026rdquo; button, you\u0026rsquo;ll see the workflow configuration window.\nWorkflow configuration Find the Run Shell Script action on the Utilities list in Library on the left, and drag it onto the right side of the panel.\nSet the following workflow configuration options as described below:\nWorkflow receives current files and folders in Finder\nShell /bin/zsh\nPass input as arguments\nClick the Option button at the bottom of the Action window and Uncheck Show this action when the workflow runs.\nPut the following script into the Run Shell Script window, replacing the YOUR_API_KEY_HERE string with your API key obtained from TinyPNG.\nUtilities used in the script — explained curl — used to make web requests (like your browser does)\ngrep — used to parse the response for the needed header (i.e., field) with the file download link\ncut — used to extract the URL from the parsed result\nsed — used to remove the trailing \u0026ldquo;carriage return\u0026rdquo; symbol at the end of extracted string\nThe response body also contains a JSON object that includes the download URL; you can parse it with jq, for example. But I intentionally refused to use the jq tool because it is not pre-installed in MacOS.\nConclusion It is simple, and it does its job fine. And you don\u0026rsquo;t need to install anything to make it work.\nTo make this a bit fancier, you might also like to add a \u0026ldquo;Display Notification\u0026rdquo; (from the Utilities library on the left) after the \u0026ldquo;Run Shell Script\u0026rdquo;. The action will display a notification once image processing is completed.\nThank you for reading!\n","permalink":"https://devdosvid.blog/2021/02/14/using-tinypng-image-compression-from-macos-finder-contextual-menu/","summary":"\u003cp\u003eI just wanted to compress one image, but went to far\u0026hellip;\u003c/p\u003e\n\u003cp\u003eor \u0026ldquo;How to add TinyPNG image compression to your macOS Finder contextual menu.\u0026rdquo;\u003c/p\u003e\n\u003ch1 id=\"what-is-it-and-how-it-works\"\u003eWhat is it and how it works\u003c/h1\u003e\n\u003cp\u003eYou select needed files or folders, then right-click on them, click on the Services menu item and choose TinyPNG.\u003c/p\u003e\n\u003cp\u003eAfter a moment, the new optimized versions of images will appear near to original files.\u003c/p\u003e\n\u003cp\u003eIf you selected a folder along with the files, the script would process all \u003ccode\u003epng\u003c/code\u003e and \u003ccode\u003ejpeg\u003c/code\u003e files in it.\u003c/p\u003e","title":"Using TinyPNG Image Compression From MacOS Finder Contextual Menu"},{"content":"I guess macOS was designed for a user, not for the ops or engineers, so this is why its customization and usage for CI/CD are not trivial (compared to something Linux-based). A smart guess, huh?\nConfiguration Management Native Apple\u0026rsquo;s Mobile device management (a.k.a MDM) and Jamf is probably the most potent combination for macOS configuration. But as much as it\u0026rsquo;s mighty, it is a cumbersome combination, and Jamf is not free.\nThen we have Ansible, Chef, Puppet, SaltStack — they all are good with Linux, but what about macOS?\nI tried to search for use cases of mentioned CM tools for macOS. However, I concluded that they wrap the execution of native macOS command-line utilities most of the time.\nAnd if you search for the \u0026lsquo;macos\u0026rsquo; word in Chef Supermarket or Puppet Forge, you won\u0026rsquo;t be impressed by the number of actively maintained packages. Although, here is a motivating article about using Chef automating-macos-provisioning-with-chef if you prefer it. I could not find something similar and fresh for Puppet, so I am sorry, Puppet fans.\nThat is why I decided to follow the KISS principle and chose Ansible.\nIt\u0026rsquo;s easy to write and read the configuration, it allows to group tasks and to add execution logic , and it feels more DevOps executing shell commands inside Ansible tasks instead of shell scripts; I know you know that 😂\nBy the way, Ansible Galaxy does not have many management packages for macOS, either. But thankfully, it has the basics:\nhomebrew with homebrew_cask and homebrew_tap — to install software launchd — to manage services osx_defaults — to manage some user settings (not all!) I used Ansible to build the macOS AMI for CI/CD, so here are some tips for such a case.\nSome values are hardcoded intentionally in the code examples for the sake of simplicity and easy reading. You would probably want to parametrize them.\nXcode installation example The following tasks will help you to automate the basics.\n- name: Install Xcode shell: \u0026#34;xip --expand Xcode.xip\u0026#34; args: chdir: /Applications - name: Accept License Agreement shell: \u0026#34;/Applications/Xcode.app/Contents/Developer/usr/bin/xcodebuild -license accept\u0026#34; - name: Accept License Agreement shell: \u0026#34;/Applications/Xcode.app/Contents/Developer/usr/bin/xcodebuild -runFirstLaunch\u0026#34; - name: Switch into newly installed Xcode context shell: \u0026#34;xcode-select --switch /Applications/Xcode.app/Contents/Developer\u0026#34; Example of software installation with Brew - name: Install common build software community.general.homebrew: name: \u0026#34;{{ item }}\u0026#34; state: latest loop: - swiftlint - swiftformat - wget ScreenSharing (remote desktop) configuration example - name: Turn On Remote Management shell: \u0026#34;./kickstart -activate -configure -allowAccessFor -specifiedUsers\u0026#34; args: chdir: /System/Library/CoreServices/RemoteManagement/ARDAgent.app/Contents/Resources/ - name: Enable Remote Management for CI user shell: \u0026#34;./kickstart -configure -users ec2-user -access -on -privs -all\u0026#34; args: chdir: /System/Library/CoreServices/RemoteManagement/ARDAgent.app/Contents/Resources/ Shell rulez, yes.\nBuilding the AMI Packer by HashiCorp, of course.\nI would love to compare Packer with EC2 Image Builder, but it does not support macOS yet (as of Feb'21).\nPacker configuration is straightforward, so I want to highlight only the things specific to the \u0026ldquo;mac1.metal\u0026rdquo; use case.\nTimeouts As I mentioned in the previous article, the creation and deletion time of the \u0026ldquo;mac1.metal\u0026rdquo; Instance is significantly bigger than Linux. That is why you should raise the polling parameters for the builder.\nExample:\n\u0026#34;aws_polling\u0026#34;: { \u0026#34;delay_seconds\u0026#34;: 30, \u0026#34;max_attempts\u0026#34;: 60 } And it would be best if you also increased the SSH timeout:\n\u0026#34;ssh_timeout\u0026#34;: \u0026#34;1h\u0026#34; Fortunately, Packer\u0026rsquo;s AMI builder does not require an explicit declaration of the Dedicated Host ID. So you can just reference the same subnet where you allocated the Host, assuming you did it with the enabled \u0026ldquo;Auto placement\u0026rdquo; parameter during the host creation.\nExample:\n\u0026#34;tenancy\u0026#34;: \u0026#34;host\u0026#34;, \u0026#34;subnet_id\u0026#34;: \u0026#34;your-subnet-id\u0026#34; Provisioning Packer has Ansible Provisioner that I used for the AMI. Its documentation is also very clean and straightforward.\nBut it is still worth mentioning that if you want to parametrize the Ansible playbook, then the following configuration example will be handy:\n\u0026#34;extra_arguments\u0026#34;: [ \u0026#34;--extra-vars\u0026#34;, \u0026#34;your-variable-foo=your-value-bar]\u0026#34; ], \u0026#34;ansible_env_vars\u0026#34;: [ \u0026#34;ANSIBLE_PYTHON_INTERPRETER=auto_legacy_silent\u0026#34;, \u0026#34;ANSIBLE_OTHER_ENV_VARIABLE=other_value\u0026#34; ] Configuration at launch If you\u0026rsquo;re familiar with AWS EC2, you probably know what the Instance user data is.\nA group of AWS developers made something similar for the macOS: EC2 macOS Init.\nIt does not support cloud-init as on Linux-based Instances, but it can run shell scripts, which is quite enough.\nEC2 macOS Init utility is a Launch Daemon (macOS terminology) that runs on behalf of the root user at system boot. It executes the commands according to the so-called Priority Groups, or the sequence in other words.\nThe number of the group corresponds to the execution order. You can put several tasks into a single Priority Group, and the tool will execute them simultaneously.\nEC2 macOS Init uses a human-readable configuration file in toml format.\nExample:\n[[Module]] Name = \u0026#34;Create-some-folder\u0026#34; PriorityGroup = 3 FatalOnError = false RunPerInstance = true [Module.Command] Cmd = [\u0026#34;mkdir\u0026#34;, \u0026#34;/Users/ec2-user/my-directory\u0026#34;] RunAsUser = \u0026#34;ec2-user\u0026#34; EnvironmentVars = [\u0026#34;MY_VAR_FOO=myValueBar\u0026#34;] I should clarify some things here.\nModules — a set of pre-defined modules for different purposes. It is something similar to the Ansible modules.\nYou can find the list of available modules here ec2-macos-init/lib/ec2macosinit\nThe RunPerInstance directive controls whether a module should run. There are three of such directives, and here is what they mean:\nRunPerBoot — module will run at every system boot RunPerInstance — module will run once for the Instance. Each Instance has a unique ID; the init tool fetches it from the AWS API before the execution and keeps its execution history per Instance ID. When you create a new Instance from the AMI, it will have a unique ID, and the module will run again. RunOnce — module will run only once, despite the instance ID change I mentioned the execution history above. When EC2 macOS Init runs on the Instance first time, it creates a unique directory with the name per Instance ID to store the execution history and user data copy.\nRunPerInstance and RunOnce directives depend on the execution history, and modules with those directives will run again on the next boot if the previous execution failed. It was not obvious to me why RunOnce keeps repeating itself every boot until I dug into the source code.\nFinally, there is a module for user data. It runs at the end by default (priority group #4) and pulls the user data script from AWS API before script execution.\nI suggest looking into the default init.toml configuration file to get yourself more familiar with the capabilities of the tool.\nThe init tool can also clear its history, which is useful for the new AMI creation.\nExample:\nec2-macos-init clean -all And you can run the init manually for debugging purposes.\nExample:\nec2-macos-init run You can also combine the EC2 macOS Init actions (made by modules) with your script in user data for more accurate nontrivial configurations.\nWrapping up As a whole, building and operating macOS-based AMI does not differ from AMI management for other platforms.\nThere are the same principle stages: prepare, clear, build, execute deployment script (if necessary). Though, the particular implementation of each step has its nuances and constraints.\nSo the whole process may look as follows:\nProvision and configure needed software with Ansible playbook Clean-up system logs and EC2 macOS Init history (again, with Ansible task) Create the AMI Add more customizations at launch with EC2 macOS Init modules and user data (that also executes your Ansible playbook or shell commands) Getting into all this was both fun and interesting. Sometimes painful, though. 😆\nI sincerely hope this article was helpful to you. Thank you for reading!\n","permalink":"https://devdosvid.blog/2021/02/01/customizing-mac1.metal-ec2-ami-new-guts-more-glory/","summary":"How to build macOS EC2 Instance AMI for CI/CD using Ansible and Packer","title":"Customizing mac1.metal EC2 AMI — new guts, more glory"},{"content":" Updated on the 23rd of October, 2021: Terraform AWS provider now supports Dedicated Hosts natively In November 2021, AWS announced the support for Mac mini instances.\nI believe this is huge, even despite the number of constraints this solution has. This offering opens the door to seamless macOS CI/CD integration into existing AWS infrastructure.\nSo here is a quick-start example of creating the dedicated host and the instance altogether using Terraform.\nI intentionally used some hardcoded values for the sake of simplicity in the example.\nresource \u0026#34;aws_ec2_host\u0026#34; \u0026#34;example_host\u0026#34; { instance_type = \u0026#34;mac1.metal\u0026#34; availability_zone = \u0026#34;us-east-1a\u0026#34; } resource \u0026#34;aws_instance\u0026#34; \u0026#34;example_instance\u0026#34; { ami = data.aws_ami.mac1metal.id host_id = aws_ec2_host.example_host.id instance_type = \u0026#34;mac1.metal\u0026#34; subnet_id = data.aws_subnet.example_subnet.id } data \u0026#34;aws_subnet\u0026#34; \u0026#34;example_subnet\u0026#34; { availability_zone = \u0026#34;us-east-1a\u0026#34; filter { name = \u0026#34;tag:Tier\u0026#34; # you should omit this filter if you don\u0026#39;t distinguish your subnets on private and public values = [\u0026#34;private\u0026#34;] } } data \u0026#34;aws_ami\u0026#34; \u0026#34;mac1metal\u0026#34; { owners = [\u0026#34;amazon\u0026#34;] most_recent = true filter { name = \u0026#34;name\u0026#34; values = [\u0026#34;amzn-ec2-macos-11*\u0026#34;] # get latest BigSur AMI } } Simple as that, yes. Now, you can integrate it into your CI system and have the Mac instance with the underlying host in a bundle. Pro tip: you can leverage the aws_ec2_instance_type_offerings Data Source and use its output with aws_subnet source to avoid availability zone hardcoding. To make the code more uniform and reusable, you can wrap it into a Terraform module that accepts specific parameters (such as instance_type or availability_zone) as input variables.\n","permalink":"https://devdosvid.blog/2021/01/20/terraforming-mac1.metal-at-aws/","summary":"How to manage MacOS EC2 Instances with Terraform","title":"Terraforming mac1.metal at AWS"},{"content":"This is the review of EC2 Mac instances, mac1.metal and mac2.metal — the new EC2 instance types that enables macOS workloads on AWS.\nUpdated in June 2022: new information added about the offering — more cool stuff 🤩 AWS announced EC2 macOS instances based on the Intel CPU on 30 November 2020.\nAfter a year and a half, the M1 Mac Instances arrived (7 July 2022).\nSome basic information about the Mac EC2 first:\nThe mac1.metal instances are Intel-based\n12 vCPU, 32 GiB RAM | 10 Gbps Network and 8 Gbps EBS bandwidth\nThe mac2.metal instances are powered by M1 Apple Silicon processors.\n8 vCPU, 16 GiB RAM, 16 core Apple Neural Engine | 10 Gbps Network and 8 Gbps EBS bandwidth\nThe Instance must be placed onto a Dedicated Host because these are physical Apple Mac minis.\nAWS has integrated the Nitro System to make Macs work as EC2 instances and connect them with many other services.\nMac minis are connected to the AWS Nitro via Thunderbolt, just a fun fact.\nYou don\u0026rsquo;t pay anything for the Instance itself, but you pay for the Dedicated Host leasing, and the minimum lease time is 24 hours.\nSubscribe to blog updates! EC2 Mac Instance Prices (June 2022) On-demand pricing (us-east-1, North Virginia:\nmac1.metal costs 1.083 USD per hour or about 780 USD per month mac2.metal costs 0.65 USD per hour or about 470 USD per month The mac2.metal costs 40% less compared to the mac1.metal Since the minimal leas time for the mac*.metal dedicated host is 24 hours, the first launch of the Instance is always costly, mind that while testing.\nOne day of mac1.metal usage costs 26 USD\nOne day of mac2.metal usage costs 15.6 USD\nTo save yourself some money, you can use Savings Plans, both Instance and Compute, and save up to 44% off On-Demand pricing.\nFor example, with the one-year commitment, partial 50% upfront payment, and the Instance Savings pricing model, you can get the 20% lower price per hour:\nmac1.metal — 0.867 USD mac2.metal — 0.52 USD Feel free to play with the numbers in the Dedicated Host Pricing Calculator\nSupported Operating Systems (June 2022) macOS Mojave 10.14.x (mac1.metal only) macOS Catalina 10.15.x (mac1.metal only) macOS Big Sur 11.x macOS Monterey 12.x What can it do Here is a list of some features that the mac1.metal and mac2.metal instances have:\nIt lives in your VPC because it is an EC2 Instance, so you can access many other services.\nFor EBS, it supports the attachment of up to 16 volumes for mac1 and 10 for mac2.\nIt supports SSM Agent and Session Manager.\nIt has several AWS tools pre-installed: AWS CLI, SSM Agent, EFS Utils, and more.\nIt has pre-installed Enhanced Network Interface drivers. My test upload/download to S3 was about 300GB/s.\nIt can report CPU metrics to CloudWatch.\nIt supports AutoScaling 🚀\nAnd you can share the instances using AWS Resource Access Manager.\nFor example, you can have a dedicated AWS account used solely as a MacOS-based farm in your organization where instances are shared with other accounts.\nAlthough there is a local SSD disk available, EC2 Mac can boot only from the EBS\nThe built-it physical SSD is still there and yours to use: build-cache, temporary storage, etc.\nHowever, AWS does not manage or support the Apple hardware\u0026rsquo;s internal SSD. So there is no guarantee for data persistency.\nWhat can\u0026rsquo;t it do It can\u0026rsquo;t recognize the attached EBS if you connected it while the instance was running — you must reboot the instance to make it visible. It does not recognize the live resize of EBS either — you must reboot the instance so resize change can take effect. And the same relates to the Elastic Network Interfaces — attach and reboot the instance to apply it. It does not support several services that rely on additional custom software, such as \u0026ldquo;EC2 Instance Connect\u0026rdquo; and \u0026ldquo;AWS Inspect.\u0026rdquo; But I think that AWS will add macOS distros for those soon. As of July 2022, mac2.metal is not supported by Host Resource Groups. Therefore you cannot use mac2.metal Instances in Auto Scaling Groups. But AWS support says they are working on that, so fingers crossed!\nLaunching the Instance Jeff Bar published an excellent how-to about kickstart of the \u0026ldquo;mac1.metal\u0026rdquo;, so I will focus on things he did not mention.\nOnce you allocated the Dedicated Host and launched an Instance, the underlying system connects the EBS with a root file system to the Mac Mini.\nThe Mac metal Instances can boot from the EBS-backed macOS AMIs only.\nIf you specified the EBS size to be more than AMI\u0026rsquo;s default, you need to resize the disk inside the system manually after the boot 1.\nThe time from the Instance launch until you can SSH into it varies between 5 and 20 minutes.\nYou have the option to access it over SSH with your private key. For example, if you need to set up Screen Sharing, you must allow it through the \u0026ldquo;kickstart\u0026rdquo; command-line utility and set the user password 2.\nCustomizing the Instance I wrote a separate post about mac1.metal AMI customization and creation, so check it out!\nCustomizing mac1.metal EC2 AMI — new guts, more glory\nThough, I would like to mention two things here:\nSystem updates are disabled by default in the macOS AMIs provided by AWS. But you can use them with no issues. For example:\nsudo softwareupdate --install --all It is possible to set a custom screen resolution when connected to the instance using native ScreenSharing or any other VNC-compatible software. There are many tools, but AWS suggests the displayplacer.\nDestroying the Instance Such an easy thing to do, right? Well, it depends.\nThe complex Instance scrubbing process begins when you click on the \u0026ldquo;Terminate\u0026rdquo; item in the Instance actions menu.\nAWS wants to ensure that anyone who uses the Host (Mac mini) after you will get your data stored neither on disks (including the physical SSD mentioned earlier) nor inside memory or NVRAM, nor anywhere else.\nAWS does not share many details of this scrubbing process, but it takes more than an hour to complete.\nWhen scrubbing is started, the Dedicated Host transitions to the Pending state.\nDedicated Host transitions to Available state once scrubbing is finished. But you must wait for another 10-15 minutes to be able to release it finally.\nI don\u0026rsquo;t know why they set the Available state value earlier than the Host is available for operations, but this is how it works now (Jan'21).\nTherefore, you can launch the next Instance on the same Host no earlier than ~1,5 hours after you terminated the previous one. That doesn\u0026rsquo;t seem very pleasant in the first couple of weeks, but you will get used to it. 😄\nAnd again: you can release the \u0026ldquo;mac1.metal\u0026rdquo; Dedicated Host no earlier than 24 hours after it was allocated. So plan your tests wisely.\nIf the lease time of a host is more than 24 hours, you don’t need to wait for the scrubbing process to finish to release that host. Legal things It is a bit tricky thing, but in short words:\nyou are allowed to use the Instances solely for developer purposes you must agree to all software EULAs on the system Here is the license agreement of the macOS Monterey if you want to deal with it like a pro — link.\nSome more cool stuff to check: EC2 macOS Init launch daemon, which initializes Mac instances. EC2 macOS Homebrew Tap (Third-Party Repository) with several management tools which come pre-installed into macOS AMI from AWS.\nIndeed it is powerful, and it has its trade-offs, such as price and some technical constraints. But it is an actual macOS device natively integrated into the AWS environment. So I guess it is worth to be tried!\nThanks for reading this! Stay tuned for more user experience feedback about baking custom AMIs, automated software provisioning with Ansible, and other adventures with mac1.metal!\nHow to resize the EBS at mac1.metal in Terminal\nGet the identifier of EBS (look for the first one with GUID_partition_scheme): diskutil list physical external\nOr here is a more advanced version to be used in a script:\nDISK_ID=$(diskutil list physical external | grep \u0026#39;GUID_partition_scheme\u0026#39;| tr -s \u0026#39; \u0026#39; | cut -d\u0026#39; \u0026#39; -f6) It would probably be disk0 if you did not attach additional EBS.\nThen run the repair job for the disk, using its identifier: diskutil repairDisk disk0\nAdvanced version:\nyes | diskutil repairDisk $DISK_ID Now get the APFS container identifier (look for Apple_APFS): diskutil list physical external\nAdvanced version:\nAPFS_ID=$(diskutil list physical external | grep \u0026#39;Apple_APFS\u0026#39; | tr -s \u0026#39; \u0026#39; | cut -d\u0026#39; \u0026#39; -f8) It would probably be disk0s2 if you did not attach additional EBS.\nFinally, resize the APFS container: diskutil apfs resizeContainer disk0s2\nAdvanced version\ndiskutil apfs resizeContainer $APFS_ID \u0026#160;\u0026#x21a9;\u0026#xfe0e; How to setup Screen Sharing at mac1.metal in Terminal\nThe kickstart command-line tool resides in /System/Library/CoreServices/RemoteManagement/ARDAgent.app/Contents/Resources/ so you\u0026rsquo;ll better to cd into that directory for convenience:\n# Turn On Remote Management for a user to be specified later sudo ./kickstart -activate -configure -allowAccessFor -specifiedUsers # Enable Remote Management for ec2-user user sudo ./kickstart -configure -users ec2-user -access -on -privs -all # Set the user password sudo passwd ec2-user \u0026#160;\u0026#x21a9;\u0026#xfe0e; ","permalink":"https://devdosvid.blog/2021/01/19/mac1.metal-and-mac2.metal-ec2-instances-user-experience/","summary":"Apple macOS development ecosystem with the power of AWS Cloud","title":"mac1.metal and mac2.metal EC2 Instances — user experience"},{"content":"A simple but cool announcement from AWS — AWS CloudShell. A tool for ad-hoc AWS management via CLI directly in your browser.\nI like when AWS releases something simple to understand and yet powerful.\nSo it is not another DevOps Guru, believe me :)\nYes, this is similar to the shells that GCE and Azure have. No, you can’t access your instances from it, so it’s not a jump server (bastion host). Yes, it has AWS CLI and other tools pre-installed. Even Python and Node.js. No, you can’t (well, you can, but should not) use it as an alternative to the day-to-day console on your laptop. Yes, you can manage all resources from that shell as much as your IAM permissions allow you (even with SSO, which is pretty cool). No, it does not support Docker. Yes, you have 1 GB of permanent storage and the ability to transfer files in and out. More Yes and No’s here: https://docs.aws.amazon.com/cloudshell/latest/userguide/faq-list.html\nhttps://aws.amazon.com/cloudshell/faqs/\n","permalink":"https://devdosvid.blog/2020/12/16/aws-cloudshell/","summary":"Native and official way to run AWS CLI in a browser","title":"AWS CloudShell"},{"content":"The work with Terraform code may become tangled sometimes. Here are some guides on how to streamline it and make it transparent for you and your team.\nIt is extremely helpful in a team, and can benefit you even if you work individually. A good workflow enables you to streamline a process, organize it, and make it less error-prone.\nThis article summaries several approaches when working with Terraform, both individually and in a team. I tried to gather the most common ones, but you might also want to develop your own.\nThe common requirement for all of them is a version control system (such as Git). This is how you ensure nothing is lost and all your code changes are properly versioned tracked.\nBasic Concepts Let’s define the basic actions first.\nAll described workflows are built on top of three key steps: Write, Plan, and Apply. Nevertheless, their details and actions vary between workflows.\nBasic 3-steps Terraform workflow Write – this is where you make changes to the code.\nPlan – this is where you review changes and decide whether to accept them.\nApply – this is where you accept changes and apply them against real infrastructure.\nIt\u0026rsquo;s a simple idea with a variety of possible implementations.\nCore individual workflow This is the most simple workflow if you work alone on a relatively small TF project. This workflow suits both local and remote backends well.\nGit-based Terraform workflow Write You clone the remote code repo or pull the latest changes, edit the configuration code, then run the terraform validate and terraform fmt commands to make sure your code works well.\nPlan This is where you run the terraform plan command to make sure that your changes do what you need. This is a good time to commit your code changes changes (or you can do it in the next step).\nApply This is when you run terraform apply and introduce the changes to real infrastructure objects. Also, this is when you push committed changes to the remote repository.\nCore team workflow This workflow is good for when you work with configuration code in a team and want to use feature branches to manage the changes accurately.\nGit-based Terraform workflow in a team Write Start by checking out a new branch, make your changes, and run the terraform validate and terraform fmt commands to make sure your code works well.\nRunning terraform plan at this step will help ensure that you\u0026rsquo;ll get what you expect.\nPlan This is where code and plan reviews happen.\nAdd the output of the terraform plan command to the Pull Request with your changes. It would be a good idea to add only the changed parts of the common output, which is the part that starts with \u0026ldquo;Terraform will perform the following actions\u0026rdquo; string.\nApply Once the PR is reviewed and merged to the upstream branch, it is safe to finally pull the upstream branch locally and apply the configuration with terraform apply.\nTeam workflow with automation In a nutshell, this workflow allows you to introduce a kind of smoke test for your infrastructure code (using plan) and also to automate the feedback in the CI process.\nThe automated part of this workflow consists of a speculative plan on commit and/or Pull Request (PR ), along with adding the output of plan to the comment of the PR. A speculative plan mean just to show the changes, and not apply them afterward.\nGit-based Terraform workflow with automation Write This step is the same as in the previous workflow.\nPlan This is where your CI tool does its job.\nLet’s review this step by step:\n1️⃣ You create a PR with the code changes you wish to implement.\n2️⃣ The CI pipeline is triggered by an event from your code repository (such as webhook push) and it runs a speculative plan against your code.\n3️⃣ The list of changes (a so-called \u0026ldquo;plan diff\u0026rdquo;) is added to PR for review by the CI.\n4️⃣ Once merged, the CI pipeline runs again and you get the final plan that\u0026rsquo;s ready to be applied to the infrastructure.\nApply Now that you have a branch (i.e. main) with the fresh code to apply, you need to pull it locally and run terraform apply.\nYou can also add the automated apply here – step 5 in the picture below. This may be very useful for disposable environments such as testing, staging, development, and so on.\nThe exact CI tool to be used here is up to you: Jenkins, GitHub Actions, and Travis CI all work well.\nAn important thing to note is that the CI pipeline must be configured in a bi-directional way with your repository to get the code from it and report back with comments to PR.\nAs an option, you may consider using Terraform Cloud which has a lot of functionality, including the above mentioned repo integration, even with the free subscription.\nIf you have never worked with Terraform Cloud before and want to advice to get started, I\u0026rsquo;ll provide the links at the end of this article.\nImport workflow This workflow refers to a situation when you have some objects already created (i.e., up and running), and you need to manage them with Terraform.\nSuppose we already have an S3 bucket in AWS called \u0026ldquo;someassetsbucket\u0026rdquo; and we want to include it into our configuration code.‌‌\nTerraform resource import workflow Prepare You should create a resource block to be used later for the real object you’re going to import.\nYou don’t need to fill the arguments in it at the start, so it may be just a blank resource block, for example:\nresource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;someassetsbucket\u0026#34; { } Import Now you need to import the information about the real object into your existing Terraform state file.\nThis can be done with the terraform import command, for example:\nterraform import aws_s3_bucket.assets \u0026#34;someassetsbucket\u0026#34; Be sure to also check the list of possible options import accepts with terraform import -h\nWrite Now you need to write the corresponding Terraform code for this bucket.\nTo avoid modifying your real object on the terraform apply action, you should specify all needed arguments with the exact values from the import phase.\nYou can see the details by running the terraform state show command, for example:\nterraform state show aws_s3_bucket.assets The output of this command will be very similar to the configuration code. But it contains both arguments and attributes of the resource, so you need to clean it up before applying it.\nYou can use one of the following tactics:\neither copy/paste it, and then run terraform validate and terraform plan several times to make sure there are no errors like \u0026ldquo;argument is not expected here\u0026rdquo; or \u0026ldquo;this field cannot be set\u0026rdquo; or you can pick and write only the necessary arguments In any case, be sure to refer to the documentation of the resource during this process.\nPlan The goal is to have a terraform plan output showing \u0026ldquo;~ update in-place\u0026rdquo; changes only.\nHowever, it is not always clear whether the real object will be modified or only the state file will be updated. This is why you should understand how a real object works and know its life cycle to make sure it is safe to apply the plan.\nApply This is usual the terraform apply action.\nOnce applied, your configuration and state file will correspond to the real object configuration.\nWrapping up Here is an overview of Terraform Cloud for those who never worked with it before: ‌‌Overview of Terraform Cloud Features\nAnd here is a nice tutorial to start with: Get Started - Terraform Cloud\nAlso, here is an overview of workflows at scale from the HashiCorp CTO which might be useful for more experienced Terraform users: Terraform Workflow Best Practices at Scale\nThank you for reading. I hope you will try one of these workflows, or develop your own!\n","permalink":"https://devdosvid.blog/2020/09/16/terraform-workflow-working-individually-and-in-a-team/","summary":"An overview of best practices for working with Terraform individually or in a team","title":"Terraform Workflow — Working Individually and in a Team"},{"content":"I successfully passed the \u0026ldquo;HashiCorp Certified — Terraform Associate\u0026rdquo; exam last Friday and decided to share some advice for exam preparation.\nMake yourself a plan Make a list of things you are going to go through: links to the study materials, practice tasks, some labs, some articles on relative blogs (Medium, Dev.to, etc.). It should look at a \u0026ldquo;todo\u0026rdquo; or \u0026ldquo;check\u0026rdquo;-list. It may seem silly at first glance, but the list with checkboxes does its \u0026ldquo;cognitive magic\u0026rdquo;. When you go point by point, marking items as \u0026ldquo;done\u0026rdquo;, you feel the progress and this motivates you to keep going further. For example, you can make a plan from the resources I outlined below in this article.\nI encourage you to explore the Internet for something by yourself as well. Who knows, perhaps you will find some learning course that fits you better. And that is great! However, when you find it, take extra 5-10 minutes to go through its curriculum and create a list with lessons.\nIt feels so nice to cross out items off the todo list, believe me 😄 Go through the official Study Guide Despite your findings on the Internet, I strongly suggest going through the official study guide\nStudy Guide - Terraform Associate Certification\nIt took me about 20 hours to complete it (including practice tasks based on topics in the guide), and it was the core of my studying. I did not buy or search for some third-party course intentionally because I did have some Terraform experience before starting the preparation. But give the official guide a chance even if you found some course. It is well-made and matches real exam questions very precisely.\nAlso, there is an official Exam Review. Someone might find this even better because it is a direct mapping of each exam objective to HashiCorp\u0026rsquo;s documentation and training.\nTake additional tutorials Here is a list of additional tutorials and materials I suggest adding into your learning program:\nOfficial guides / documentation: Automate Terraform Collaborate using Terraform Cloud Terraform tutorials Reuse Configuration with Modules A Practitioner’s Guide to Using HashiCorp Terraform Cloud with GitHub Enforce Policy with Sentinel Third-party articles and guides: Using the terraform console to debug interpolation syntax YouTube playlist with exam-like questions review Find yourself some practice Mockup a project You can greatly improve your practice by mocking some real business cases.\nIf you already work in some company you can set up the project you\u0026rsquo;re working with using Terraform. If you don’t have a real project or afraid to accidentally violate NDA, try this open-source demo project: Real World Example Apps.\nIt is a collection of different codebases for front-end and back-end used to build the same project. Just find the combination that suits your experience better and try to build the infrastructure for it using Terraform.\nAnswer forum topics Last but not least advice — try to answer some questions on the official Terraform forum.\nThis is a nice way to test your knowledge, help others, and develop the community around Terraform. Just register there, look for the latest topics, and have fun!\n🍀 I sincerely wish you exciting preparation and a successful exam! 🍀\n","permalink":"https://devdosvid.blog/2020/09/15/terraform-certification-tips/","summary":"Summary of a learning path to HashiCorp Certified — Terraform Associate","title":"Terraform Certification Tips"},{"content":"Surprisingly, a lot of beginners skip over Terraform modules for the sake of simplicity, or so they think. Later, they find themselves going through hundreds of lines of configuration code.\nI assume you already know some basics about Terraform or even tried to use it in some way before reading the article.\nPlease note: I do not use real code examples with some specific provider like AWS or Google intentionally, just for the sake of simplicity.\nSubscribe to blog updates! Terraform modules You already write modules even if you think you don’t.\nEven when you don\u0026rsquo;t create a module intentionally, if you use Terraform, you are already writing a module – a so-called \u0026ldquo;root\u0026rdquo; module.\nAny number of Terraform configuration files (.tf) in a directory (even one) forms a module.\nWhat does the module do? A Terraform module allows you to create logical abstraction on the top of some resource set. In other words, a module allows you to group resources together and reuse this group later, possibly many times.\nLet\u0026rsquo;s assume we have a virtual server with some features hosted in the cloud. What set of resources might describe that server? For example: – the virtual machine itself (created from some image) – an attached block device of specified size (for additional storage) – a static public IP mapped to the server\u0026rsquo;s virtual network interface – a set of firewall rules to be attached to the server – something else\u0026hellip; (i.e. another block device, additional network interface, etc)\nTerraform module example Now let\u0026rsquo;s assume that you need to create this server with a set of resources many times. This is where modules are really helpful – you don\u0026rsquo;t want to repeat the same configuration code over and over again, do you?\nHere is an example that illustrates how our \u0026ldquo;server\u0026rdquo; module might be called. \u0026ldquo;To call a module\u0026rdquo; means to use it in the configuration file.\nHere we create 5 instances of the \u0026ldquo;server\u0026rdquo; using single set of configurations (in the module):\nmodule \u0026#34;server\u0026#34; { count = 5 source = \u0026#34;./module_server\u0026#34; some_variable = some_value } Modules organisation: child and root Of course, you would probably want to create more than one module. Here are some common examples:\nfor a network (i.e. VPC) for a static content hosting (i.e. buckets) for a load balancer and it\u0026rsquo;s related resources for a logging configuration and whatever else you consider a distinct logical component of the infrastructure Let\u0026rsquo;s say we have two different modules: a \u0026ldquo;server\u0026rdquo; module and a \u0026ldquo;network\u0026rdquo; module. The module called \u0026ldquo;network\u0026rdquo; is where we define and configure our virtual network and place servers in it:\nmodule \u0026#34;server\u0026#34; { source = \u0026#34;./module_server\u0026#34; some_variable = some_value } module \u0026#34;network\u0026#34; { source = \u0026#34;./module_network\u0026#34; some_other_variable = some_other_value } Once we have some custom modules, we can refer to them as \u0026ldquo;child\u0026rdquo; modules. And the configuration file where we call child modules relates to the root module.\nTerraform modules relations A child module can be sourced from a number of places:\nlocal paths official Terraform Registry (if you\u0026rsquo;re familiar with other registries, i.e. Docker Registry then you already understand the idea) Git repository (a custom one or GitHub/BitBucket) HTTP URL to .zip archive with module But how can you pass resources details between modules?\nIn our example, the servers should be created in a network. So how can we tell the \u0026ldquo;server\u0026rdquo; module to create VMs in a network which was created in a module called \u0026ldquo;network\u0026rdquo;?\nThis is where encapsulation comes in.\nModule encapsulation Encapsulation in Terraform consists of two basic concepts: module scope and explicit resources exposure.\nModule Scope All resource instances, names, and therefore, resource visibility, are isolated in a module\u0026rsquo;s scope. For example, module \u0026ldquo;A\u0026rdquo; can\u0026rsquo;t see and does not know about resources in module \u0026ldquo;B\u0026rdquo; by default.\nResource visibility, sometimes called resource isolation, ensures that resources will have unique names within a module\u0026rsquo;s namespace. For example, with our 5 instances of the \u0026ldquo;server\u0026rdquo; module:\nmodule.server[0].resource_type.resource_name module.server[1].resource_type.resource_name module.server[2].resource_type.resource_name On the other hand, we could create two instances of the same module with different names:\nmodule \u0026#34;server-alpha\u0026#34; { source = \u0026#34;./module_server\u0026#34; some_variable = some_value } module \u0026#34;server-beta\u0026#34; { source = \u0026#34;./module_server\u0026#34; some_variable = some_value } In this case, the naming or address of resources would be as follows:\nmodule.server-alpha.resource_type.resource_name module.server-beta.resource_type.resource_name Explicit resources exposure If you want to access some details for the resources in another module, you\u0026rsquo;ll need to explicitly configure that.\nBy default, our module \u0026ldquo;server\u0026rdquo; doesn\u0026rsquo;t know about the network that was created in the \u0026ldquo;network\u0026rdquo; module.\nResource incapsulation in Terraform modules So we must declare an output value in the \u0026ldquo;network\u0026rdquo; module to export its resource, or an attribute of a resource, to other modules.\nThe module \u0026ldquo;server\u0026rdquo; must declare a variable to be used later as the input.\nModule outputs and input variables This explicit declaration of the output is the way to expose some resource (or information about it) outside — to the scope of the \u0026lsquo;root\u0026rsquo; module, hence to make it available for other modules.\nNext, when we call the child module \u0026ldquo;server\u0026rdquo; in the root module, we should assign the output from the \u0026ldquo;network\u0026rdquo; module to the variable of the \u0026ldquo;server\u0026rdquo; module:\nnetwork_id = module.network.network_id Here is how the final code for calling our child modules will look like in result:\nmodule \u0026#34;server\u0026#34; { count = 5 source = \u0026#34;./module_server\u0026#34; some_variable = some_value network_id = module.network.network_id } module \u0026#34;network\u0026#34; { source = \u0026#34;./module_network\u0026#34; some_other_variable = some_other_value } This example configuration would create 5 instances of the same server, with all the necessary resources, in the network we created with as a separate module.\nWrap up Now you should understand what modules are and what do they do.\nIf you\u0026rsquo;re at the beginning of your Terraform journey, here are some suggestions for the next steps.\nI encourage you to take this short tutorial from HashiCorp, the creators of Terraform, about modules: \u0026ldquo;Organize Configuration\u0026rdquo;\nAlso, there is a great comprehensive study guide which covers everything from beginner to advanced concepts about Terraform: \u0026ldquo;Study Guide - Terraform Associate Certification\u0026rdquo;\nThe modular code structure makes your configuration more flexible and yet easy to be understood by others. The latter is especially useful in teamwork.\n","permalink":"https://devdosvid.blog/2020/09/09/what-are-terraform-modules-and-how-do-they-work/","summary":"Explanation of Terraform modules and their main concepts in English.","title":"What are Terraform Modules and how do they work?"},{"content":"Here is some CLI shortcuts I use day-to-day to simplify and speed-up my Terraform workflow. Requirements \u0026mdash; bash-compatible interpreter, because aliases and functions described below will work with bash, zsh and ohmyzsh.\nIn order to use any of described aliases of functions, you need to place it in your ~/.bashrc or ~/.zshrc file (or any other configuration file you have for your shell).\nThen just source this file, for example: source ~/.zshrc\nFunction: list outputs and variables of given module You need to provide the path to module directory, and this function will list all declared variables and outputs module has. It comes very useful when you don\u0026rsquo;t remember them all and just need to take a quick look.\n## TerraForm MOdule Explained function tfmoe { echo -e \u0026#34;\\nOutputs:\u0026#34; grep -r \u0026#34;output \\\u0026#34;.*\\\u0026#34;\u0026#34; $1 |awk \u0026#39;{print \u0026#34;\\t\u0026#34;,$2}\u0026#39; |tr -d \u0026#39;\u0026#34;\u0026#39; echo -e \u0026#34;\\nVariables:\u0026#34; grep -r \u0026#34;variable \\\u0026#34;.*\\\u0026#34;\u0026#34; $1 |awk \u0026#39;{print \u0026#34;\\t\u0026#34;,$2}\u0026#39; |tr -d \u0026#39;\u0026#34;\u0026#39; } Example usage:\nuser@localhost $: tfmoe ./module_alb Outputs: alb_arn Variables: acm_certificate_arn lb_name alb_sg_list subnets_id_list tags Function: pre-fill module directory with configuration files You need to provide a path to the module directory and this function will create a bunch of empty \u0026lsquo;default\u0026rsquo; .tf files in it.\n#TerraForm MOdule Initialize function tfmoi { touch $1/variables.tf touch $1/outputs.tf touch $1/versions.tf touch $1/main.tf } Example usage:\nuser@localhost $: mkdir ./module_foo \u0026amp;\u0026amp; temoi $_ user@localhost $: ls ./module_foo main.tf outputs.tf variables.tf versions.tf Aliases The purpose of these aliases is just to keep you from typing long commands when you want to do a simple action.\nalias tf=\u0026#39;terraform\u0026#39; alias tfv=\u0026#39;terraform validate\u0026#39; alias tfi=\u0026#39;terraform init\u0026#39; alias tfp=\u0026#39;terraform plan\u0026#39; This one is useful because it makes format tool to go in-depth (recursively) through directories.\nalias tfm=\u0026#39;terraform fmt -recursive\u0026#39; Example usage:\nuser@localhost $: tfm module_ecs_cluster/ecs.tf module_alb/alb.tf ","permalink":"https://devdosvid.blog/2020/08/25/terraform-cli-shortcuts/","summary":"A bunch of small tools I use to simplify Terraform workflow","title":"Terraform CLI shortcuts"},{"content":"Lookup plugins for Ansible allow you to do a lot of cool things. One of them is to securely pass sensitive information to your playbooks.\nIf you manage some apps in AWS with Ansible, then using Parameter Store or Secrets Manager along with it might greatly improve your security.\nDescribed plugins are the part of amazon.aws collection in Ansible Galaxy.\nRun the following command to install this collection:\nansible-galaxy collection install amazon.aws Variables with SSM Parameter Store Let\u0026rsquo;s say you have some variables defined in \u0026lsquo;defaults/main.yaml\u0026rsquo; file of your role or maybe in group_vars.yaml file.\n# content of dev.vars.yaml to be included in your play or role use_tls: true application_port: 3000 app_env: development stripe_api_key: 1HGASU2eZvKYlo2CT5MEcnC39HqLyjWD If you store such things locally on Ansible control node, you probably encrypt it with ansible-vault\nSSM Parameter Store gives you more flexibility and security by centralized storage and management of parameters and secrets, so let\u0026rsquo;s use it with Ansible:\n# content of dev.vars.yaml to be included in your play or role use_tls: \u0026#34;{{ lookup(\u0026#39;amazon.aws.aws_ssm\u0026#39;, \u0026#39;/dev/webserver/use_tls\u0026#39;, bypath=true) }}\u0026#34; application_port: \u0026#34;{{ lookup(\u0026#39;amazon.aws.aws_ssm\u0026#39;, \u0026#39;/dev/webserver/application_port\u0026#39;, bypath=true) }}\u0026#34; app_env: \u0026#34;{{ lookup(\u0026#39;amazon.aws.aws_ssm\u0026#39;, \u0026#39;/dev/webserver/app_env\u0026#39;, bypath=true) }}\u0026#34; stripe_api_key: \u0026#34;{{ lookup(\u0026#39;amazon.aws.aws_ssm\u0026#39;, \u0026#39;/dev/webserver/stripe_api_key\u0026#39;, bypath=true) }}\u0026#34; The syntax is fairly simple:\nThe aws_ssm – is the name of the lookup plugin.\nThe /dev/webserver/use_tls – is the path to the key in the SSM Paramter Store.\nYou can use this anywhere you can use templating: in a play, in variables file, or a Jinja2 template.\nVariables with Secret Manager Another lookup plugin is Secrets Manager. Secrets Manager stores sensitive information in JSON format, supports rotation, encryption and other cool stuff.\nHere is a quick example of its functionality in a Playbook:\n- name: Extract something secrets from Secret Manager debug: msg: \u0026#34;{{ lookup(\u0026#39;amazon.aws.aws_secret\u0026#39;, \u0026#39;DatabaseConnectionSettings\u0026#39;)}}\u0026#34; The above task will generate the following output\nTASK [Extract something secrets from Secret Manager] **************************************************** ok: [some_server] =\u0026gt; { \u0026#34;msg\u0026#34;: { \u0026#34;dbname\u0026#34;: \u0026#34;database\u0026#34;, \u0026#34;engine\u0026#34;: \u0026#34;mysql\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;p@$$w0rd\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;3306\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;db_user\u0026#34; } } This is nice if you want to insert a JSON as is, but you will need additional parsing in case you want to get only some of JSON elements.\nBenefits from using lookup plugins The biggest win here is the security — no sensitive information in the source code.\nAnother benefit is the convenience of data management: instead using built-in local vault, you can manage the secrets in centralized way.\nOne of the common use cases for this kind of setup is CI/CD pipelines that generally run in stateless environments.\nWhen using lookup plugins for Secrets Manager and Parameter Store, mind the access permissions. The assumed IAM role must allow at least the read access to SSM Parameter Store (+ KMS read access to be able to decrypt the keys) or the read access to Secrets Manager.\nYou can find documentation for described plugins here aws_ssm and here aws_secret.\nMore about lookup plugins: https://docs.ansible.com/ansible/latest/plugins/lookup.html\n","permalink":"https://devdosvid.blog/2020/08/06/manage-ansible-playbook-secrets-with-aws-services/","summary":"A quick guide to secure secrets management with Ansible in AWS","title":"Manage Ansible playbook secrets with AWS services"},{"content":"You might have heard about Terraform before, but if you have never tried it, this blog can help you to get the main point.\nA few words about “Infrastructure as Code\u0026quot; First of all, Terraform is the way to manage the infrastructure in the form of code. The same way developers write the code to create applications, Terraform code can create the resources in virtual data centers (i.e., clouds).\nInfrastructure as Code, or IaC, is when you describe and manage your infrastructure as… (guess what?) …code, literally.\nIn a nutshell, that means you can define all the elements (servers, networks, storage, etc.) and resources (memory, CPU, etc.) of your infrastructure in configuration files, and manage it in a way similar to how you handle the source code of the applications: branches, releases, and all that stuff.\nAnd the main idea behind the IaC approach is that it manages the state of things and must remain the single source of truth (configuration truth) for your infrastructure.\nFirst, you define the state via the code. Then IaC tool (Terraform, for example) applies this state to the infrastructure: all that is missing according to the code will be created, all that differs from the code will be changed, and all that exists in the infrastructure but is not described via code — will be destroyed.\nWhy and when do you need the Terraform for a project? Terraform is a specific tool, hence like any other tool, it has its particular application area. There is no strict definition of project kind that needs Terraform (surprise!), but in general, you need to consider using Terraform if you answer ‘yes’ to one of the following questions:\nDo you have multiple logical elements of the same kind (in plural) in your infrastructure, i.e., several web servers, several application servers, several database servers? Do you have numerous environments (or workspaces) where you run your applications, i.e., development, staging, QA, production? Do you spend a significant amount of time managing the changes in the environment(s) where you run your applications? How does it work? Terraform works with the source code of configuration and interprets the code into real objects inside on-premise or cloud platforms.\nHow Terraform works in a nutshell Terraform supports many platforms: cloud providers such as AWS, Azure, GCP, DigitalOcean, and other platforms such as OVH, 1\u0026amp;1, Hetzner, etc. It also supports infrastructure software such as Docker, Kubernetes, Chef, and even databases and monitoring software. That is why Terraform is so popular — it is an actual Swiss knife in the operations world.\nSo to create, change, or destroy the infrastructure, Terraform needs the source code.\nThe source code is a set of configuration files that defines your infrastructure state. The code uses its syntax, but it looks very user-friendly. Here is an example: the following configuration block describes the virtual server (EC2 instance) in AWS.\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;web_server\u0026#34; { ami = \u0026#34;ami-a1b2c3d4\u0026#34; instance_type = \u0026#34;t3.micro\u0026#34; } Terraform can automatically detect the dependencies between resources described in the code and also allows you to add custom dependencies when needed.\nWhen you apply the code the first time, Terraform creates a so-called “state file,\u0026quot; which Terraform uses to map your code to resources created in the hosting platform. Terraform will use each subsequent “apply\u0026quot; action to compare the code changes with the sate file to decide what should be done (and in what order) against real infrastructure.\nOne of the essential functions of the state file is the management of dependencies between the resources. For example (some technical nuances are omitted for simplicity): if you have a server created inside some network and you are going to change the network configuration in Terraform code, Terraform will know it should change that server configuration, or the server should be re-created inside the updated network.\nWhat does Terraform consist of: Terraform configuration code consists of several elements: providers, resources, modules, input variables, output values, local values, expressions, functions.\nProvider Provider is an entity that defines what exactly is possible to do with the cloud or on-premises infrastructure platform you manage via Terraform.\nIt translates your code into proper API calls to the hosting provider, transforming your configuration into real object: servers, networks, databases, and so on.\nResource Resource is the essential part of the configuration code. That is where the definition of infrastructure objects happens.\nResources are the main building blocks of the whole code. A resource can represent some object in the hosting provider (example: server) or the part of a compound object (example: attachable storage for a server)\nEvery resource has a type and local name. For example, here is how EC2 instance configuration may look like:\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;web_server\u0026#34; { ami = \u0026#34;ami-a1b2c3d4\u0026#34; instance_type = \u0026#34;t3.micro\u0026#34; } The aws_instance is a resource type, and web_server is the local resource name. Later, when Terraform applies this code, it will create an EC2 instance with some particular ID in AWS.\nOnce created, Terraform will store the ID in the state file with mapping information that logically connects it with web_server.\nThe ami, instance_type, and private_ip are the arguments with values that define the actual state of the resource. However, there are many value types, depending on the particular argument and particular resource type, so I will not focus on them here.\nModules Terraform module Modules is the kind of logical containers or groups for resources you define and use together. The purpose of modules is the grouping of resources and the possibility of reusing the same code with different variables.\nLet’s get back to the example with the EC2 instance and say you need to have a static public IP address with it. In such a case, here is how the module for web server may look like:\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;web_server\u0026#34; { ami = \u0026#34;ami-a1b2c3d4\u0026#34; instance_type = \u0026#34;t3.micro\u0026#34; } resource \u0026#34;aws_eip\u0026#34; \u0026#34;web_server_public_ip\u0026#34; { instance = \u0026#34;${aws_instance.web_server.id}\u0026#34; } Having these two resources together allows us to think of it as a stand-alone unit you can reuse later, for example, in our development, staging, and production environments. And not by copying and pasting it, but via reference to the module defined only once.\nPlease note: we specified an instance argument inside the aws_eip resource to reference another resource details (the ID of an instance). It is possible because of the way how Terraform treats dependencies. For example, when it detects the dependency (or you define it explicitly), Terraform creates the leading resource first. Only after the resource is created and available Terraform will create the dependent one.\nThe modules is a kind of standalone topic in Terraform. There is a separate article in my blog that explains what modules are and how do they work.\nVariables Input variables work as parameters for the modules so module code could be reusable. Let’s look at the previous example: it has some hardcoded values — instance image ID and instance type. Here is how you can make it more abstract and reusable:\nvariable \u0026#34;image_id\u0026#34; { type = string } variable \u0026#34;instance_type\u0026#34; { type = string } resource \u0026#34;aws_instance\u0026#34; \u0026#34;web_server\u0026#34; { ami = var.image_id instance_type = var.instance_type } Values for the variables then can be passed either via CLI and environment variables (if you have only the one, so-called root module) or via explicit values in the block where you call a module, for example:\nmodule \u0026#34;web_server_production\u0026#34; { source = \u0026#34;./modules/web_server\u0026#34; image_id = \u0026#34;ami-a1b2c3d4\u0026#34; instance_type = \u0026#34;m5.large\u0026#34; } module \u0026#34;web_server_development\u0026#34; { source = \u0026#34;./modules/web_server\u0026#34; image_id = \u0026#34;ami-a2b3c4d5\u0026#34; instance_type = \u0026#34;t3.micro\u0026#34; } Output values are similar to the \u0026ldquo;return\u0026rdquo; of a function in development language. You can use them for dependencies management (for example, when a module requires something from another module) and print specific values at the end of Terraform work (for example, to be used for notification in the CI/CD process).\nLocal values, expressions, functions — three more things that augment the capabilities of Terraform and make it more similar to a programming language (which is excellent, by the way).\nThe local values are used inside modules for extended data manipulations.\nThe expressions are used to set the values (for many things), such as the value of some argument in resource configuration. For example, they used either to refer something (just as we referenced instance ID \u0026quot;${aws_instance.web_server.id}\u0026quot; in the example above) or to compute the value within your configuration.\nThe functions in Terraform are built-in jobs you can call to transform and combine values. For example, the tolist() function converts its argument to a list value.\nAnd this is it? Yes, in short words — this is what Terraform is. Not rocket science if it\u0026rsquo;s about to manage a small infrastructure, but it gets more complicated with bigger infrastructure. Like any other engineering tool, though.\nOkay, what next? If you read down to this point, then it means it is worth \u0026ldquo;get your hands dirty\u0026rdquo; and to try building your Infrastructure with Terraform. There are plenty of courses and books (and the \u0026ldquo;Terraform up and running\u0026rdquo; is one of the most popular). Still, my learning path started from the following: Official guide from Hashicorp — comprehensive and free guide from Terraform developers. Just pick your favorite cloud (AWS, Azure, GCP) and go through the topics.\nAnother thing worth your attention is A Comprehensive Guide to Terraform.\nOnce you finish this guide, I suggest jumping into the more real-world things and describing the infrastructure of the most common project you work with.\nYour hands-on experience is the best way to learn Terraform!\n","permalink":"https://devdosvid.blog/2020/05/02/terraform-explained-in-english/","summary":"What is Terraform, how does it work, why and when do you need it","title":"Terraform explained in English"},{"content":"Although Github Actions service is generally available since November 13, 2020, and there are about 243,000,000 results for \u0026ldquo;github actions\u0026rdquo; in Google search already, I have just reached it\u0026hellip;\nIt\u0026rsquo;s half past midnight, it took me about 35 commits to make my first github automation work, but it finally works and this blog post was built and published automatically!\nActions everywhere One of the most (or maybe the most one) powerful things in Actions is \u0026hellip; Actions! Github made a simple but genius thing: they turned well-known snippets (we do with pipelines) into the marketplace of well-made (sometimes not) simple and complex applications you can use in your automation workflow. https://github.com/marketplace?type=actions\nSo now you can either re-invent your wheel or re-use someone else\u0026rsquo;s code to make the needed automation.\nI decided to automate publications to this blog via Actions in order to have some practice.\nThere are two workflows: one for the blog (website), and one for the CV (cv).\nactions/checkout@v2 actions/upload-artifact@v2 actions/download-artifact@v2 In both workflows, the build job is performed within a container, which is different per workflow: Ruby for the blog and Pandoc for CV.\nHere is how the build job looks like for the blog:\njobs: build: runs-on: ubuntu-latest container: image: ruby:2.6.4 options: --workdir /src steps: - name: Checkout uses: actions/checkout@v2 - name: Build blog run: | bundle install bundle exec jekyll build --verbose --destination _site - name: Upload artifacts uses: actions/upload-artifact@v2 with: name: _site path: _site As you can see, I run the steps within the Ruby container. This simplifies things related to file permissions and directory mounting because checkout is made inside the container.\nThe deploy step is performed via shell run command for now, for better clearness (can be replaced to third-party action or custom-made one): it makes a commit to gh-pages branch which is configured for Github Pages.\ndeploy: if: github.ref == \u0026#39;refs/heads/master\u0026#39; needs: build runs-on: ubuntu-latest steps: - name: Checkout gh-pages branch uses: actions/checkout@v2 with: ref: \u0026#39;gh-pages\u0026#39; - name: Get the build artifact uses: actions/download-artifact@v2 with: name: _site path: ./ - name: Deploy (push) to gh-pages run: | git config user.name \u0026#34;$GITHUB_ACTOR\u0026#34; git config user.email \u0026#34;${GITHUB_ACTOR}@bots.github.com\u0026#34; git add -A git commit -a -m \u0026#34;Updated Website\u0026#34; git remote set-url origin \u0026#34;https://x-access-token:${{ secrets.DEPLOY_TOKEN }}@github.com/vasylenko/serhii.vasylenko.info.git\u0026#34; git push --force-with-lease origin gh-pages Old good things made better A lot of common things have been introduced to GitHubActions with some sweet additions:\nyou can also specify different environments for your jobs in the same workflow; you can use environment variables with a different visibility scope: either workflow, or job, or step; you can use cache for dependencies and reuse it between workflow runs while keeping workflow directory clean; you can trigger a workflow by repo events and have a quite complex conditional logic or filters (if needed), external webhooks and by a schedule; you can pass artifacts between jobs inside a workflow with ease - Github provides simple actions for this, so you don\u0026rsquo;t need to dance around temporary directories or files; and much more ","permalink":"https://devdosvid.blog/2020/03/18/github-actions-first-impression/","summary":"My first meet with github actions\u0026hellip; in action.","title":"Github Actions - First impression"},{"content":"926 out of 1000 Last week I\u0026rsquo;ve successfully passed AWS SAA exam with 926 points from 1000 possible. I can\u0026rsquo;t help saying this and showing off my verification page{:target=\u0026quot;_blank\u0026quot;}, just because I am very happy so please excuse me my bragging.\nWhat helped me But I would like to share some advices and tips with anyone who reads this and wants to pass the exam. I mean, I could just twit about it if that was only about saying \u0026ldquo;hey look at me!\u0026rdquo;, right?\nIt took me a month of intensive studying and here is what helped me:\nVideo course at CloudGuru - AWS Certified Solutions Architect Associate{:target=\u0026quot;_blank\u0026quot;}.\nPrice: $50 for a monthly subscription.\nTips: They have a 7 days free trial, which is actually quite enough to view the whole course. But I strongly recommend purchasing a full month, because it is better to view the lectures gradually during couple of weeks for better learning. Plus they have a nice exam simulator where you can practice several times.\nPractice Tests set at Udemy - AWS Certified Solutions Architect Associate Practice Exams{:target=\u0026quot;_blank\u0026quot;}.\nPrice: $40 or only $12 if you\u0026rsquo;re lucky to get it during a sale. But they make sales quite often and they frequently provide discuounts for new students. I purchaced it for $12.\nTips: practice tests are very useful, do not skip buying them. You will find your weak spots and also learn a lot by passing these tests. This particular set has a quite good explanations for each question.\nExam Guide at O\u0026rsquo;relly Media AWS Certified Solutions Architect Associate All-in-One Exam Guide{:target=\u0026quot;_blank\u0026quot;}.\nPrice: this one can be easily read during 10 days free trial period :wink:\nTips: The new exam version is released on 23rd of March, so it is better to find a new updated version of exam guide. And I suggest reading the guide after the video course or vise versa, but do not mix them.\nMaking notes. Seriously, note taking helps you memorize better. Do not skip it, and note your video courses as well as exam guide. Later, you will find your notes very helpful before the exam day - they will fresh up your memory.\nThank you for reading down to this point. I hope my advices were helpful and you will pass the exam!\n","permalink":"https://devdosvid.blog/2020/03/15/aws-saa-exam-results/","summary":"Sharing my AWS SAA exam results","title":"AWS SAA exam results"},{"content":"/2025/04/04/delegate-for-growth-scaling-your-impact-through-others/\n","permalink":"https://devdosvid.blog/latest-post/","summary":"\u003cp\u003e/2025/04/04/delegate-for-growth-scaling-your-impact-through-others/\u003c/p\u003e","title":""},{"content":"Hello! I am Serhii, a software engineer by day and a tech enthusiast by night, currently crafting DevSecOps magic at Grammarly.\nMy passion lies in creating and integrating developer experience and security automation tools.\nMy journey has been accompanied by leadership roles in strategic projects, which have strengthened infrastructure and boosted team efficiency.\nCuriosity fuels my days, and I believe there\u0026rsquo;s always room for improvement.\nAnd I welcome you to read more about me in my CV!\nTechnical blogging is my hobby; I am also fond of astronomy and history.\nBy the way, the blog name — \u0026ldquo;devDosvid\u0026rdquo; — is made of the word \u0026ldquo;dosvid\u0026rdquo; (pronounced as [dɔsʋid]), which means \u0026ldquo;experience\u0026rdquo; in Ukrainian.\nFeel free to connect if our paths cross, whether it\u0026rsquo;s for a tech chat or a new opportunity! 🙌\ncontact@devdosvid.blog.\nSome of selected tech talks [EN] Building Trust: Strengthening Your Software Supply Chain Security DevOps fwdays\u0026#39;25 conference February 22, 2025 View Slides This talk highlights automating Supply Chain security by integrating tools like Renovate, Wiz, GitLab, and JFrog Artifactory into existing workflows. It details centralized artifact management, embedding automated security scans that block vulnerable deployments (with an override option), and maintaining source code security with runtime monitoring and effective SLAs.\n[UA] DevOps not about tooling — practical experience of launching an internal education project Благодійний івент «DevOps у стані 2023» May 13, 2023 View Slides In this talk, I share my experience of creating and launching tutorial-based project — Platform University — to onboard new employees onto the internal DevOps tooling.\n[EN] Scalable CI/CD Infrastructure with EKS and Karpenter KCD Ukraine Fundraiser 2023 April 6, 2023 Watch Recording View Slides This talk shares the ways we manage the GitLab CI runners infrastructure with EKS and Karpenter: what the permissions (IAM) and network management look like; how we mutate K8s pods at launch with the Kyverno policy engine to provide more agility for CI configuration to users; how ArgoCD and GitLab are relevant in this process.\n","permalink":"https://devdosvid.blog/about/","summary":"\u003cp\u003eHello! I am Serhii, a software engineer by day and a tech enthusiast by night, currently crafting DevSecOps magic at \u003ca href=\"https://www.grammarly.com/\"\u003eGrammarly\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eMy passion lies in creating and integrating developer experience and security automation tools.\u003cbr\u003e\nMy journey has been accompanied by leadership roles in strategic projects, which have strengthened infrastructure and boosted team efficiency.\u003cbr\u003e\nCuriosity fuels my days, and I believe there\u0026rsquo;s always room for improvement.\u003c/p\u003e\n\u003cp\u003eAnd I welcome you to read more about me \u003ca href=\"/cv\"\u003ein my CV\u003c/a\u003e!\u003c/p\u003e","title":"About me — Serhii Vasylenko"},{"content":"Staff Software Engineer with expertise in infrastructure modernization, CI/CD optimization, and security automation.\nProven ability to architect robust systems, enhance developer productivity, and cultivate high-performing engineering teams through effective leadership and mentorship.\nExperience Staff Engineer, Infrastructure \u0026amp; Security, Grammarly, 11/2020 - Present Reduced critical vulnerabilities by 59% by leading the Supply Chain Security initiative (CISO\u0026rsquo;s top priority) and implementing end-to-end security controls for over 70 production services while maintaining a seamless developer experience with guided break-glass procedures. Established a foundation for strategic enterprise initiatives by leading a team of five engineers to architect and deliver the Infra and CI/CD stack for the Customer Cloud PoC, enabling the deployment of Grammarly\u0026rsquo;s text-processing services in Kubernetes environments for enterprise customers. Achieved an annual cost reduction of $500K in CI by redesigning and launching infrastructure for Linux-based GitLab Runners on Kubernetes while improving scalability and developer self-service capabilities. Delivered a 40% faster CI and 35% cost savings for iOS and macOS teams by implementing macOS GitLab Runners with self-service horizontal scaling and Apple Silicon processor support. Improved security operations by formalizing vulnerability management processes and boosting CI security coverage by reducing security job failures from 30% to 4.5%. Enhanced security coverage for business-critical resources by establishing Wiz-JFrog integration for workload scanning within the internal network, addressing security audit requirements. Reduced engineer onboarding time from four weeks to one week by creating the Platform University learning program. Improved engineering productivity by integrating tools like Sourcegraph and GitHub Copilot, recognized internally as top productivity enhancements (~220 MAU). Enabled direct reports to lead complex initiatives independently by mentoring and leading teams of two to five engineers. Strengthened the hiring process by shaping and conducting System Design and SRE interviews for Platform teams and creating specialized formats for critical engineering roles. DevOps Coach, Hillel IT School, part-time, 06/2020 - 04/2023 Achieved an 85% job placement rate for graduates by delivering practical DevOps training focused on industry-relevant skills and technologies. Developed and delivered a comprehensive curriculum covering CI/CD, infrastructure-as-code, cloud architecture, and container orchestration. Created hands-on AWS lab environments to provide students with practical experience in cloud infrastructure management. Maintained a 4.8/5 satisfaction rating across all training sessions while adapting teaching methods to diverse learning styles and technical backgrounds. Engineering Manager, IT Craft, 03/2017 - 06/2020 Scaled engineering team from 1 to 8 engineers while successfully delivering 40+ projects, demonstrating effective talent acquisition and project management. Created a new revenue stream by establishing the \u0026lsquo;DevOps as a Service\u0026rsquo; business model, revolutionizing the department\u0026rsquo;s approach to client engagement. Improved organizational visibility by tripling cross-department project collaboration through enhanced team reputation and cross-functional partnerships. Developed a technical leadership pipeline by mentoring junior engineers into senior contributors through structured development programs. Drove business growth by effectively articulating technical value propositions during pre-sales activities. Technical Lead, YourServerAdmin, 06/2011 - 09/2016 Implemented ITIL practices and PRINCE2 methodology that improved cross-department service delivery and project management. Led technical team transformation through Agile adoption and capability development initiatives. Designed high-performance infrastructure solutions and recovery procedures that ensured business continuity. Progressed from L1 Technical Support to Technical Lead through demonstrated technical expertise and leadership. Skills Kubernetes, AWS, Terraform, CI/CD (GitLab, GitHub Actions), Golang.\nSupply Chain Security, Vulnerability Management, Security Automation.\nSystem Design, Developer Productivity, Performance Optimization, Technical Leadership.\n🇬🇧 English C2 (CEFR), 🇩🇪 Deutsch A1 (CEFR)\nEducation National University of Radioelectronics, Kharkiv, Ukraine\nBachelor\u0026rsquo;s degree, Computer Science\nActivities and interests Travel: I like to explore new cities and countries, and I also enjoy trekking and hiking. I’ve been to the Annapurna base camp, so the next stop is Everest’s base camp.\nBlogging: I love to blog about the technologies I use and learn at devdosvid.blog. Actively participate in technical communities as an AWS Community Builder and HashiCorp Ambassador.\nScience: Fond of History, Astronomy, and Physics. I wish to see the Betelgeuse supernova explosion someday, even though the expected explosion date is between today and 100k years.\n","permalink":"https://devdosvid.blog/cv/","summary":"Serhii Vasylenko — professional experience","title":"Serhii Vasylenko"}]